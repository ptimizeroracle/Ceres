{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ondine - LLM Dataset Engine","text":"<pre><code> \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584  \u2584\u2584        \u2584  \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584   \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584  \u2584\u2584        \u2584  \u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\n\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\u2590\u2591\u2591\u258c      \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\u2590\u2591\u2591\u258c      \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\n\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2588\u2591\u258c\u2590\u2591\u258c\u2591\u258c     \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2588\u2591\u258c \u2580\u2580\u2580\u2580\u2588\u2591\u2588\u2580\u2580\u2580\u2580 \u2590\u2591\u258c\u2591\u258c     \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\n\u2590\u2591\u258c       \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c       \u2590\u2591\u258c    \u2590\u2591\u258c     \u2590\u2591\u258c\u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c\n\u2590\u2591\u258c       \u2590\u2591\u258c\u2590\u2591\u258c \u2590\u2591\u258c   \u2590\u2591\u258c\u2590\u2591\u258c       \u2590\u2591\u258c    \u2590\u2591\u258c     \u2590\u2591\u258c \u2590\u2591\u258c   \u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\n\u2590\u2591\u258c       \u2590\u2591\u258c\u2590\u2591\u258c  \u2590\u2591\u258c  \u2590\u2591\u258c\u2590\u2591\u258c       \u2590\u2591\u258c    \u2590\u2591\u258c     \u2590\u2591\u258c  \u2590\u2591\u258c  \u2590\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\n\u2590\u2591\u258c       \u2590\u2591\u258c\u2590\u2591\u258c   \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u258c       \u2590\u2591\u258c    \u2590\u2591\u258c     \u2590\u2591\u258c   \u2590\u2591\u258c \u2590\u2591\u258c\u2590\u2591\u2588\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\n\u2590\u2591\u258c       \u2590\u2591\u258c\u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c       \u2590\u2591\u258c    \u2590\u2591\u258c     \u2590\u2591\u258c    \u2590\u2591\u258c\u2590\u2591\u258c\u2590\u2591\u258c\n\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2588\u2591\u258c\u2590\u2591\u258c     \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2588\u2591\u258c\u2584\u2584\u2584\u2584\u2588\u2591\u2588\u2584\u2584\u2584\u2584 \u2590\u2591\u258c     \u2590\u2591\u2590\u2591\u258c\u2590\u2591\u2588\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\u2584\n\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\u2590\u2591\u258c      \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c \u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\u2590\u2591\u258c      \u2590\u2591\u2591\u258c\u2590\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u258c\n \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580  \u2580        \u2580\u2580  \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580   \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580  \u2580        \u2580\u2580  \u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\u2580\n</code></pre> <p>Production-grade SDK for batch processing tabular datasets with LLMs. Built on LlamaIndex for provider abstraction, adds batch orchestration, automatic cost tracking, checkpointing, and YAML configuration for dataset transformation at scale.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Quick API: 3-line hello world with smart defaults and auto-detection</li> <li>Simple API: Fluent builder pattern for full control when needed</li> <li>Reliability: Automatic retries, checkpointing, error policies (99.9% completion rate)</li> <li>Cost Control: Pre-execution estimation, budget limits, real-time tracking</li> <li>Observability: Progress bars, structured logging, metrics, cost reports</li> <li>Extensibility: Plugin architecture, custom stages, multiple LLM providers</li> <li>Production Ready: Zero data loss on crashes, resume from checkpoint</li> <li>Multiple Providers: OpenAI, Azure OpenAI, Anthropic Claude, Groq, MLX (Apple Silicon), and custom APIs</li> <li>Local Inference: Run models locally with MLX (Apple Silicon) or Ollama - 100% free, private, offline-capable</li> <li>Multi-Column Processing: Generate multiple output columns with composition or JSON parsing</li> <li>Custom Providers: Integrate any OpenAI-compatible API (Together.AI, vLLM, Ollama, custom endpoints)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#option-1-quick-api-recommended","title":"Option 1: Quick API (Recommended)","text":"<p>The simplest way to get started - just provide your data, prompt, and model:</p> <pre><code>from ondine import QuickPipeline\n\n# Process data with smart defaults\npipeline = QuickPipeline.create(\n    data=\"data.csv\",\n    prompt=\"Clean this text: {description}\",\n    model=\"gpt-4o-mini\"\n)\n\n# Execute pipeline\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.processed_rows} rows\")\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\n</code></pre> <p>What's auto-detected:</p> <ul> <li>Input columns from <code>{placeholders}</code> in prompt</li> <li>Provider from model name (gpt-4 \u2192 openai, claude \u2192 anthropic)</li> <li>Parser type (JSON for multi-column, text for single column)</li> <li>Sensible batch size and concurrency for the provider</li> </ul>"},{"location":"#option-2-builder-api-full-control","title":"Option 2: Builder API (Full Control)","text":"<p>For advanced use cases requiring explicit configuration:</p> <pre><code>from ondine import PipelineBuilder\n\n# Build with explicit settings\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"description\"],\n              output_columns=[\"cleaned\"])\n    .with_prompt(\"Clean this text: {description}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_batch_size(100)\n    .with_concurrency(5)\n    .build()\n)\n\n# Estimate cost before running\nestimate = pipeline.estimate_cost()\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\n\n# Execute pipeline\nresult = pipeline.execute()\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>Install with pip or uv:</p> <pre><code>pip install ondine\n</code></pre> <p>Or with optional dependencies:</p> <pre><code># For Apple Silicon local inference\npip install ondine[mlx]\n\n# For observability (OpenTelemetry)\npip install ondine[observability]\n\n# For development\npip install ondine[dev]\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed installation instructions</li> <li>Quickstart - Your first pipeline in 5 minutes</li> <li>Core Concepts - Understanding pipelines, stages, and specifications</li> <li>Execution Modes - When to use sync, async, or streaming</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Ondine excels at:</p> <ul> <li>Data cleaning and normalization (PII detection, standardization)</li> <li>Content enrichment (classification, tagging, summarization)</li> <li>Extraction tasks (structured data from unstructured text)</li> <li>Translation and localization at scale</li> <li>Synthetic data generation with cost controls</li> <li>Quality assurance (validation, scoring, feedback)</li> </ul>"},{"location":"#why-ondine","title":"Why Ondine?","text":"<ul> <li>Production-Grade: Checkpointing, auto-retry, budget controls, observability</li> <li>Developer-Friendly: Fluent API, YAML config, CLI tools, extensive examples</li> <li>Cost-Aware: Pre-run estimation, real-time tracking, budget limits</li> <li>Reliable: 99.9% completion rate in production workloads</li> <li>Flexible: Multiple providers, custom stages, extensible architecture</li> <li>Well-Tested: 95%+ code coverage, integration tests with real APIs</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"contributing/","title":"Contributing to Ondine","text":"<p>Thank you for your interest in contributing to Ondine!</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Fork and Clone <pre><code>git clone https://github.com/YOUR_USERNAME/Ondine.git\ncd Ondine\n</code></pre></p> </li> <li> <p>Set Up Development Environment <pre><code># Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install dependencies\nuv sync --extra dev --extra observability\n\n# Install pre-commit hooks\nuv run pre-commit install\n</code></pre></p> </li> <li> <p>Create a Branch <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=ondine --cov-report=html\n\n# Run specific test file\nuv run pytest tests/unit/test_pipeline_builder.py\n\n# Run with verbose output\nuv run pytest -v\n</code></pre>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<pre><code># Format code\nuv run ruff format ondine/ tests/\n\n# Lint code\nuv run ruff check ondine/ tests/\n\n# Type check\nuv run mypy ondine/\n\n# Run all quality checks\njust lint\n</code></pre>"},{"location":"contributing/#using-justfile","title":"Using Justfile","text":"<p>We provide a <code>justfile</code> for common tasks:</p> <pre><code># Run tests\njust test\n\n# Run tests with coverage\njust test-coverage\n\n# Format and lint\njust format\njust lint\n\n# Run all checks\njust check\n\n# View all available commands\n   just --list\n</code></pre>"},{"location":"contributing/#code-guidelines","title":"Code Guidelines","text":""},{"location":"contributing/#style","title":"Style","text":"<ul> <li>Follow PEP 8 and the Zen of Python</li> <li>Use type hints for all function signatures</li> <li>Keep functions small and focused (KISS principle)</li> <li>Write self-documenting code with clear variable names</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features (TDD encouraged)</li> <li>Maintain or improve test coverage (currently 95%+)</li> <li>Include both unit and integration tests where appropriate</li> <li>Use descriptive test names: <code>test_&lt;what&gt;_&lt;when&gt;_&lt;expected&gt;</code></li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update README.md if adding user-facing features</li> <li>Add docstrings to all public functions and classes</li> <li>Include examples for new features in <code>examples/</code> directory</li> <li>Update CHANGELOG.md following Keep a Changelog</li> </ul>"},{"location":"contributing/#commits","title":"Commits","text":"<ul> <li>Write clear, descriptive commit messages</li> <li>Use conventional commits format:</li> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation changes</li> <li><code>test:</code> - Test additions or changes</li> <li><code>refactor:</code> - Code refactoring</li> <li><code>chore:</code> - Maintenance tasks</li> </ul> <p>Example: <pre><code>feat: add support for custom retry strategies\n\n- Implement RetryStrategy interface\n- Add exponential backoff with jitter\n- Update documentation\n</code></pre></p>"},{"location":"contributing/#architecture","title":"Architecture","text":"<p>Ondine follows a 5-layer clean architecture:</p> <ol> <li>Core - Domain models and business logic</li> <li>Adapters - External integrations (LLM clients, I/O)</li> <li>Stages - Pipeline processing stages</li> <li>Orchestration - Execution strategies and state management</li> <li>API - Public interfaces (builders, composers)</li> </ol>"},{"location":"contributing/#plugin-system","title":"Plugin System","text":"<p>Ondine uses decorators for extensibility:</p> <ul> <li><code>@provider</code> - Register custom LLM providers</li> <li><code>@stage</code> - Register custom pipeline stages</li> </ul> <p>See <code>examples/15_custom_llm_provider.py</code> and <code>examples/16_custom_pipeline_stage.py</code> for details.</p>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"contributing/#adding-a-new-llm-provider","title":"Adding a New LLM Provider","text":"<ol> <li>Create a new class inheriting from <code>BaseLLMProvider</code></li> <li>Implement <code>invoke()</code> and <code>estimate_tokens()</code> methods</li> <li>Register with <code>@provider(\"your_provider_name\")</code></li> <li>Add tests in <code>tests/unit/test_providers.py</code></li> <li>Add example in <code>examples/</code></li> <li>Update README.md</li> </ol>"},{"location":"contributing/#adding-a-new-pipeline-stage","title":"Adding a New Pipeline Stage","text":"<ol> <li>Create a new class inheriting from <code>PipelineStage</code></li> <li>Implement <code>execute()</code> method</li> <li>Register with <code>@stage(\"your_stage_name\")</code></li> <li>Add tests in <code>tests/unit/test_stages.py</code></li> <li>Add example in <code>examples/</code></li> <li>Update documentation</li> </ol>"},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<ol> <li>Check if the bug is already reported in Issues</li> <li>If not, create a new issue with:</li> <li>Clear title and description</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (OS, Python version, Ondine version)</li> <li>Minimal reproducible example</li> </ol>"},{"location":"contributing/#suggesting-features","title":"Suggesting Features","text":"<ol> <li>Check existing feature requests</li> <li>Open a new issue with:</li> <li>Clear use case description</li> <li>Proposed API or interface</li> <li>Example usage</li> <li>Why this would benefit users</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Before submitting:</li> <li>Ensure all tests pass: <code>just test</code></li> <li>Run code quality checks: <code>just check</code></li> <li>Update documentation if needed</li> <li> <p>Add entry to CHANGELOG.md</p> </li> <li> <p>Submit PR:</p> </li> <li>Write a clear title and description</li> <li>Reference related issues (e.g., \"Fixes #123\")</li> <li>Ensure CI passes (tests, linting, security)</li> <li> <p>Respond to code review feedback</p> </li> <li> <p>After approval:</p> </li> <li>Maintainers will merge your PR</li> <li>Your contribution will be included in the next release!</li> </ol>"},{"location":"contributing/#good-first-issues","title":"Good First Issues","text":"<p>Look for issues labeled <code>good first issue</code> - these are great for newcomers!</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Questions? Open a Discussion</li> <li>Bugs? Open an Issue</li> <li>Chat? Join our community (link coming soon)</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Focus on constructive feedback</li> <li>Help others learn and grow</li> </ul>"},{"location":"contributing/#thank-you","title":"Thank You!","text":"<p>Every contribution, no matter how small, makes Ondine better. We appreciate your time and effort!</p> <p>Happy coding!</p>"},{"location":"api/","title":"ondine","text":""},{"location":"api/#ondine","title":"ondine","text":"<p>LLM Dataset Processing Engine.</p> <p>A production-grade SDK for processing tabular datasets using Large Language Models with reliability, observability, and cost control.</p>"},{"location":"api/#ondine.DatasetProcessor","title":"DatasetProcessor","text":"<pre><code>DatasetProcessor(data: str | DataFrame, input_column: str, output_column: str, prompt: str, llm_config: dict[str, any])\n</code></pre> <p>Simplified API for single-prompt, single-column use cases.</p> <p>This is a convenience wrapper around PipelineBuilder for users who don't need fine-grained control.</p> Example <p>processor = DatasetProcessor(     data=\"data.csv\",     input_column=\"description\",     output_column=\"cleaned\",     prompt=\"Clean this text: {description}\",     llm_config={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\"} ) result = processor.run()</p> <p>Initialize dataset processor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>CSV file path or DataFrame</p> required <code>input_column</code> <code>str</code> <p>Input column name</p> required <code>output_column</code> <code>str</code> <p>Output column name</p> required <code>prompt</code> <code>str</code> <p>Prompt template</p> required <code>llm_config</code> <code>dict[str, any]</code> <p>LLM configuration dict</p> required Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    input_column: str,\n    output_column: str,\n    prompt: str,\n    llm_config: dict[str, any],\n):\n    \"\"\"\n    Initialize dataset processor.\n\n    Args:\n        data: CSV file path or DataFrame\n        input_column: Input column name\n        output_column: Output column name\n        prompt: Prompt template\n        llm_config: LLM configuration dict\n    \"\"\"\n    self.data = data\n    self.input_column = input_column\n    self.output_column = output_column\n    self.prompt = prompt\n    self.llm_config = llm_config\n\n    # Build pipeline internally\n    builder = PipelineBuilder.create()\n\n    # Configure data source\n    if isinstance(data, str):\n        builder.from_csv(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    elif isinstance(data, pd.DataFrame):\n        builder.from_dataframe(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    else:\n        raise ValueError(\"data must be file path or DataFrame\")\n\n    # Configure prompt\n    builder.with_prompt(prompt)\n\n    # Configure LLM\n    provider = llm_config.get(\"provider\", \"openai\")\n    model = llm_config.get(\"model\", \"gpt-4o-mini\")\n    api_key = llm_config.get(\"api_key\")\n    temperature = llm_config.get(\"temperature\", 0.0)\n    max_tokens = llm_config.get(\"max_tokens\")\n\n    builder.with_llm(\n        provider=provider,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    # Build pipeline\n    self.pipeline = builder.build()\n</code></pre>"},{"location":"api/#ondine.DatasetProcessor.run","title":"run","text":"<pre><code>run() -&gt; pd.DataFrame\n</code></pre> <p>Execute processing and return results.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute processing and return results.\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    result = self.pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/#ondine.DatasetProcessor.run_sample","title":"run_sample","text":"<pre><code>run_sample(n: int = 10) -&gt; pd.DataFrame\n</code></pre> <p>Test on first N rows.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of rows to process</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with sample results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run_sample(self, n: int = 10) -&gt; pd.DataFrame:\n    \"\"\"\n    Test on first N rows.\n\n    Args:\n        n: Number of rows to process\n\n    Returns:\n        DataFrame with sample results\n    \"\"\"\n    # Create sample pipeline\n    if isinstance(self.data, str):\n        df = pd.read_csv(self.data).head(n)\n    else:\n        df = self.data.head(n)\n\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df,\n            input_columns=[self.input_column],\n            output_columns=[self.output_column],\n        )\n        .with_prompt(self.prompt)\n        .with_llm(\n            provider=self.llm_config.get(\"provider\", \"openai\"),\n            model=self.llm_config.get(\"model\", \"gpt-4o-mini\"),\n            api_key=self.llm_config.get(\"api_key\"),\n            temperature=self.llm_config.get(\"temperature\", 0.0),\n        )\n    )\n\n    sample_pipeline = builder.build()\n    result = sample_pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/#ondine.DatasetProcessor.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; float\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost in USD</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def estimate_cost(self) -&gt; float:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Estimated cost in USD\n    \"\"\"\n    estimate = self.pipeline.estimate_cost()\n    return float(estimate.total_cost)\n</code></pre>"},{"location":"api/#ondine.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(specifications: PipelineSpecifications, dataframe: DataFrame | None = None, executor: ExecutionStrategy | None = None)\n</code></pre> <p>Main pipeline class - Facade for dataset processing.</p> <p>Provides high-level interface for building and executing LLM-powered data transformations.</p> Example <p>pipeline = Pipeline(specifications) result = pipeline.execute()</p> <p>Initialize pipeline with specifications.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Complete pipeline configuration</p> required <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded DataFrame</p> <code>None</code> <code>executor</code> <code>ExecutionStrategy | None</code> <p>Optional execution strategy (default: SyncExecutor)</p> <code>None</code> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def __init__(\n    self,\n    specifications: PipelineSpecifications,\n    dataframe: pd.DataFrame | None = None,\n    executor: ExecutionStrategy | None = None,\n):\n    \"\"\"\n    Initialize pipeline with specifications.\n\n    Args:\n        specifications: Complete pipeline configuration\n        dataframe: Optional pre-loaded DataFrame\n        executor: Optional execution strategy (default: SyncExecutor)\n    \"\"\"\n    self.id = uuid4()\n    self.specifications = specifications\n    self.dataframe = dataframe\n    self.executor = executor or SyncExecutor()\n    self.observers: list[ExecutionObserver] = []\n    self.logger = get_logger(f\"{__name__}.{self.id}\")\n</code></pre>"},{"location":"api/#ondine.Pipeline.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; Pipeline\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"Pipeline\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/#ondine.Pipeline.validate","title":"validate","text":"<pre><code>validate() -&gt; ValidationResult\n</code></pre> <p>Validate pipeline configuration.</p> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with any errors/warnings</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def validate(self) -&gt; ValidationResult:\n    \"\"\"\n    Validate pipeline configuration.\n\n    Returns:\n        ValidationResult with any errors/warnings\n    \"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Validate dataset spec\n    if not self.specifications.dataset.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    if not self.specifications.dataset.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    # Validate that input columns exist in dataframe (if dataframe is provided)\n    if self.dataframe is not None and self.specifications.dataset.input_columns:\n        df_cols = set(self.dataframe.columns)\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_cols = input_cols - df_cols\n        if missing_cols:\n            result.add_error(\n                f\"Input columns not found in dataframe: {missing_cols}\"\n            )\n\n    # Validate prompt spec\n    if not self.specifications.prompt.template:\n        result.add_error(\"No prompt template specified\")\n    else:\n        # Check that template variables match input columns\n        import re\n\n        template_vars = set(\n            re.findall(r\"\\{(\\w+)\\}\", self.specifications.prompt.template)\n        )\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_vars = template_vars - input_cols\n        if missing_vars:\n            result.add_error(\n                f\"Template variables not in input columns: {missing_vars}\"\n            )\n\n    # Validate LLM spec\n    if not self.specifications.llm.model:\n        result.add_error(\"No LLM model specified\")\n\n    return result\n</code></pre>"},{"location":"api/#ondine.Pipeline.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; CostEstimate\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def estimate_cost(self) -&gt; CostEstimate:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    # Create stages\n    loader = DataLoaderStage(self.dataframe)\n\n    # Load first few rows for estimation\n    df = loader.process(self.specifications.dataset, ExecutionContext())\n    sample_size = min(10, len(df))\n    sample_df = df.head(sample_size)\n\n    # Create formatter and get prompts\n    formatter = PromptFormatterStage(self.specifications.processing.batch_size)\n    batches = formatter.process(\n        (sample_df, self.specifications.prompt), ExecutionContext()\n    )\n\n    # Create LLM client and estimate\n    llm_client = create_llm_client(self.specifications.llm)\n    llm_stage = LLMInvocationStage(llm_client)\n\n    sample_estimate = llm_stage.estimate_cost(batches)\n\n    # Scale to full dataset\n    scale_factor = Decimal(len(df)) / Decimal(sample_size)\n\n    return CostEstimate(\n        total_cost=sample_estimate.total_cost * scale_factor,\n        total_tokens=int(sample_estimate.total_tokens * float(scale_factor)),\n        input_tokens=int(sample_estimate.input_tokens * float(scale_factor)),\n        output_tokens=int(sample_estimate.output_tokens * float(scale_factor)),\n        rows=len(df),\n        confidence=\"sample-based\",\n    )\n</code></pre>"},{"location":"api/#ondine.Pipeline.execute","title":"execute","text":"<pre><code>execute(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    # Validate first\n    validation = self.validate()\n    if not validation.is_valid:\n        raise ValueError(f\"Pipeline validation failed: {validation.errors}\")\n\n    # Create or restore execution context\n    state_manager = StateManager(\n        storage=LocalFileCheckpointStorage(\n            self.specifications.processing.checkpoint_dir\n        ),\n        checkpoint_interval=self.specifications.processing.checkpoint_interval,\n    )\n\n    if resume_from:\n        # Resume from checkpoint\n        context = state_manager.load_checkpoint(resume_from)\n        if not context:\n            raise ValueError(f\"No checkpoint found for session {resume_from}\")\n        self.logger.info(\n            f\"Resuming from checkpoint at row {context.last_processed_row}\"\n        )\n    else:\n        # Create new context\n        context = ExecutionContext(pipeline_id=self.id)\n\n    # Add default observers if none specified\n    if not self.observers:\n        self.observers = [\n            ProgressBarObserver(),\n            LoggingObserver(),\n            CostTrackingObserver(),\n        ]\n\n    # Attach observers to context for progress notifications\n    context.observers = self.observers\n\n    # Notify observers of start\n    for observer in self.observers:\n        observer.on_pipeline_start(self, context)\n\n    try:\n        # Execute stages (preprocessing happens inside if enabled)\n        result_df = self._execute_stages(context, state_manager)\n\n        # Mark completion\n        context.end_time = datetime.now()\n\n        # Create execution result\n        result = ExecutionResult(\n            data=result_df,\n            metrics=context.get_stats(),\n            costs=CostEstimate(\n                total_cost=context.accumulated_cost,\n                total_tokens=context.accumulated_tokens,\n                input_tokens=0,\n                output_tokens=0,\n                rows=context.total_rows,\n                confidence=\"actual\",\n            ),\n            execution_id=context.session_id,\n            start_time=context.start_time,\n            end_time=context.end_time,\n            success=True,\n        )\n\n        # Optional: Auto-retry failed rows\n        if self.specifications.processing.auto_retry_failed:\n            # Get preprocessed data from context (or loaded data if no preprocessing)\n            retry_source_df = context.intermediate_data.get(\"preprocessed_data\")\n            if retry_source_df is None:\n                retry_source_df = context.intermediate_data.get(\"loaded_data\")\n            result = self._auto_retry_failed_rows(result, retry_source_df)\n\n        # Cleanup checkpoints on success\n        state_manager.cleanup_checkpoints(context.session_id)\n\n        # Notify observers of completion\n        for observer in self.observers:\n            observer.on_pipeline_complete(context, result)\n\n        return result\n\n    except Exception as e:\n        # Save checkpoint on error\n        state_manager.save_checkpoint(context)\n        self.logger.error(\n            f\"Pipeline failed. Checkpoint saved. \"\n            f\"Resume with: pipeline.execute(resume_from=UUID('{context.session_id}'))\"\n        )\n\n        # Notify observers of error\n        for observer in self.observers:\n            observer.on_pipeline_error(context, e)\n        raise\n</code></pre>"},{"location":"api/#ondine.Pipeline.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline asynchronously.</p> <p>Uses AsyncExecutor for non-blocking execution. Ideal for integration with FastAPI, aiohttp, and other async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support async</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>async def execute_async(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline asynchronously.\n\n    Uses AsyncExecutor for non-blocking execution. Ideal for integration\n    with FastAPI, aiohttp, and other async frameworks.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        ValueError: If executor doesn't support async\n    \"\"\"\n    if not self.executor.supports_async():\n        raise ValueError(\n            \"Current executor doesn't support async. \"\n            \"Use AsyncExecutor: Pipeline(specs, executor=AsyncExecutor())\"\n        )\n\n    # For now, wrap synchronous execution in async\n    # TODO: Implement fully async execution pipeline\n    import asyncio\n\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, self.execute, resume_from)\n</code></pre>"},{"location":"api/#ondine.Pipeline.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(chunk_size: int | None = None) -&gt; Iterator[ExecutionResult]\n</code></pre> <p>Execute pipeline in streaming mode.</p> <p>Processes data in chunks for memory-efficient handling of large datasets. Ideal for datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int | None</code> <p>Number of rows per chunk (uses executor's chunk_size if None)</p> <code>None</code> <p>Yields:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult objects for each processed chunk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support streaming</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute_stream(\n    self, chunk_size: int | None = None\n) -&gt; Iterator[ExecutionResult]:\n    \"\"\"\n    Execute pipeline in streaming mode.\n\n    Processes data in chunks for memory-efficient handling of large datasets.\n    Ideal for datasets that don't fit in memory.\n\n    Args:\n        chunk_size: Number of rows per chunk (uses executor's chunk_size if None)\n\n    Yields:\n        ExecutionResult objects for each processed chunk\n\n    Raises:\n        ValueError: If executor doesn't support streaming\n    \"\"\"\n    if not self.executor.supports_streaming():\n        raise ValueError(\n            \"Current executor doesn't support streaming. \"\n            \"Use StreamingExecutor: Pipeline(specs, executor=StreamingExecutor())\"\n        )\n\n    # Use executor's chunk_size if not provided\n    if chunk_size is None and isinstance(self.executor, StreamingExecutor):\n        chunk_size = self.executor.chunk_size\n    elif chunk_size is None:\n        chunk_size = 1000  # Default fallback\n\n    # For now, execute the full pipeline and split result into chunks\n    # TODO: Implement proper streaming execution that processes chunks independently\n    result = self.execute()\n\n    # Split the result data into chunks and yield as separate ExecutionResults\n    total_rows = len(result.data)\n    for start_idx in range(0, total_rows, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_data = result.data.iloc[start_idx:end_idx].copy()\n\n        # Create a chunk result with proportional metrics\n        chunk_rows = len(chunk_data)\n        chunk_result = ExecutionResult(\n            data=chunk_data,\n            metrics=ProcessingStats(\n                total_rows=chunk_rows,\n                processed_rows=chunk_rows,\n                failed_rows=0,\n                skipped_rows=0,\n                rows_per_second=result.metrics.rows_per_second,\n                total_duration_seconds=result.metrics.total_duration_seconds\n                * (chunk_rows / total_rows),\n                stage_durations=result.metrics.stage_durations,\n            ),\n            costs=CostEstimate(\n                total_cost=result.costs.total_cost\n                * Decimal(chunk_rows / total_rows),\n                total_tokens=int(\n                    result.costs.total_tokens * (chunk_rows / total_rows)\n                ),\n                input_tokens=int(\n                    result.costs.input_tokens * (chunk_rows / total_rows)\n                ),\n                output_tokens=int(\n                    result.costs.output_tokens * (chunk_rows / total_rows)\n                ),\n                rows=chunk_rows,\n                confidence=result.costs.confidence,\n            ),\n            execution_id=result.execution_id,\n            start_time=result.start_time,\n            end_time=result.end_time,\n            success=True,\n        )\n        yield chunk_result\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder","title":"PipelineBuilder","text":"<pre><code>PipelineBuilder()\n</code></pre> <p>Fluent builder for constructing pipelines.</p> <p>Provides an intuitive, chainable API for common use cases.</p> Example <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p> <p>Initialize builder with None values.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with None values.\"\"\"\n    self._dataset_spec: DatasetSpec | None = None\n    self._prompt_spec: PromptSpec | None = None\n    self._llm_spec: LLMSpec | None = None\n    self._processing_spec: ProcessingSpec = ProcessingSpec()\n    self._output_spec: OutputSpec | None = None\n    self._dataframe: pd.DataFrame | None = None\n    self._executor: ExecutionStrategy | None = None\n    self._custom_parser: any | None = None\n    self._custom_llm_client: any | None = None\n    self._custom_stages: list[dict] = []  # For custom stage injection\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create() -&gt; PipelineBuilder\n</code></pre> <p>Start builder chain.</p> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>New PipelineBuilder instance</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef create() -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Start builder chain.\n\n    Returns:\n        New PipelineBuilder instance\n    \"\"\"\n    return PipelineBuilder()\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_specifications","title":"from_specifications  <code>staticmethod</code>","text":"<pre><code>from_specifications(specs: PipelineSpecifications) -&gt; PipelineBuilder\n</code></pre> <p>Create builder from existing specifications.</p> <p>Useful for loading from YAML and modifying programmatically.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>PipelineSpecifications</code> <p>Complete pipeline specifications</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>PipelineBuilder pre-configured with specs</p> Example <p>specs = load_pipeline_config(\"config.yaml\") builder = PipelineBuilder.from_specifications(specs) pipeline = builder.build()</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef from_specifications(specs: PipelineSpecifications) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Create builder from existing specifications.\n\n    Useful for loading from YAML and modifying programmatically.\n\n    Args:\n        specs: Complete pipeline specifications\n\n    Returns:\n        PipelineBuilder pre-configured with specs\n\n    Example:\n        specs = load_pipeline_config(\"config.yaml\")\n        builder = PipelineBuilder.from_specifications(specs)\n        pipeline = builder.build()\n    \"\"\"\n    builder = PipelineBuilder()\n    builder._dataset_spec = specs.dataset\n    builder._prompt_spec = specs.prompt\n    builder._llm_spec = specs.llm\n    builder._processing_spec = specs.processing\n    builder._output_spec = specs.output\n    return builder\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_csv","title":"from_csv","text":"<pre><code>from_csv(path: str, input_columns: list[str], output_columns: list[str], delimiter: str = ',', encoding: str = 'utf-8') -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_csv(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV data source.\n\n    Args:\n        path: Path to CSV file\n        input_columns: Input column names\n        output_columns: Output column names\n        delimiter: CSV delimiter\n        encoding: File encoding\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.CSV,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        delimiter=delimiter,\n        encoding=encoding,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_excel","title":"from_excel","text":"<pre><code>from_excel(path: str, input_columns: list[str], output_columns: list[str], sheet_name: str | int = 0) -&gt; PipelineBuilder\n</code></pre> <p>Configure Excel data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Excel file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_excel(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    sheet_name: str | int = 0,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Excel data source.\n\n    Args:\n        path: Path to Excel file\n        input_columns: Input column names\n        output_columns: Output column names\n        sheet_name: Sheet name or index\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.EXCEL,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        sheet_name=sheet_name,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_parquet","title":"from_parquet","text":"<pre><code>from_parquet(path: str, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure Parquet data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_parquet(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Parquet data source.\n\n    Args:\n        path: Path to Parquet file\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.PARQUET,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.from_dataframe","title":"from_dataframe","text":"<pre><code>from_dataframe(df: DataFrame, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure DataFrame source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_dataframe(\n    self,\n    df: pd.DataFrame,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure DataFrame source.\n\n    Args:\n        df: Pandas DataFrame\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.DATAFRAME,\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    self._dataframe = df\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_prompt","title":"with_prompt","text":"<pre><code>with_prompt(template: str, system_message: str | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Configure prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {variable} placeholders</p> required <code>system_message</code> <code>str | None</code> <p>Optional system message</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_prompt(\n    self,\n    template: str,\n    system_message: str | None = None,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure prompt template.\n\n    Args:\n        template: Prompt template with {variable} placeholders\n        system_message: Optional system message\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._prompt_spec = PromptSpec(\n        template=template,\n        system_message=system_message,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_llm","title":"with_llm","text":"<pre><code>with_llm(provider: str, model: str, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, **kwargs: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (openai, azure_openai, anthropic) or custom provider ID</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>api_key</code> <code>str | None</code> <p>API key (or from env)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm(\n    self,\n    provider: str,\n    model: str,\n    api_key: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    **kwargs: any,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM provider.\n\n    Args:\n        provider: Provider name (openai, azure_openai, anthropic) or custom provider ID\n        model: Model identifier\n        api_key: API key (or from env)\n        temperature: Sampling temperature\n        max_tokens: Max output tokens\n        **kwargs: Provider-specific parameters\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Try to convert to enum for built-in providers\n    try:\n        provider_enum = LLMProvider(provider.lower())\n    except ValueError:\n        # Not a built-in provider - check if it's a custom provider\n        if ProviderRegistry.is_registered(provider):\n            # Use a dummy enum value for validation, but store the actual provider string\n            provider_enum = LLMProvider.OPENAI  # Dummy for Pydantic validation\n            kwargs[\"_custom_provider_id\"] = provider\n        else:\n            raise ValueError(\n                f\"Unknown provider: {provider}. \"\n                f\"Available providers: {', '.join(ProviderRegistry.list_providers())}\"\n            )\n\n    self._llm_spec = LLMSpec(\n        provider=provider_enum,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec","title":"with_llm_spec","text":"<pre><code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM using a pre-built LLMSpec object.</p> <p>This method allows using LLMSpec objects directly, enabling: - Reusable provider configurations - Use of LLMProviderPresets for common providers - Custom LLMSpec instances for advanced use cases</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification object</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If spec is not an LLMSpec instance</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm_spec(self, spec: LLMSpec) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM using a pre-built LLMSpec object.\n\n    This method allows using LLMSpec objects directly, enabling:\n    - Reusable provider configurations\n    - Use of LLMProviderPresets for common providers\n    - Custom LLMSpec instances for advanced use cases\n\n    Args:\n        spec: LLM specification object\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        TypeError: If spec is not an LLMSpec instance\n\n    Example:\n        # Use preset\n        from ondine.core.specifications import LLMProviderPresets\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Process: {text}\")\n            .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n            .build()\n        )\n\n        # Custom spec\n        custom = LLMSpec(\n            provider=LLMProvider.OPENAI,\n            model=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n        pipeline.with_llm_spec(custom)\n\n        # Override preset\n        spec = LLMProviderPresets.GPT4O_MINI.model_copy(\n            update={\"temperature\": 0.9}\n        )\n        pipeline.with_llm_spec(spec)\n    \"\"\"\n    if not isinstance(spec, LLMSpec):\n        raise TypeError(\n            f\"Expected LLMSpec, got {type(spec).__name__}. \"\n            f\"Use with_llm() for parameter-based configuration.\"\n        )\n\n    self._llm_spec = spec\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec--use-preset","title":"Use preset","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec--custom-spec","title":"Custom spec","text":"<p>custom = LLMSpec(     provider=LLMProvider.OPENAI,     model=\"gpt-4o-mini\",     temperature=0.7 ) pipeline.with_llm_spec(custom)</p>"},{"location":"api/#ondine.PipelineBuilder.with_llm_spec--override-preset","title":"Override preset","text":"<p>spec = LLMProviderPresets.GPT4O_MINI.model_copy(     update={\"temperature\": 0.9} ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/#ondine.PipelineBuilder.with_custom_llm_client","title":"with_custom_llm_client","text":"<pre><code>with_custom_llm_client(client: any) -&gt; PipelineBuilder\n</code></pre> <p>Provide a custom LLM client instance directly.</p> <p>This allows advanced users to create their own LLM client implementations by extending the LLMClient base class. The custom client will be used instead of the factory-created client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>any</code> <p>Custom LLM client instance (must inherit from LLMClient)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <p>class MyCustomClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         # Custom implementation         ...</p> <p>pipeline = (     PipelineBuilder.create()     .from_dataframe(df, ...)     .with_prompt(\"...\")     .with_custom_llm_client(MyCustomClient(spec))     .build() )</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_custom_llm_client(self, client: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Provide a custom LLM client instance directly.\n\n    This allows advanced users to create their own LLM client implementations\n    by extending the LLMClient base class. The custom client will be used\n    instead of the factory-created client.\n\n    Args:\n        client: Custom LLM client instance (must inherit from LLMClient)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        class MyCustomClient(LLMClient):\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                # Custom implementation\n                ...\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_dataframe(df, ...)\n            .with_prompt(\"...\")\n            .with_custom_llm_client(MyCustomClient(spec))\n            .build()\n        )\n    \"\"\"\n    from ondine.adapters.llm_client import LLMClient\n\n    if not isinstance(client, LLMClient):\n        raise TypeError(\n            f\"Custom client must inherit from LLMClient, got {type(client).__name__}\"\n        )\n\n    self._custom_llm_client = client\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_batch_size","title":"with_batch_size","text":"<pre><code>with_batch_size(size: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure batch size.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Rows per batch</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_size(self, size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure batch size.\n\n    Args:\n        size: Rows per batch\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.batch_size = size\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_concurrency","title":"with_concurrency","text":"<pre><code>with_concurrency(threads: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure concurrent requests.</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int</code> <p>Number of concurrent threads</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_concurrency(self, threads: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure concurrent requests.\n\n    Args:\n        threads: Number of concurrent threads\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.concurrency = threads\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_checkpoint_interval","title":"with_checkpoint_interval","text":"<pre><code>with_checkpoint_interval(rows: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint frequency.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Rows between checkpoints</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_interval(self, rows: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint frequency.\n\n    Args:\n        rows: Rows between checkpoints\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_interval = rows\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_rate_limit","title":"with_rate_limit","text":"<pre><code>with_rate_limit(rpm: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure rate limiting.</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_rate_limit(self, rpm: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure rate limiting.\n\n    Args:\n        rpm: Requests per minute\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.rate_limit_rpm = rpm\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_max_retries","title":"with_max_retries","text":"<pre><code>with_max_retries(retries: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>Maximum number of retry attempts</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_retries(self, retries: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum retry attempts.\n\n    Args:\n        retries: Maximum number of retry attempts\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_retries = retries\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_max_budget","title":"with_max_budget","text":"<pre><code>with_max_budget(budget: float) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget</code> <code>float</code> <p>Maximum budget in USD</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_budget(self, budget: float) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum budget.\n\n    Args:\n        budget: Maximum budget in USD\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_budget = Decimal(str(budget))\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_error_policy","title":"with_error_policy","text":"<pre><code>with_error_policy(policy: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure error handling policy.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>Error policy ('skip', 'fail', 'retry', 'use_default')</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_error_policy(self, policy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure error handling policy.\n\n    Args:\n        policy: Error policy ('skip', 'fail', 'retry', 'use_default')\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.core.specifications import ErrorPolicy\n\n    self._processing_spec.error_policy = ErrorPolicy(policy.lower())\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_checkpoint_dir","title":"with_checkpoint_dir","text":"<pre><code>with_checkpoint_dir(directory: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to checkpoint directory</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_dir(self, directory: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint directory.\n\n    Args:\n        directory: Path to checkpoint directory\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_dir = Path(directory)\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_parser","title":"with_parser","text":"<pre><code>with_parser(parser: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure response parser.</p> <p>This method allows setting a custom parser. The parser type determines the response_format in the prompt spec.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>any</code> <p>Parser instance (JSONParser, RegexParser, PydanticParser, etc.)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_parser(self, parser: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure response parser.\n\n    This method allows setting a custom parser. The parser type\n    determines the response_format in the prompt spec.\n\n    Args:\n        parser: Parser instance (JSONParser, RegexParser, PydanticParser, etc.)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    # Store the parser for later use in the pipeline\n    # We'll configure response_format based on parser type\n    if hasattr(parser, \"__class__\"):\n        parser_name = parser.__class__.__name__\n        if \"JSON\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            # Update the existing prompt spec's response_format\n            self._prompt_spec.response_format = \"json\"\n        elif \"Regex\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            self._prompt_spec.response_format = \"regex\"\n            if hasattr(parser, \"patterns\"):\n                self._prompt_spec.regex_patterns = parser.patterns\n\n    # Store the parser instance in metadata for the pipeline to use\n    if not hasattr(self, \"_custom_parser\"):\n        self._custom_parser = parser\n\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.to_csv","title":"to_csv","text":"<pre><code>to_csv(path: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV output destination.</p> <p>Alias for with_output(path, format='csv').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output CSV file path</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def to_csv(self, path: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV output destination.\n\n    Alias for with_output(path, format='csv').\n\n    Args:\n        path: Output CSV file path\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    return self.with_output(path, format=\"csv\")\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_output","title":"with_output","text":"<pre><code>with_output(path: str, format: str = 'csv', merge_strategy: str = 'replace') -&gt; PipelineBuilder\n</code></pre> <p>Configure output destination.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path</p> required <code>format</code> <code>str</code> <p>Output format (csv, excel, parquet)</p> <code>'csv'</code> <code>merge_strategy</code> <code>str</code> <p>Merge strategy (replace, append, update)</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_output(\n    self,\n    path: str,\n    format: str = \"csv\",\n    merge_strategy: str = \"replace\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure output destination.\n\n    Args:\n        path: Output file path\n        format: Output format (csv, excel, parquet)\n        merge_strategy: Merge strategy (replace, append, update)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    format_map = {\n        \"csv\": DataSourceType.CSV,\n        \"excel\": DataSourceType.EXCEL,\n        \"parquet\": DataSourceType.PARQUET,\n    }\n\n    merge_map = {\n        \"replace\": MergeStrategy.REPLACE,\n        \"append\": MergeStrategy.APPEND,\n        \"update\": MergeStrategy.UPDATE,\n    }\n\n    self._output_spec = OutputSpec(\n        destination_type=format_map[format.lower()],\n        destination_path=Path(path),\n        merge_strategy=merge_map[merge_strategy.lower()],\n    )\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_executor","title":"with_executor","text":"<pre><code>with_executor(executor: ExecutionStrategy) -&gt; PipelineBuilder\n</code></pre> <p>Set custom execution strategy.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ExecutionStrategy</code> <p>ExecutionStrategy instance</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_executor(self, executor: ExecutionStrategy) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set custom execution strategy.\n\n    Args:\n        executor: ExecutionStrategy instance\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = executor\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_async_execution","title":"with_async_execution","text":"<pre><code>with_async_execution(max_concurrency: int = 10) -&gt; PipelineBuilder\n</code></pre> <p>Use async execution strategy.</p> <p>Enables async/await for non-blocking execution. Ideal for FastAPI, aiohttp, and async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_async_execution(self, max_concurrency: int = 10) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use async execution strategy.\n\n    Enables async/await for non-blocking execution.\n    Ideal for FastAPI, aiohttp, and async frameworks.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = AsyncExecutor(max_concurrency=max_concurrency)\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_streaming","title":"with_streaming","text":"<pre><code>with_streaming(chunk_size: int = 1000) -&gt; PipelineBuilder\n</code></pre> <p>Use streaming execution strategy.</p> <p>Processes data in chunks for memory-efficient handling. Ideal for large datasets (100K+ rows).</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_streaming(self, chunk_size: int = 1000) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use streaming execution strategy.\n\n    Processes data in chunks for memory-efficient handling.\n    Ideal for large datasets (100K+ rows).\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = StreamingExecutor(chunk_size=chunk_size)\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_stage","title":"with_stage","text":"<pre><code>with_stage(stage_name: str, position: str = 'before_prompt', **stage_kwargs) -&gt; PipelineBuilder\n</code></pre> <p>Add a custom pipeline stage by name.</p> <p>Enables injection of custom processing stages at specific points in the pipeline. Stages must be registered via StageRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Registered stage name (e.g., \"rag_retrieval\")</p> required <code>position</code> <code>str</code> <p>Where to inject the stage. Options: - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing - \"after_parser\": After response parsing</p> <code>'before_prompt'</code> <code>**stage_kwargs</code> <p>Arguments to pass to stage constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name not registered or position invalid</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_stage(\n    self,\n    stage_name: str,\n    position: str = \"before_prompt\",\n    **stage_kwargs,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add a custom pipeline stage by name.\n\n    Enables injection of custom processing stages at specific points\n    in the pipeline. Stages must be registered via StageRegistry.\n\n    Args:\n        stage_name: Registered stage name (e.g., \"rag_retrieval\")\n        position: Where to inject the stage. Options:\n            - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting\n            - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation\n            - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing\n            - \"after_parser\": After response parsing\n        **stage_kwargs: Arguments to pass to stage constructor\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If stage_name not registered or position invalid\n\n    Example:\n        # RAG retrieval example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])\n            .with_stage(\n                \"rag_retrieval\",\n                position=\"before_prompt\",\n                vector_store=\"pinecone\",\n                index_name=\"my-docs\",\n                top_k=5\n            )\n            .with_prompt(\"Context: {retrieved_context}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o\")\n            .build()\n        )\n\n        # Content moderation example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])\n            .with_stage(\n                \"content_moderation\",\n                position=\"before_llm\",\n                block_patterns=[\"spam\", \"offensive\"]\n            )\n            .with_prompt(\"Moderate: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n    \"\"\"\n    from ondine.stages.stage_registry import StageRegistry\n\n    # Validate position\n    valid_positions = [\n        \"after_loader\",\n        \"before_prompt\",\n        \"after_prompt\",\n        \"before_llm\",\n        \"after_llm\",\n        \"before_parser\",\n        \"after_parser\",\n    ]\n    if position not in valid_positions:\n        raise ValueError(\n            f\"Invalid position '{position}'. Must be one of: {', '.join(valid_positions)}\"\n        )\n\n    # Get stage class from registry (this will raise ValueError if not found)\n    stage_class = StageRegistry.get(stage_name)\n\n    # Store stage config for later instantiation\n    self._custom_stages.append(\n        {\n            \"name\": stage_name,\n            \"class\": stage_class,\n            \"position\": position,\n            \"kwargs\": stage_kwargs,\n        }\n    )\n\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineBuilder.with_stage--rag-retrieval-example","title":"RAG retrieval example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])     .with_stage(         \"rag_retrieval\",         position=\"before_prompt\",         vector_store=\"pinecone\",         index_name=\"my-docs\",         top_k=5     )     .with_prompt(\"Context: {retrieved_context}\\n\\nQuestion: {question}\\n\\nAnswer:\")     .with_llm(provider=\"openai\", model=\"gpt-4o\")     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.with_stage--content-moderation-example","title":"Content moderation example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])     .with_stage(         \"content_moderation\",         position=\"before_llm\",         block_patterns=[\"spam\", \"offensive\"]     )     .with_prompt(\"Moderate: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p>"},{"location":"api/#ondine.PipelineBuilder.build","title":"build","text":"<pre><code>build() -&gt; Pipeline\n</code></pre> <p>Build final Pipeline.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required specifications missing</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def build(self) -&gt; Pipeline:\n    \"\"\"\n    Build final Pipeline.\n\n    Returns:\n        Configured Pipeline\n\n    Raises:\n        ValueError: If required specifications missing\n    \"\"\"\n    # Validate required specs\n    if not self._dataset_spec:\n        raise ValueError(\"Dataset specification required\")\n    if not self._prompt_spec:\n        raise ValueError(\"Prompt specification required\")\n\n    # LLM spec is optional if custom client is provided\n    if not self._llm_spec and not self._custom_llm_client:\n        raise ValueError(\"Either LLM specification or custom LLM client required\")\n\n    # Prepare metadata with custom parser, custom client, and/or custom stages if provided\n    metadata = {}\n    if self._custom_parser is not None:\n        metadata[\"custom_parser\"] = self._custom_parser\n    if self._custom_llm_client is not None:\n        metadata[\"custom_llm_client\"] = self._custom_llm_client\n    if self._custom_stages:\n        metadata[\"custom_stages\"] = self._custom_stages\n\n    # Create specifications bundle\n    # If custom client provided but no llm_spec, create a dummy spec\n    llm_spec = self._llm_spec\n    if llm_spec is None and self._custom_llm_client is not None:\n        # Create minimal spec using custom client's attributes\n        llm_spec = LLMSpec(\n            provider=LLMProvider.OPENAI,  # Dummy provider\n            model=self._custom_llm_client.model,\n            temperature=self._custom_llm_client.temperature,\n            max_tokens=self._custom_llm_client.max_tokens,\n        )\n\n    specifications = PipelineSpecifications(\n        dataset=self._dataset_spec,\n        prompt=self._prompt_spec,\n        llm=llm_spec,\n        processing=self._processing_spec,\n        output=self._output_spec,\n        metadata=metadata,\n    )\n\n    # Create and return pipeline\n    return Pipeline(\n        specifications,\n        dataframe=self._dataframe,\n        executor=self._executor,\n    )\n</code></pre>"},{"location":"api/#ondine.QuickPipeline","title":"QuickPipeline","text":"<p>Simplified pipeline API with smart defaults.</p> <p>Designed for rapid prototyping and common use cases. Automatically detects: - Input columns from prompt template placeholders - Provider from model name (e.g., gpt-4 \u2192 openai, claude \u2192 anthropic) - Parser type (JSON for multi-column, text for single column) - Reasonable defaults for batch size, concurrency, retries</p> <p>Examples:</p> <p>Minimal usage:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"data.csv\",\n...     prompt=\"Categorize this text: {text}\"\n... )\n&gt;&gt;&gt; result = pipeline.execute()\n</code></pre> <p>With explicit outputs:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"products.csv\",\n...     prompt=\"Extract: {description}\",\n...     output_columns=[\"brand\", \"model\", \"price\"]\n... )\n</code></pre> <p>Override defaults:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=df,\n...     prompt=\"Summarize: {content}\",\n...     model=\"gpt-4o\",\n...     temperature=0.7,\n...     max_budget=Decimal(\"5.0\")\n... )\n</code></pre>"},{"location":"api/#ondine.QuickPipeline.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(data: str | Path | DataFrame, prompt: str, model: str = 'gpt-4o-mini', output_columns: list[str] | str | None = None, provider: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, max_budget: Decimal | float | str | None = None, batch_size: int | None = None, concurrency: int | None = None, **kwargs: Any) -&gt; Pipeline\n</code></pre> <p>Create a pipeline with smart defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path | DataFrame</code> <p>CSV/Excel/Parquet file path or DataFrame</p> required <code>prompt</code> <code>str</code> <p>Prompt template with {placeholders}</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o-mini)</p> <code>'gpt-4o-mini'</code> <code>output_columns</code> <code>list[str] | str | None</code> <p>Output column name(s). If None, uses [\"output\"]</p> <code>None</code> <code>provider</code> <code>str | None</code> <p>LLM provider. If None, auto-detected from model name</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens (default: provider's default)</p> <code>None</code> <code>max_budget</code> <code>Decimal | float | str | None</code> <p>Maximum cost budget in USD</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Rows per batch (default: auto-sized based on data)</p> <code>None</code> <code>concurrency</code> <code>int | None</code> <p>Parallel requests (default: auto-sized)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to PipelineBuilder</p> <code>{}</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data cannot be loaded or prompt is invalid</p> Source code in <code>ondine/api/quick.py</code> <pre><code>@staticmethod\ndef create(\n    data: str | Path | pd.DataFrame,\n    prompt: str,\n    model: str = \"gpt-4o-mini\",\n    output_columns: list[str] | str | None = None,\n    provider: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    max_budget: Decimal | float | str | None = None,\n    batch_size: int | None = None,\n    concurrency: int | None = None,\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"\n    Create a pipeline with smart defaults.\n\n    Args:\n        data: CSV/Excel/Parquet file path or DataFrame\n        prompt: Prompt template with {placeholders}\n        model: Model name (default: gpt-4o-mini)\n        output_columns: Output column name(s). If None, uses [\"output\"]\n        provider: LLM provider. If None, auto-detected from model name\n        temperature: Sampling temperature (default: 0.0 for deterministic)\n        max_tokens: Max output tokens (default: provider's default)\n        max_budget: Maximum cost budget in USD\n        batch_size: Rows per batch (default: auto-sized based on data)\n        concurrency: Parallel requests (default: auto-sized)\n        **kwargs: Additional arguments passed to PipelineBuilder\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If input data cannot be loaded or prompt is invalid\n    \"\"\"\n    # 1. Load data\n    df = QuickPipeline._load_data(data)\n\n    # 2. Auto-detect input columns from prompt template\n    input_columns = QuickPipeline._extract_placeholders(prompt)\n    if not input_columns:\n        raise ValueError(\n            f\"No placeholders found in prompt: {prompt}\\n\"\n            \"Expected format: 'Your prompt with {{column_name}} placeholders'\"\n        )\n\n    # Validate input columns exist in data\n    missing = [col for col in input_columns if col not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Input columns {missing} not found in data. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    # 3. Normalize output columns\n    if output_columns is None:\n        output_columns = [\"output\"]\n    elif isinstance(output_columns, str):\n        output_columns = [output_columns]\n\n    # 4. Auto-detect provider from model name\n    if provider is None:\n        provider = QuickPipeline._detect_provider(model)\n\n    # 5. Auto-select parser (JSON for multi-column, text for single)\n    parser = QuickPipeline._select_parser(output_columns)\n\n    # 6. Smart defaults for batch_size and concurrency\n    if batch_size is None:\n        batch_size = QuickPipeline._default_batch_size(len(df))\n    if concurrency is None:\n        concurrency = QuickPipeline._default_concurrency(provider)\n\n    # 7. Build pipeline\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df, input_columns=input_columns, output_columns=output_columns\n        )\n        .with_prompt(template=prompt)\n        .with_llm(\n            provider=provider,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n    )\n\n    # Add optional parser if multi-column\n    if parser:\n        builder = builder.with_parser(parser)\n\n    # Add batch/concurrency settings\n    builder = builder.with_batch_size(batch_size).with_concurrency(concurrency)\n\n    # Add budget if specified\n    if max_budget is not None:\n        # Convert to float for PipelineBuilder (it expects float)\n        if isinstance(max_budget, Decimal | str):\n            max_budget = float(max_budget)\n        builder = builder.with_max_budget(budget=max_budget)\n\n    # Add sensible retry defaults\n    builder = builder.with_max_retries(3)\n\n    return builder.build()\n</code></pre>"},{"location":"api/#ondine.CostEstimate","title":"CostEstimate  <code>dataclass</code>","text":"<pre><code>CostEstimate(total_cost: Decimal, total_tokens: int, input_tokens: int, output_tokens: int, rows: int, breakdown_by_stage: dict[str, Decimal] = dict(), confidence: str = 'estimate')\n</code></pre> <p>Cost estimation for pipeline execution.</p>"},{"location":"api/#ondine.ExecutionResult","title":"ExecutionResult  <code>dataclass</code>","text":"<pre><code>ExecutionResult(data: DataFrame, metrics: ProcessingStats, costs: CostEstimate, errors: list[ErrorInfo] = list(), execution_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, success: bool = True, metadata: dict[str, Any] = dict())\n</code></pre> <p>Complete result from pipeline execution.</p>"},{"location":"api/#ondine.ExecutionResult.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Get execution duration in seconds.</p>"},{"location":"api/#ondine.ExecutionResult.error_rate","title":"error_rate  <code>property</code>","text":"<pre><code>error_rate: float\n</code></pre> <p>Get error rate as percentage.</p>"},{"location":"api/#ondine.ExecutionResult.validate_output_quality","title":"validate_output_quality","text":"<pre><code>validate_output_quality(output_columns: list[str]) -&gt; QualityReport\n</code></pre> <p>Validate the quality of output data by checking for null/empty values.</p> <p>Parameters:</p> Name Type Description Default <code>output_columns</code> <code>list[str]</code> <p>List of output column names to check</p> required <p>Returns:</p> Type Description <code>QualityReport</code> <p>QualityReport with quality metrics and warnings</p> Source code in <code>ondine/core/models.py</code> <pre><code>def validate_output_quality(self, output_columns: list[str]) -&gt; \"QualityReport\":\n    \"\"\"\n    Validate the quality of output data by checking for null/empty values.\n\n    Args:\n        output_columns: List of output column names to check\n\n    Returns:\n        QualityReport with quality metrics and warnings\n    \"\"\"\n    total = len(self.data)\n\n    # Count null and empty values across output columns\n    null_count = 0\n    empty_count = 0\n\n    for col in output_columns:\n        if col in self.data.columns:\n            # Count nulls (None, NaN, NaT)\n            null_count += self.data[col].isna().sum()\n            # Count empty strings (only for string columns)\n            if self.data[col].dtype == \"object\":\n                empty_count += (self.data[col].astype(str).str.strip() == \"\").sum()\n\n    # Calculate per-column metrics (exclude both nulls and empties)\n    valid_outputs = total - null_count - empty_count\n    success_rate = (valid_outputs / total * 100) if total &gt; 0 else 0.0\n\n    # Determine quality score\n    if success_rate &gt;= 95.0:\n        quality_score = \"excellent\"\n    elif success_rate &gt;= 80.0:\n        quality_score = \"good\"\n    elif success_rate &gt;= 50.0:\n        quality_score = \"poor\"\n    else:\n        quality_score = \"critical\"\n\n    # Generate warnings and issues\n    warnings = []\n    issues = []\n\n    if success_rate &lt; 70.0:\n        issues.append(\n            f\"\u26a0\ufe0f  LOW SUCCESS RATE: Only {success_rate:.1f}% of outputs are valid \"\n            f\"({valid_outputs}/{total} rows)\"\n        )\n\n    if null_count &gt; total * 0.3:  # &gt; 30% nulls\n        issues.append(\n            f\"\u26a0\ufe0f  HIGH NULL RATE: {null_count} null values found \"\n            f\"({null_count / total * 100:.1f}% of rows)\"\n        )\n\n    if empty_count &gt; total * 0.1:  # &gt; 10% empty\n        warnings.append(\n            f\"Empty outputs detected: {empty_count} rows \"\n            f\"({empty_count / total * 100:.1f}%)\"\n        )\n\n    # Check if reported metrics match actual data quality\n    if self.metrics.failed_rows == 0 and null_count &gt; 0:\n        issues.append(\n            f\"\u26a0\ufe0f  METRICS MISMATCH: Pipeline reported 0 failures but \"\n            f\"{null_count} rows have null outputs. This may indicate silent errors.\"\n        )\n\n    return QualityReport(\n        total_rows=total,\n        valid_outputs=valid_outputs,\n        null_outputs=null_count,\n        empty_outputs=empty_count,\n        success_rate=success_rate,\n        quality_score=quality_score,\n        warnings=warnings,\n        issues=issues,\n    )\n</code></pre>"},{"location":"api/#ondine.ProcessingStats","title":"ProcessingStats  <code>dataclass</code>","text":"<pre><code>ProcessingStats(total_rows: int, processed_rows: int, failed_rows: int, skipped_rows: int, rows_per_second: float, total_duration_seconds: float, stage_durations: dict[str, float] = dict())\n</code></pre> <p>Statistics from pipeline execution.</p>"},{"location":"api/#ondine.QualityReport","title":"QualityReport  <code>dataclass</code>","text":"<pre><code>QualityReport(total_rows: int, valid_outputs: int, null_outputs: int, empty_outputs: int, success_rate: float, quality_score: str, warnings: list[str] = list(), issues: list[str] = list())\n</code></pre> <p>Quality assessment of pipeline output.</p>"},{"location":"api/#ondine.QualityReport.is_acceptable","title":"is_acceptable  <code>property</code>","text":"<pre><code>is_acceptable: bool\n</code></pre> <p>Check if quality is acceptable (&gt;= 70% success).</p>"},{"location":"api/#ondine.QualityReport.has_issues","title":"has_issues  <code>property</code>","text":"<pre><code>has_issues: bool\n</code></pre> <p>Check if there are any issues.</p>"},{"location":"api/#ondine.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for data source configuration.</p>"},{"location":"api/#ondine.DatasetSpec.validate_source_path","title":"validate_source_path  <code>classmethod</code>","text":"<pre><code>validate_source_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"source_path\")\n@classmethod\ndef validate_source_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/#ondine.DatasetSpec.validate_no_overlap","title":"validate_no_overlap  <code>classmethod</code>","text":"<pre><code>validate_no_overlap(v: list[str], info: Any) -&gt; list[str]\n</code></pre> <p>Ensure output columns don't overlap with input columns.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"output_columns\")\n@classmethod\ndef validate_no_overlap(cls, v: list[str], info: Any) -&gt; list[str]:\n    \"\"\"Ensure output columns don't overlap with input columns.\"\"\"\n    if \"input_columns\" in info.data:\n        input_cols = set(info.data[\"input_columns\"])\n        output_cols = set(v)\n        overlap = input_cols &amp; output_cols\n        if overlap:\n            raise ValueError(f\"Output columns overlap with input: {overlap}\")\n    return v\n</code></pre>"},{"location":"api/#ondine.LLMSpec","title":"LLMSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for LLM provider configuration.</p>"},{"location":"api/#ondine.LLMSpec.validate_base_url_format","title":"validate_base_url_format  <code>classmethod</code>","text":"<pre><code>validate_base_url_format(v: str | None) -&gt; str | None\n</code></pre> <p>Validate base_url is a valid HTTP(S) URL with a host.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"base_url\")\n@classmethod\ndef validate_base_url_format(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate base_url is a valid HTTP(S) URL with a host.\"\"\"\n    if v is None:\n        return v\n    from urllib.parse import urlparse\n\n    parsed = urlparse(v)\n    if parsed.scheme not in {\"http\", \"https\"}:\n        raise ValueError(\"base_url must start with http:// or https://\")\n    if not parsed.netloc:\n        raise ValueError(\n            \"base_url must include a host (e.g., localhost, api.example.com)\"\n        )\n    return v\n</code></pre>"},{"location":"api/#ondine.LLMSpec.validate_azure_config","title":"validate_azure_config  <code>classmethod</code>","text":"<pre><code>validate_azure_config(v: str | None, info: Any) -&gt; str | None\n</code></pre> <p>Validate Azure-specific configuration.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"azure_endpoint\", \"azure_deployment\")\n@classmethod\ndef validate_azure_config(cls, v: str | None, info: Any) -&gt; str | None:\n    \"\"\"Validate Azure-specific configuration.\"\"\"\n    if info.data.get(\"provider\") == LLMProvider.AZURE_OPENAI and v is None:\n        field_name = info.field_name\n        raise ValueError(f\"{field_name} required for Azure OpenAI provider\")\n    return v\n</code></pre>"},{"location":"api/#ondine.LLMSpec.validate_provider_requirements","title":"validate_provider_requirements","text":"<pre><code>validate_provider_requirements() -&gt; LLMSpec\n</code></pre> <p>Validate provider-specific requirements.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    \"\"\"Validate provider-specific requirements.\"\"\"\n    # Check openai_compatible requires base_url\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre>"},{"location":"api/#ondine.PipelineSpecifications","title":"PipelineSpecifications","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for all pipeline specifications.</p>"},{"location":"api/#ondine.ProcessingSpec","title":"ProcessingSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for processing parameters.</p>"},{"location":"api/#ondine.ProcessingSpec.validate_checkpoint_dir","title":"validate_checkpoint_dir  <code>classmethod</code>","text":"<pre><code>validate_checkpoint_dir(v: str | Path) -&gt; Path\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"checkpoint_dir\")\n@classmethod\ndef validate_checkpoint_dir(cls, v: str | Path) -&gt; Path:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/#ondine.PromptSpec","title":"PromptSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for prompt template configuration.</p>"},{"location":"api/#ondine.PromptSpec.validate_template","title":"validate_template  <code>classmethod</code>","text":"<pre><code>validate_template(v: str) -&gt; str\n</code></pre> <p>Validate template has at least one variable.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"template\")\n@classmethod\ndef validate_template(cls, v: str) -&gt; str:\n    \"\"\"Validate template has at least one variable.\"\"\"\n    if \"{\" not in v or \"}\" not in v:\n        raise ValueError(\n            \"Template must contain at least one variable in {var} format\"\n        )\n    return v\n</code></pre>"},{"location":"api/#ondine.PromptSpec.validate_response_format","title":"validate_response_format  <code>classmethod</code>","text":"<pre><code>validate_response_format(v: str) -&gt; str\n</code></pre> <p>Validate response format is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"response_format\")\n@classmethod\ndef validate_response_format(cls, v: str) -&gt; str:\n    \"\"\"Validate response format is supported.\"\"\"\n    allowed = [\"raw\", \"json\", \"regex\"]\n    if v not in allowed:\n        raise ValueError(f\"response_format must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>ondine<ul> <li>adapters<ul> <li>checkpoint_storage</li> <li>data_io</li> <li>llm_client</li> <li>provider_registry</li> </ul> </li> <li>api<ul> <li>dataset_processor</li> <li>health_check</li> <li>pipeline</li> <li>pipeline_builder</li> <li>pipeline_composer</li> <li>quick</li> </ul> </li> <li>cli<ul> <li>main</li> </ul> </li> <li>config<ul> <li>config_loader</li> </ul> </li> <li>core<ul> <li>error_handler</li> <li>models</li> <li>specifications</li> </ul> </li> <li>integrations<ul> <li>airflow</li> <li>prefect</li> </ul> </li> <li>observability<ul> <li>observer</li> <li>sanitizer</li> <li>tracer</li> </ul> </li> <li>orchestration<ul> <li>async_executor</li> <li>execution_context</li> <li>execution_strategy</li> <li>observers</li> <li>pipeline_executor</li> <li>state_manager</li> <li>streaming_executor</li> <li>sync_executor</li> </ul> </li> <li>stages<ul> <li>data_loader_stage</li> <li>llm_invocation_stage</li> <li>multi_run_stage</li> <li>parser_factory</li> <li>pipeline_stage</li> <li>prompt_formatter_stage</li> <li>response_parser_stage</li> <li>result_writer_stage</li> <li>stage_registry</li> <li>streaming_loader_stage</li> </ul> </li> <li>utils<ul> <li>budget_controller</li> <li>cost_calculator</li> <li>cost_tracker</li> <li>input_preprocessing</li> <li>logging_utils</li> <li>metrics_exporter</li> <li>rate_limiter</li> <li>retry_handler</li> </ul> </li> </ul> </li> </ul>"},{"location":"api/adapters/","title":"adapters","text":""},{"location":"api/adapters/#ondine.adapters","title":"adapters","text":"<p>Infrastructure adapters for external systems.</p>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage","title":"CheckpointStorage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for checkpoint storage implementations.</p> <p>Follows Strategy pattern for pluggable storage backends.</p>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Unique session identifier</p> required <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Save checkpoint data.\n\n    Args:\n        session_id: Unique session identifier\n        data: Checkpoint data to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load latest checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Checkpoint data or None if not found</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Load latest checkpoint data.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Checkpoint data or None if not found\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.list_checkpoints","title":"list_checkpoints  <code>abstractmethod</code>","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CheckpointStorage.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if exists</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if checkpoint exists.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if exists\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage","title":"LocalFileCheckpointStorage","text":"<pre><code>LocalFileCheckpointStorage(checkpoint_dir: Path = Path('.checkpoints'), use_json: bool = True)\n</code></pre> <p>               Bases: <code>CheckpointStorage</code></p> <p>Local filesystem checkpoint storage implementation.</p> <p>Stores checkpoints as JSON files for human readability and debugging.</p> <p>Initialize local file checkpoint storage.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory for checkpoints</p> <code>Path('.checkpoints')</code> <code>use_json</code> <code>bool</code> <p>Use JSON format (True) or pickle (False)</p> <code>True</code> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path = Path(\".checkpoints\"),\n    use_json: bool = True,\n):\n    \"\"\"\n    Initialize local file checkpoint storage.\n\n    Args:\n        checkpoint_dir: Directory for checkpoints\n        use_json: Use JSON format (True) or pickle (False)\n    \"\"\"\n    self.checkpoint_dir = checkpoint_dir\n    self.use_json = use_json\n\n    # Create directory if doesn't exist\n    self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.save","title":"save","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint to local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"Save checkpoint to local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    # Add metadata\n    checkpoint_data = {\n        \"version\": \"1.0\",\n        \"session_id\": str(session_id),\n        \"timestamp\": datetime.now().isoformat(),\n        \"data\": data,\n    }\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path, \"w\") as f:\n                json.dump(\n                    checkpoint_data,\n                    f,\n                    indent=2,\n                    default=str,  # Handle non-serializable types\n                )\n        else:\n            with open(checkpoint_path, \"wb\") as f:\n                pickle.dump(checkpoint_data, f)\n\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.load","title":"load","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load checkpoint from local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"Load checkpoint from local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if not checkpoint_path.exists():\n        return None\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path) as f:\n                checkpoint_data = json.load(f)\n        else:\n            with open(checkpoint_path, \"rb\") as f:\n                checkpoint_data = pickle.load(f)\n\n        return checkpoint_data.get(\"data\")\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all checkpoints in directory.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"List all checkpoints in directory.\"\"\"\n    checkpoints = []\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        try:\n            # Extract session ID from filename\n            session_id_str = checkpoint_file.stem.replace(\"checkpoint_\", \"\")\n            session_id = UUID(session_id_str)\n\n            # Get file stats\n            stat = checkpoint_file.stat()\n\n            # Try to load checkpoint for additional info\n            data = self.load(session_id)\n            row_index = data.get(\"last_processed_row\", 0) if data else 0\n            stage_index = data.get(\"current_stage_index\", 0) if data else 0\n\n            checkpoints.append(\n                CheckpointInfo(\n                    session_id=session_id,\n                    checkpoint_path=str(checkpoint_file),\n                    row_index=row_index,\n                    stage_index=stage_index,\n                    timestamp=datetime.fromtimestamp(stat.st_mtime),\n                    size_bytes=stat.st_size,\n                )\n            )\n        except Exception:  # nosec B112\n            # Skip invalid checkpoint files\n            continue\n\n    return sorted(checkpoints, key=lambda x: x.timestamp, reverse=True)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.delete","title":"delete","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"Delete checkpoint file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if checkpoint_path.exists():\n        try:\n            checkpoint_path.unlink()\n            return True\n        except Exception:\n            return False\n    return False\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.exists","title":"exists","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"Check if checkpoint exists.\"\"\"\n    return self._get_checkpoint_path(session_id).exists()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LocalFileCheckpointStorage.cleanup_old_checkpoints","title":"cleanup_old_checkpoints","text":"<pre><code>cleanup_old_checkpoints(days: int = 7) -&gt; int\n</code></pre> <p>Delete checkpoints older than specified days.</p> <p>Parameters:</p> Name Type Description Default <code>days</code> <code>int</code> <p>Age threshold in days</p> <code>7</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of checkpoints deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def cleanup_old_checkpoints(self, days: int = 7) -&gt; int:\n    \"\"\"\n    Delete checkpoints older than specified days.\n\n    Args:\n        days: Age threshold in days\n\n    Returns:\n        Number of checkpoints deleted\n    \"\"\"\n    deleted = 0\n    cutoff = datetime.now().timestamp() - (days * 86400)\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        if checkpoint_file.stat().st_mtime &lt; cutoff:\n            try:\n                checkpoint_file.unlink()\n                deleted += 1\n            except Exception:  # nosec B112\n                continue\n\n    return deleted\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVReader","title":"CSVReader","text":"<pre><code>CSVReader(file_path: Path, delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>CSV file reader implementation.</p> <p>Initialize CSV reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to CSV file</p> required <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(\n    self,\n    file_path: Path,\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n):\n    \"\"\"\n    Initialize CSV reader.\n\n    Args:\n        file_path: Path to CSV file\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.file_path = file_path\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire CSV file.\"\"\"\n    return pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read CSV in chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Read CSV in chunks.\"\"\"\n    yield from pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n        chunksize=chunk_size,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVWriter","title":"CSVWriter","text":"<pre><code>CSVWriter(delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataWriter</code></p> <p>CSV file writer implementation.</p> <p>Initialize CSV writer.</p> <p>Parameters:</p> Name Type Description Default <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, delimiter: str = \",\", encoding: str = \"utf-8\"):\n    \"\"\"\n    Initialize CSV writer.\n\n    Args:\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV file.\"\"\"\n    data.to_csv(\n        path,\n        sep=self.delimiter,\n        encoding=self.encoding,\n        index=False,\n    )\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.CSVWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        # Write to temp file\n        data.to_csv(\n            temp_path,\n            sep=self.delimiter,\n            encoding=self.encoding,\n            index=False,\n        )\n\n        # Atomic rename\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        # Cleanup on failure\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(dataframe: DataFrame)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>In-memory DataFrame reader (pass-through).</p> <p>Initialize DataFrame reader.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame):\n    \"\"\"\n    Initialize DataFrame reader.\n\n    Args:\n        dataframe: Pandas DataFrame\n    \"\"\"\n    self.dataframe = dataframe.copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataFrameReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Return DataFrame copy.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame copy.\"\"\"\n    return self.dataframe.copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataFrameReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Yield DataFrame chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Yield DataFrame chunks.\"\"\"\n    for i in range(0, len(self.dataframe), chunk_size):\n        yield self.dataframe.iloc[i : i + chunk_size].copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataReader","title":"DataReader","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data readers.</p> <p>Follows Open/Closed principle: open for extension via new readers, closed for modification.</p>"},{"location":"api/adapters/#ondine.adapters.DataReader.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with all data</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Read entire dataset.\n\n    Returns:\n        DataFrame with all data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataReader.read_chunked","title":"read_chunked  <code>abstractmethod</code>","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read data in chunks for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrame chunks</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read data in chunks for memory efficiency.\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Yields:\n        DataFrame chunks\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataWriter","title":"DataWriter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data writers.</p> <p>Follows Single Responsibility: only handles data persistence.</p>"},{"location":"api/adapters/#ondine.adapters.DataWriter.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data to destination.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data to destination.\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.DataWriter.atomic_write","title":"atomic_write  <code>abstractmethod</code>","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data atomically (with rollback on failure).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data atomically (with rollback on failure).\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelReader","title":"ExcelReader","text":"<pre><code>ExcelReader(file_path: Path, sheet_name: str | int = 0)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Excel file reader implementation.</p> <p>Initialize Excel reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Excel file</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path, sheet_name: str | int = 0):\n    \"\"\"\n    Initialize Excel reader.\n\n    Args:\n        file_path: Path to Excel file\n        sheet_name: Sheet name or index\n    \"\"\"\n    self.file_path = file_path\n    self.sheet_name = sheet_name\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Excel file.\"\"\"\n    return pd.read_excel(self.file_path, sheet_name=self.sheet_name)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Excel in chunks.</p> <p>Note: Excel doesn't support native chunking, so we load all and yield chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Excel in chunks.\n\n    Note: Excel doesn't support native chunking, so we load all\n    and yield chunks.\n    \"\"\"\n    df = self.read()\n    for i in range(0, len(df), chunk_size):\n        yield df.iloc[i : i + chunk_size]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelWriter","title":"ExcelWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Excel file writer implementation.</p>"},{"location":"api/adapters/#ondine.adapters.ExcelWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel file.\"\"\"\n    data.to_excel(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ExcelWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_excel(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetReader","title":"ParquetReader","text":"<pre><code>ParquetReader(file_path: Path)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Parquet file reader implementation.</p> <p>Initialize Parquet reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Parquet file</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path):\n    \"\"\"\n    Initialize Parquet reader.\n\n    Args:\n        file_path: Path to Parquet file\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Parquet file.\"\"\"\n    return pd.read_parquet(self.file_path)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Parquet in chunks using Polars for efficiency.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Parquet in chunks using Polars for efficiency.\n    \"\"\"\n    # Use Polars for efficient chunked reading\n    lf = pl.scan_parquet(self.file_path)\n\n    # Read in batches\n    total_rows = lf.select(pl.len()).collect().item()\n\n    for i in range(0, total_rows, chunk_size):\n        chunk = lf.slice(i, chunk_size).collect().to_pandas()\n        yield chunk\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetWriter","title":"ParquetWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Parquet file writer implementation.</p>"},{"location":"api/adapters/#ondine.adapters.ParquetWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet file.\"\"\"\n    data.to_parquet(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ParquetWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_parquet(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AnthropicClient","title":"AnthropicClient","text":"<pre><code>AnthropicClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Anthropic Claude LLM client implementation.</p> <p>Initialize Anthropic client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Anthropic client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        raise ValueError(\"ANTHROPIC_API_KEY not found in spec or environment\")\n\n    self.client = Anthropic(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens or 1024,\n    )\n\n    # Anthropic uses approximate token counting\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AnthropicClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Anthropic API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Anthropic API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Approximate token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AnthropicClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens (approximate for Anthropic).</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens (approximate for Anthropic).\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AzureOpenAIClient","title":"AzureOpenAIClient","text":"<pre><code>AzureOpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Azure OpenAI LLM client implementation.</p> <p>Initialize Azure OpenAI client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Azure OpenAI client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"AZURE_OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"AZURE_OPENAI_API_KEY not found in spec or environment\")\n\n    if not spec.azure_endpoint:\n        raise ValueError(\"azure_endpoint required for Azure OpenAI\")\n\n    if not spec.azure_deployment:\n        raise ValueError(\"azure_deployment required for Azure OpenAI\")\n\n    self.client = AzureOpenAI(\n        model=spec.model,\n        deployment_name=spec.azure_deployment,\n        api_key=api_key,\n        azure_endpoint=spec.azure_endpoint,\n        api_version=spec.api_version or \"2024-02-15-preview\",\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AzureOpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Azure OpenAI API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Azure OpenAI API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.AzureOpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.GroqClient","title":"GroqClient","text":"<pre><code>GroqClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Groq LLM client implementation.</p> <p>Initialize Groq client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Groq client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"GROQ_API_KEY\")\n    if not api_key:\n        raise ValueError(\"GROQ_API_KEY not found in spec or environment\")\n\n    self.client = Groq(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Use tiktoken for token estimation\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.GroqClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Groq API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Groq API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract text from response - handle both string and object responses\n    if hasattr(response, \"message\") and hasattr(response.message, \"content\"):\n        response_text = response.message.content or \"\"\n    elif hasattr(response, \"content\"):\n        response_text = response.content or \"\"\n    else:\n        response_text = str(response) if response else \"\"\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(response_text))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.GroqClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient","title":"LLMClient","text":"<pre><code>LLMClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for LLM clients.</p> <p>Defines the contract that all LLM provider implementations must follow, enabling easy swapping of providers (Strategy pattern).</p> <p>Initialize LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"\n    Initialize LLM client.\n\n    Args:\n        spec: LLM specification\n    \"\"\"\n    self.spec = spec\n    self.model = spec.model\n    self.temperature = spec.temperature\n    self.max_tokens = spec.max_tokens\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.invoke","title":"invoke  <code>abstractmethod</code>","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke LLM with a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke LLM with a single prompt.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional model parameters\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.estimate_tokens","title":"estimate_tokens  <code>abstractmethod</code>","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.batch_invoke","title":"batch_invoke","text":"<pre><code>batch_invoke(prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]\n</code></pre> <p>Invoke LLM with multiple prompts.</p> <p>Default implementation: sequential invocation. Subclasses can override for provider-optimized batch processing.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of text prompts</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[LLMResponse]</code> <p>List of LLMResponse objects</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def batch_invoke(self, prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]:\n    \"\"\"\n    Invoke LLM with multiple prompts.\n\n    Default implementation: sequential invocation.\n    Subclasses can override for provider-optimized batch processing.\n\n    Args:\n        prompts: List of text prompts\n        **kwargs: Additional model parameters\n\n    Returns:\n        List of LLMResponse objects\n    \"\"\"\n    return [self.invoke(prompt, **kwargs) for prompt in prompts]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.LLMClient.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for token usage.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost in USD</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for token usage.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost in USD\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.spec.input_cost_per_1k_tokens or Decimal(\"0.0\"),\n        output_cost_per_1k=self.spec.output_cost_per_1k_tokens or Decimal(\"0.0\"),\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.OpenAIClient","title":"OpenAIClient","text":"<pre><code>OpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>OpenAI LLM client implementation.</p> <p>Initialize OpenAI client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize OpenAI client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY not found in spec or environment\")\n\n    self.client = OpenAI(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.OpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke OpenAI API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke OpenAI API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.OpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry","title":"ProviderRegistry","text":"<p>Global registry for LLM provider plugins.</p> <p>Enables registration and discovery of custom LLM providers without modifying core code. Uses lazy initialization for built-in providers.</p> Example"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry--register-custom-provider","title":"Register custom provider","text":"<p>@ProviderRegistry.register(\"my_llm\") class MyLLMClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         ...</p>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline.with_llm(provider=\"my_llm\", model=\"my-model\")</p>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(provider_id: str, client_class: type) -&gt; type\n</code></pre> <p>Register an LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")</p> required <code>client_class</code> <code>type</code> <p>LLM client class implementing LLMClient interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered client class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider_id already registered</p> Example <p>@ProviderRegistry.register(\"replicate\") class ReplicateClient(LLMClient):     ...</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef register(cls, provider_id: str, client_class: type) -&gt; type:\n    \"\"\"\n    Register an LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")\n        client_class: LLM client class implementing LLMClient interface\n\n    Returns:\n        The registered client class (enables use as decorator)\n\n    Raises:\n        ValueError: If provider_id already registered\n\n    Example:\n        @ProviderRegistry.register(\"replicate\")\n        class ReplicateClient(LLMClient):\n            ...\n    \"\"\"\n    if provider_id in cls._providers:\n        raise ValueError(\n            f\"Provider '{provider_id}' already registered. \"\n            f\"Use a different provider_id or unregister first.\"\n        )\n\n    cls._providers[provider_id] = client_class\n    return client_class\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(provider_id: str) -&gt; type\n</code></pre> <p>Get provider class by ID.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>LLM client class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Example <p>client_class = ProviderRegistry.get(\"openai\") client = client_class(spec)</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef get(cls, provider_id: str) -&gt; type:\n    \"\"\"\n    Get provider class by ID.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        LLM client class\n\n    Raises:\n        ValueError: If provider not found\n\n    Example:\n        client_class = ProviderRegistry.get(\"openai\")\n        client = client_class(spec)\n    \"\"\"\n    cls._ensure_builtins_registered()\n\n    if provider_id not in cls._providers:\n        available = \", \".join(sorted(cls._providers.keys()))\n        raise ValueError(\n            f\"Unknown provider: '{provider_id}'. Available providers: {available}\"\n        )\n\n    return cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.list_providers","title":"list_providers  <code>classmethod</code>","text":"<pre><code>list_providers() -&gt; dict[str, type]\n</code></pre> <p>List all registered providers.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping provider IDs to client classes</p> Example <p>providers = ProviderRegistry.list_providers() print(f\"Available: {list(providers.keys())}\")</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef list_providers(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered providers.\n\n    Returns:\n        Dictionary mapping provider IDs to client classes\n\n    Example:\n        providers = ProviderRegistry.list_providers()\n        print(f\"Available: {list(providers.keys())}\")\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return cls._providers.copy()\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(provider_id: str) -&gt; bool\n</code></pre> <p>Check if provider is registered.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, provider_id: str) -&gt; bool:\n    \"\"\"\n    Check if provider is registered.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return provider_id in cls._providers\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.ProviderRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(provider_id: str) -&gt; None\n</code></pre> <p>Unregister a provider (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, provider_id: str) -&gt; None:\n    \"\"\"\n    Unregister a provider (mainly for testing).\n\n    Args:\n        provider_id: Provider identifier\n\n    Raises:\n        ValueError: If provider not found\n    \"\"\"\n    if provider_id not in cls._providers:\n        raise ValueError(f\"Provider '{provider_id}' not registered\")\n\n    del cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_data_reader","title":"create_data_reader","text":"<pre><code>create_data_reader(source_type: DataSourceType, source_path: Path | None = None, dataframe: DataFrame | None = None, **kwargs: any) -&gt; DataReader\n</code></pre> <p>Factory function to create appropriate data reader.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>DataSourceType</code> <p>Type of data source</p> required <code>source_path</code> <code>Path | None</code> <p>Path to file (for file sources)</p> <code>None</code> <code>dataframe</code> <code>DataFrame | None</code> <p>DataFrame (for DataFrame source)</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Additional reader-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataReader</code> <p>Configured DataReader</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source type not supported or parameters invalid</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_reader(\n    source_type: DataSourceType,\n    source_path: Path | None = None,\n    dataframe: pd.DataFrame | None = None,\n    **kwargs: any,\n) -&gt; DataReader:\n    \"\"\"\n    Factory function to create appropriate data reader.\n\n    Args:\n        source_type: Type of data source\n        source_path: Path to file (for file sources)\n        dataframe: DataFrame (for DataFrame source)\n        **kwargs: Additional reader-specific parameters\n\n    Returns:\n        Configured DataReader\n\n    Raises:\n        ValueError: If source type not supported or parameters invalid\n    \"\"\"\n    if source_type == DataSourceType.CSV:\n        if not source_path:\n            raise ValueError(\"source_path required for CSV\")\n        return CSVReader(\n            source_path,\n            delimiter=kwargs.get(\"delimiter\", \",\"),\n            encoding=kwargs.get(\"encoding\", \"utf-8\"),\n        )\n    if source_type == DataSourceType.EXCEL:\n        if not source_path:\n            raise ValueError(\"source_path required for Excel\")\n        return ExcelReader(source_path, sheet_name=kwargs.get(\"sheet_name\", 0))\n    if source_type == DataSourceType.PARQUET:\n        if not source_path:\n            raise ValueError(\"source_path required for Parquet\")\n        return ParquetReader(source_path)\n    if source_type == DataSourceType.DATAFRAME:\n        if dataframe is None:\n            raise ValueError(\"dataframe required for DataFrame source\")\n        return DataFrameReader(dataframe)\n    raise ValueError(f\"Unsupported source type: {source_type}\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_data_writer","title":"create_data_writer","text":"<pre><code>create_data_writer(destination_type: DataSourceType) -&gt; DataWriter\n</code></pre> <p>Factory function to create appropriate data writer.</p> <p>Parameters:</p> Name Type Description Default <code>destination_type</code> <code>DataSourceType</code> <p>Type of destination</p> required <p>Returns:</p> Type Description <code>DataWriter</code> <p>Configured DataWriter</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If destination type not supported</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_writer(destination_type: DataSourceType) -&gt; DataWriter:\n    \"\"\"\n    Factory function to create appropriate data writer.\n\n    Args:\n        destination_type: Type of destination\n\n    Returns:\n        Configured DataWriter\n\n    Raises:\n        ValueError: If destination type not supported\n    \"\"\"\n    if destination_type == DataSourceType.CSV:\n        return CSVWriter()\n    if destination_type == DataSourceType.EXCEL:\n        return ExcelWriter()\n    if destination_type == DataSourceType.PARQUET:\n        return ParquetWriter()\n    raise ValueError(f\"Unsupported destination: {destination_type}\")\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_llm_client","title":"create_llm_client","text":"<pre><code>create_llm_client(spec: LLMSpec) -&gt; LLMClient\n</code></pre> <p>Factory function to create appropriate LLM client using ProviderRegistry.</p> <p>Supports both built-in providers (via LLMProvider enum) and custom providers (registered via ProviderRegistry).</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required <p>Returns:</p> Type Description <code>LLMClient</code> <p>Configured LLM client</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not supported</p> Example Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def create_llm_client(spec: LLMSpec) -&gt; LLMClient:\n    \"\"\"\n    Factory function to create appropriate LLM client using ProviderRegistry.\n\n    Supports both built-in providers (via LLMProvider enum) and custom\n    providers (registered via ProviderRegistry).\n\n    Args:\n        spec: LLM specification\n\n    Returns:\n        Configured LLM client\n\n    Raises:\n        ValueError: If provider not supported\n\n    Example:\n        # Built-in provider\n        spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\")\n        client = create_llm_client(spec)\n\n        # Custom provider (registered via @provider decorator)\n        spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\")\n        client = create_llm_client(spec)\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Check if custom provider ID is specified (from PipelineBuilder.with_llm)\n    custom_provider_id = getattr(spec, \"_custom_provider_id\", None)\n    if custom_provider_id:\n        provider_id = custom_provider_id\n    else:\n        # Convert enum to string for registry lookup\n        provider_id = (\n            spec.provider.value\n            if isinstance(spec.provider, LLMProvider)\n            else spec.provider\n        )\n\n    # Get provider class from registry\n    provider_class = ProviderRegistry.get(provider_id)\n\n    # Instantiate and return\n    return provider_class(spec)\n</code></pre>"},{"location":"api/adapters/#ondine.adapters.create_llm_client--built-in-provider","title":"Built-in provider","text":"<p>spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/#ondine.adapters.create_llm_client--custom-provider-registered-via-provider-decorator","title":"Custom provider (registered via @provider decorator)","text":"<p>spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/#ondine.adapters.provider","title":"provider","text":"<pre><code>provider(provider_id: str)\n</code></pre> <p>Decorator to register a custom LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@provider(\"replicate\") class ReplicateClient(LLMClient):     def init(self, spec: LLMSpec):         super().init(spec)         import replicate         self.client = replicate.Client(api_token=spec.api_key)</p> <pre><code>def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n    output = self.client.run(self.model, input={\"prompt\": prompt})\n    return LLMResponse(\n        text=output,\n        tokens_in=self.estimate_tokens(prompt),\n        tokens_out=self.estimate_tokens(output),\n        model=self.model,\n        cost=self.calculate_cost(...),\n        latency_ms=...\n    )\n\ndef estimate_tokens(self, text: str) -&gt; int:\n    return len(text.split())\n</code></pre> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>def provider(provider_id: str):\n    \"\"\"\n    Decorator to register a custom LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @provider(\"replicate\")\n        class ReplicateClient(LLMClient):\n            def __init__(self, spec: LLMSpec):\n                super().__init__(spec)\n                import replicate\n                self.client = replicate.Client(api_token=spec.api_key)\n\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                output = self.client.run(self.model, input={\"prompt\": prompt})\n                return LLMResponse(\n                    text=output,\n                    tokens_in=self.estimate_tokens(prompt),\n                    tokens_out=self.estimate_tokens(output),\n                    model=self.model,\n                    cost=self.calculate_cost(...),\n                    latency_ms=...\n                )\n\n            def estimate_tokens(self, text: str) -&gt; int:\n                return len(text.split())\n    \"\"\"\n\n    def decorator(cls):\n        ProviderRegistry.register(provider_id, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/","title":"checkpoint_storage","text":""},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage","title":"checkpoint_storage","text":"<p>Checkpoint storage for fault tolerance.</p> <p>Provides persistent storage of execution state to enable resume after failures.</p>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage","title":"CheckpointStorage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for checkpoint storage implementations.</p> <p>Follows Strategy pattern for pluggable storage backends.</p>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Unique session identifier</p> required <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Save checkpoint data.\n\n    Args:\n        session_id: Unique session identifier\n        data: Checkpoint data to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load latest checkpoint data.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>dict[str, Any] | None</code> <p>Checkpoint data or None if not found</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Load latest checkpoint data.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Checkpoint data or None if not found\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.list_checkpoints","title":"list_checkpoints  <code>abstractmethod</code>","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.CheckpointStorage.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if exists</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>@abstractmethod\ndef exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if checkpoint exists.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if exists\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage","title":"LocalFileCheckpointStorage","text":"<pre><code>LocalFileCheckpointStorage(checkpoint_dir: Path = Path('.checkpoints'), use_json: bool = True)\n</code></pre> <p>               Bases: <code>CheckpointStorage</code></p> <p>Local filesystem checkpoint storage implementation.</p> <p>Stores checkpoints as JSON files for human readability and debugging.</p> <p>Initialize local file checkpoint storage.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory for checkpoints</p> <code>Path('.checkpoints')</code> <code>use_json</code> <code>bool</code> <p>Use JSON format (True) or pickle (False)</p> <code>True</code> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path = Path(\".checkpoints\"),\n    use_json: bool = True,\n):\n    \"\"\"\n    Initialize local file checkpoint storage.\n\n    Args:\n        checkpoint_dir: Directory for checkpoints\n        use_json: Use JSON format (True) or pickle (False)\n    \"\"\"\n    self.checkpoint_dir = checkpoint_dir\n    self.use_json = use_json\n\n    # Create directory if doesn't exist\n    self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.save","title":"save","text":"<pre><code>save(session_id: UUID, data: dict[str, Any]) -&gt; bool\n</code></pre> <p>Save checkpoint to local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def save(self, session_id: UUID, data: dict[str, Any]) -&gt; bool:\n    \"\"\"Save checkpoint to local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    # Add metadata\n    checkpoint_data = {\n        \"version\": \"1.0\",\n        \"session_id\": str(session_id),\n        \"timestamp\": datetime.now().isoformat(),\n        \"data\": data,\n    }\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path, \"w\") as f:\n                json.dump(\n                    checkpoint_data,\n                    f,\n                    indent=2,\n                    default=str,  # Handle non-serializable types\n                )\n        else:\n            with open(checkpoint_path, \"wb\") as f:\n                pickle.dump(checkpoint_data, f)\n\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.load","title":"load","text":"<pre><code>load(session_id: UUID) -&gt; dict[str, Any] | None\n</code></pre> <p>Load checkpoint from local file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def load(self, session_id: UUID) -&gt; dict[str, Any] | None:\n    \"\"\"Load checkpoint from local file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if not checkpoint_path.exists():\n        return None\n\n    try:\n        if self.use_json:\n            with open(checkpoint_path) as f:\n                checkpoint_data = json.load(f)\n        else:\n            with open(checkpoint_path, \"rb\") as f:\n                checkpoint_data = pickle.load(f)\n\n        return checkpoint_data.get(\"data\")\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all checkpoints in directory.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"List all checkpoints in directory.\"\"\"\n    checkpoints = []\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        try:\n            # Extract session ID from filename\n            session_id_str = checkpoint_file.stem.replace(\"checkpoint_\", \"\")\n            session_id = UUID(session_id_str)\n\n            # Get file stats\n            stat = checkpoint_file.stat()\n\n            # Try to load checkpoint for additional info\n            data = self.load(session_id)\n            row_index = data.get(\"last_processed_row\", 0) if data else 0\n            stage_index = data.get(\"current_stage_index\", 0) if data else 0\n\n            checkpoints.append(\n                CheckpointInfo(\n                    session_id=session_id,\n                    checkpoint_path=str(checkpoint_file),\n                    row_index=row_index,\n                    stage_index=stage_index,\n                    timestamp=datetime.fromtimestamp(stat.st_mtime),\n                    size_bytes=stat.st_size,\n                )\n            )\n        except Exception:  # nosec B112\n            # Skip invalid checkpoint files\n            continue\n\n    return sorted(checkpoints, key=lambda x: x.timestamp, reverse=True)\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.delete","title":"delete","text":"<pre><code>delete(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoint file.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def delete(self, session_id: UUID) -&gt; bool:\n    \"\"\"Delete checkpoint file.\"\"\"\n    checkpoint_path = self._get_checkpoint_path(session_id)\n\n    if checkpoint_path.exists():\n        try:\n            checkpoint_path.unlink()\n            return True\n        except Exception:\n            return False\n    return False\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.exists","title":"exists","text":"<pre><code>exists(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if checkpoint exists.</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def exists(self, session_id: UUID) -&gt; bool:\n    \"\"\"Check if checkpoint exists.\"\"\"\n    return self._get_checkpoint_path(session_id).exists()\n</code></pre>"},{"location":"api/adapters/checkpoint_storage/#ondine.adapters.checkpoint_storage.LocalFileCheckpointStorage.cleanup_old_checkpoints","title":"cleanup_old_checkpoints","text":"<pre><code>cleanup_old_checkpoints(days: int = 7) -&gt; int\n</code></pre> <p>Delete checkpoints older than specified days.</p> <p>Parameters:</p> Name Type Description Default <code>days</code> <code>int</code> <p>Age threshold in days</p> <code>7</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of checkpoints deleted</p> Source code in <code>ondine/adapters/checkpoint_storage.py</code> <pre><code>def cleanup_old_checkpoints(self, days: int = 7) -&gt; int:\n    \"\"\"\n    Delete checkpoints older than specified days.\n\n    Args:\n        days: Age threshold in days\n\n    Returns:\n        Number of checkpoints deleted\n    \"\"\"\n    deleted = 0\n    cutoff = datetime.now().timestamp() - (days * 86400)\n\n    pattern = \"*.json\" if self.use_json else \"*.pkl\"\n    for checkpoint_file in self.checkpoint_dir.glob(pattern):\n        if checkpoint_file.stat().st_mtime &lt; cutoff:\n            try:\n                checkpoint_file.unlink()\n                deleted += 1\n            except Exception:  # nosec B112\n                continue\n\n    return deleted\n</code></pre>"},{"location":"api/adapters/data_io/","title":"data_io","text":""},{"location":"api/adapters/data_io/#ondine.adapters.data_io","title":"data_io","text":"<p>Data I/O adapters for reading and writing tabular data.</p> <p>Provides unified interface for multiple data formats following the Adapter pattern.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataReader","title":"DataReader","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data readers.</p> <p>Follows Open/Closed principle: open for extension via new readers, closed for modification.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataReader.read","title":"read  <code>abstractmethod</code>","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with all data</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Read entire dataset.\n\n    Returns:\n        DataFrame with all data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataReader.read_chunked","title":"read_chunked  <code>abstractmethod</code>","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read data in chunks for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrame chunks</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read data in chunks for memory efficiency.\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Yields:\n        DataFrame chunks\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVReader","title":"CSVReader","text":"<pre><code>CSVReader(file_path: Path, delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>CSV file reader implementation.</p> <p>Initialize CSV reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to CSV file</p> required <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(\n    self,\n    file_path: Path,\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n):\n    \"\"\"\n    Initialize CSV reader.\n\n    Args:\n        file_path: Path to CSV file\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.file_path = file_path\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire CSV file.\"\"\"\n    return pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read CSV in chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Read CSV in chunks.\"\"\"\n    yield from pd.read_csv(\n        self.file_path,\n        delimiter=self.delimiter,\n        encoding=self.encoding,\n        chunksize=chunk_size,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelReader","title":"ExcelReader","text":"<pre><code>ExcelReader(file_path: Path, sheet_name: str | int = 0)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Excel file reader implementation.</p> <p>Initialize Excel reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Excel file</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path, sheet_name: str | int = 0):\n    \"\"\"\n    Initialize Excel reader.\n\n    Args:\n        file_path: Path to Excel file\n        sheet_name: Sheet name or index\n    \"\"\"\n    self.file_path = file_path\n    self.sheet_name = sheet_name\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Excel file.\"\"\"\n    return pd.read_excel(self.file_path, sheet_name=self.sheet_name)\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Excel in chunks.</p> <p>Note: Excel doesn't support native chunking, so we load all and yield chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Excel in chunks.\n\n    Note: Excel doesn't support native chunking, so we load all\n    and yield chunks.\n    \"\"\"\n    df = self.read()\n    for i in range(0, len(df), chunk_size):\n        yield df.iloc[i : i + chunk_size]\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetReader","title":"ParquetReader","text":"<pre><code>ParquetReader(file_path: Path)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>Parquet file reader implementation.</p> <p>Initialize Parquet reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to Parquet file</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, file_path: Path):\n    \"\"\"\n    Initialize Parquet reader.\n\n    Args:\n        file_path: Path to Parquet file\n    \"\"\"\n    self.file_path = file_path\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Read entire Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Read entire Parquet file.\"\"\"\n    return pd.read_parquet(self.file_path)\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Read Parquet in chunks using Polars for efficiency.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Read Parquet in chunks using Polars for efficiency.\n    \"\"\"\n    # Use Polars for efficient chunked reading\n    lf = pl.scan_parquet(self.file_path)\n\n    # Read in batches\n    total_rows = lf.select(pl.len()).collect().item()\n\n    for i in range(0, total_rows, chunk_size):\n        chunk = lf.slice(i, chunk_size).collect().to_pandas()\n        yield chunk\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataFrameReader","title":"DataFrameReader","text":"<pre><code>DataFrameReader(dataframe: DataFrame)\n</code></pre> <p>               Bases: <code>DataReader</code></p> <p>In-memory DataFrame reader (pass-through).</p> <p>Initialize DataFrame reader.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame):\n    \"\"\"\n    Initialize DataFrame reader.\n\n    Args:\n        dataframe: Pandas DataFrame\n    \"\"\"\n    self.dataframe = dataframe.copy()\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataFrameReader.read","title":"read","text":"<pre><code>read() -&gt; pd.DataFrame\n</code></pre> <p>Return DataFrame copy.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read(self) -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame copy.\"\"\"\n    return self.dataframe.copy()\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataFrameReader.read_chunked","title":"read_chunked","text":"<pre><code>read_chunked(chunk_size: int) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Yield DataFrame chunks.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def read_chunked(self, chunk_size: int) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Yield DataFrame chunks.\"\"\"\n    for i in range(0, len(self.dataframe), chunk_size):\n        yield self.dataframe.iloc[i : i + chunk_size].copy()\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataWriter","title":"DataWriter","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data writers.</p> <p>Follows Single Responsibility: only handles data persistence.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataWriter.write","title":"write  <code>abstractmethod</code>","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data to destination.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data to destination.\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.DataWriter.atomic_write","title":"atomic_write  <code>abstractmethod</code>","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write data atomically (with rollback on failure).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to write</p> required <code>path</code> <code>Path</code> <p>Destination path</p> required <p>Returns:</p> Type Description <code>WriteConfirmation</code> <p>WriteConfirmation with details</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>@abstractmethod\ndef atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"\n    Write data atomically (with rollback on failure).\n\n    Args:\n        data: DataFrame to write\n        path: Destination path\n\n    Returns:\n        WriteConfirmation with details\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVWriter","title":"CSVWriter","text":"<pre><code>CSVWriter(delimiter: str = ',', encoding: str = 'utf-8')\n</code></pre> <p>               Bases: <code>DataWriter</code></p> <p>CSV file writer implementation.</p> <p>Initialize CSV writer.</p> <p>Parameters:</p> Name Type Description Default <code>delimiter</code> <code>str</code> <p>Column delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def __init__(self, delimiter: str = \",\", encoding: str = \"utf-8\"):\n    \"\"\"\n    Initialize CSV writer.\n\n    Args:\n        delimiter: Column delimiter\n        encoding: File encoding\n    \"\"\"\n    self.delimiter = delimiter\n    self.encoding = encoding\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV file.\"\"\"\n    data.to_csv(\n        path,\n        sep=self.delimiter,\n        encoding=self.encoding,\n        index=False,\n    )\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.CSVWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to CSV atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to CSV atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        # Write to temp file\n        data.to_csv(\n            temp_path,\n            sep=self.delimiter,\n            encoding=self.encoding,\n            index=False,\n        )\n\n        # Atomic rename\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        # Cleanup on failure\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelWriter","title":"ExcelWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Excel file writer implementation.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel file.\"\"\"\n    data.to_excel(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ExcelWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Excel atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Excel atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_excel(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetWriter","title":"ParquetWriter","text":"<p>               Bases: <code>DataWriter</code></p> <p>Parquet file writer implementation.</p>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetWriter.write","title":"write","text":"<pre><code>write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet file.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet file.\"\"\"\n    data.to_parquet(path, index=False)\n\n    return WriteConfirmation(\n        path=str(path),\n        rows_written=len(data),\n        success=True,\n    )\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.ParquetWriter.atomic_write","title":"atomic_write","text":"<pre><code>atomic_write(data: DataFrame, path: Path) -&gt; WriteConfirmation\n</code></pre> <p>Write to Parquet atomically.</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def atomic_write(self, data: pd.DataFrame, path: Path) -&gt; WriteConfirmation:\n    \"\"\"Write to Parquet atomically.\"\"\"\n    temp_path = path.with_suffix(\".tmp\")\n\n    try:\n        data.to_parquet(temp_path, index=False)\n        temp_path.replace(path)\n\n        return WriteConfirmation(\n            path=str(path),\n            rows_written=len(data),\n            success=True,\n        )\n    except Exception as e:\n        if temp_path.exists():\n            temp_path.unlink()\n        raise e\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.create_data_reader","title":"create_data_reader","text":"<pre><code>create_data_reader(source_type: DataSourceType, source_path: Path | None = None, dataframe: DataFrame | None = None, **kwargs: any) -&gt; DataReader\n</code></pre> <p>Factory function to create appropriate data reader.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>DataSourceType</code> <p>Type of data source</p> required <code>source_path</code> <code>Path | None</code> <p>Path to file (for file sources)</p> <code>None</code> <code>dataframe</code> <code>DataFrame | None</code> <p>DataFrame (for DataFrame source)</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Additional reader-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataReader</code> <p>Configured DataReader</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source type not supported or parameters invalid</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_reader(\n    source_type: DataSourceType,\n    source_path: Path | None = None,\n    dataframe: pd.DataFrame | None = None,\n    **kwargs: any,\n) -&gt; DataReader:\n    \"\"\"\n    Factory function to create appropriate data reader.\n\n    Args:\n        source_type: Type of data source\n        source_path: Path to file (for file sources)\n        dataframe: DataFrame (for DataFrame source)\n        **kwargs: Additional reader-specific parameters\n\n    Returns:\n        Configured DataReader\n\n    Raises:\n        ValueError: If source type not supported or parameters invalid\n    \"\"\"\n    if source_type == DataSourceType.CSV:\n        if not source_path:\n            raise ValueError(\"source_path required for CSV\")\n        return CSVReader(\n            source_path,\n            delimiter=kwargs.get(\"delimiter\", \",\"),\n            encoding=kwargs.get(\"encoding\", \"utf-8\"),\n        )\n    if source_type == DataSourceType.EXCEL:\n        if not source_path:\n            raise ValueError(\"source_path required for Excel\")\n        return ExcelReader(source_path, sheet_name=kwargs.get(\"sheet_name\", 0))\n    if source_type == DataSourceType.PARQUET:\n        if not source_path:\n            raise ValueError(\"source_path required for Parquet\")\n        return ParquetReader(source_path)\n    if source_type == DataSourceType.DATAFRAME:\n        if dataframe is None:\n            raise ValueError(\"dataframe required for DataFrame source\")\n        return DataFrameReader(dataframe)\n    raise ValueError(f\"Unsupported source type: {source_type}\")\n</code></pre>"},{"location":"api/adapters/data_io/#ondine.adapters.data_io.create_data_writer","title":"create_data_writer","text":"<pre><code>create_data_writer(destination_type: DataSourceType) -&gt; DataWriter\n</code></pre> <p>Factory function to create appropriate data writer.</p> <p>Parameters:</p> Name Type Description Default <code>destination_type</code> <code>DataSourceType</code> <p>Type of destination</p> required <p>Returns:</p> Type Description <code>DataWriter</code> <p>Configured DataWriter</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If destination type not supported</p> Source code in <code>ondine/adapters/data_io.py</code> <pre><code>def create_data_writer(destination_type: DataSourceType) -&gt; DataWriter:\n    \"\"\"\n    Factory function to create appropriate data writer.\n\n    Args:\n        destination_type: Type of destination\n\n    Returns:\n        Configured DataWriter\n\n    Raises:\n        ValueError: If destination type not supported\n    \"\"\"\n    if destination_type == DataSourceType.CSV:\n        return CSVWriter()\n    if destination_type == DataSourceType.EXCEL:\n        return ExcelWriter()\n    if destination_type == DataSourceType.PARQUET:\n        return ParquetWriter()\n    raise ValueError(f\"Unsupported destination: {destination_type}\")\n</code></pre>"},{"location":"api/adapters/llm_client/","title":"llm_client","text":""},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client","title":"llm_client","text":"<p>LLM client abstractions and implementations.</p> <p>Provides unified interface for multiple LLM providers following the Adapter pattern and Dependency Inversion principle.</p>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient","title":"LLMClient","text":"<pre><code>LLMClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for LLM clients.</p> <p>Defines the contract that all LLM provider implementations must follow, enabling easy swapping of providers (Strategy pattern).</p> <p>Initialize LLM client.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"\n    Initialize LLM client.\n\n    Args:\n        spec: LLM specification\n    \"\"\"\n    self.spec = spec\n    self.model = spec.model\n    self.temperature = spec.temperature\n    self.max_tokens = spec.max_tokens\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.invoke","title":"invoke  <code>abstractmethod</code>","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke LLM with a single prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke LLM with a single prompt.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional model parameters\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.estimate_tokens","title":"estimate_tokens  <code>abstractmethod</code>","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count for text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>@abstractmethod\ndef estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count for text.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.batch_invoke","title":"batch_invoke","text":"<pre><code>batch_invoke(prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]\n</code></pre> <p>Invoke LLM with multiple prompts.</p> <p>Default implementation: sequential invocation. Subclasses can override for provider-optimized batch processing.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of text prompts</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[LLMResponse]</code> <p>List of LLMResponse objects</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def batch_invoke(self, prompts: list[str], **kwargs: Any) -&gt; list[LLMResponse]:\n    \"\"\"\n    Invoke LLM with multiple prompts.\n\n    Default implementation: sequential invocation.\n    Subclasses can override for provider-optimized batch processing.\n\n    Args:\n        prompts: List of text prompts\n        **kwargs: Additional model parameters\n\n    Returns:\n        List of LLMResponse objects\n    \"\"\"\n    return [self.invoke(prompt, **kwargs) for prompt in prompts]\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.LLMClient.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for token usage.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost in USD</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for token usage.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost in USD\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.spec.input_cost_per_1k_tokens or Decimal(\"0.0\"),\n        output_cost_per_1k=self.spec.output_cost_per_1k_tokens or Decimal(\"0.0\"),\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAIClient","title":"OpenAIClient","text":"<pre><code>OpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>OpenAI LLM client implementation.</p> <p>Initialize OpenAI client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize OpenAI client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY not found in spec or environment\")\n\n    self.client = OpenAI(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke OpenAI API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke OpenAI API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AzureOpenAIClient","title":"AzureOpenAIClient","text":"<pre><code>AzureOpenAIClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Azure OpenAI LLM client implementation.</p> <p>Initialize Azure OpenAI client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Azure OpenAI client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"AZURE_OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"AZURE_OPENAI_API_KEY not found in spec or environment\")\n\n    if not spec.azure_endpoint:\n        raise ValueError(\"azure_endpoint required for Azure OpenAI\")\n\n    if not spec.azure_deployment:\n        raise ValueError(\"azure_deployment required for Azure OpenAI\")\n\n    self.client = AzureOpenAI(\n        model=spec.model,\n        deployment_name=spec.azure_deployment,\n        api_key=api_key,\n        azure_endpoint=spec.azure_endpoint,\n        api_version=spec.api_version or \"2024-02-15-preview\",\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Initialize tokenizer\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(spec.model)\n    except KeyError:\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AzureOpenAIClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Azure OpenAI API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Azure OpenAI API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AzureOpenAIClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AnthropicClient","title":"AnthropicClient","text":"<pre><code>AnthropicClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Anthropic Claude LLM client implementation.</p> <p>Initialize Anthropic client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Anthropic client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        raise ValueError(\"ANTHROPIC_API_KEY not found in spec or environment\")\n\n    self.client = Anthropic(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens or 1024,\n    )\n\n    # Anthropic uses approximate token counting\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AnthropicClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Anthropic API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Anthropic API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Approximate token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(str(response)))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=str(response),\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.AnthropicClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens (approximate for Anthropic).</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens (approximate for Anthropic).\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.GroqClient","title":"GroqClient","text":"<pre><code>GroqClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Groq LLM client implementation.</p> <p>Initialize Groq client.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"Initialize Groq client.\"\"\"\n    super().__init__(spec)\n\n    api_key = spec.api_key or os.getenv(\"GROQ_API_KEY\")\n    if not api_key:\n        raise ValueError(\"GROQ_API_KEY not found in spec or environment\")\n\n    self.client = Groq(\n        model=spec.model,\n        api_key=api_key,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Use tiktoken for token estimation\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.GroqClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke Groq API.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"Invoke Groq API.\"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract text from response - handle both string and object responses\n    if hasattr(response, \"message\") and hasattr(response.message, \"content\"):\n        response_text = response.message.content or \"\"\n    elif hasattr(response, \"content\"):\n        response_text = response.content or \"\"\n    else:\n        response_text = str(response) if response else \"\"\n\n    # Extract token usage\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(response_text))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=self.model,\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.GroqClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"Estimate tokens using tiktoken.\"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAICompatibleClient","title":"OpenAICompatibleClient","text":"<pre><code>OpenAICompatibleClient(spec: LLMSpec)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>Client for OpenAI-compatible API endpoints.</p> <p>Supports custom providers like Ollama, vLLM, Together.ai, Anyscale, and any other API that implements the OpenAI chat completions format.</p> <p>Initialize OpenAI-compatible client.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification with base_url required</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If base_url not provided</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec):\n    \"\"\"\n    Initialize OpenAI-compatible client.\n\n    Args:\n        spec: LLM specification with base_url required\n\n    Raises:\n        ValueError: If base_url not provided\n    \"\"\"\n    super().__init__(spec)\n\n    if not spec.base_url:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n\n    # Get API key (optional for local APIs like Ollama)\n    api_key = spec.api_key or os.getenv(\"OPENAI_COMPATIBLE_API_KEY\") or \"dummy\"\n\n    # Initialize OpenAI client with custom base URL\n    self.client = OpenAI(\n        model=spec.model,\n        api_key=api_key,\n        api_base=spec.base_url,\n        temperature=spec.temperature,\n        max_tokens=spec.max_tokens,\n    )\n\n    # Use provider_name for logging/metrics, or default\n    self.provider_name = spec.provider_name or \"OpenAI-Compatible\"\n\n    # Initialize tokenizer (use default encoding for custom providers)\n    self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAICompatibleClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke OpenAI-compatible API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke OpenAI-compatible API.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional model parameters\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    start_time = time.time()\n\n    message = ChatMessage(role=\"user\", content=prompt)\n    response = self.client.chat([message])\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Extract text from response\n    response_text = str(response) if response else \"\"\n\n    # Estimate token usage (approximate for custom providers)\n    tokens_in = len(self.tokenizer.encode(prompt))\n    tokens_out = len(self.tokenizer.encode(response_text))\n\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=f\"{self.provider_name}/{self.model}\",  # Show provider in metrics\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.OpenAICompatibleClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate tokens using tiktoken.</p> <p>Note: This is approximate for custom providers.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate tokens using tiktoken.\n\n    Note: This is approximate for custom providers.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    return len(self.tokenizer.encode(text))\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.MLXClient","title":"MLXClient","text":"<pre><code>MLXClient(spec: LLMSpec, _mlx_lm_module=None)\n</code></pre> <p>               Bases: <code>LLMClient</code></p> <p>MLX client for Apple Silicon local inference.</p> <p>MLX is Apple's optimized ML framework for M-series chips. This client enables fast, local LLM inference without API costs.</p> <p>Requires: pip install ondine[mlx] Platform: macOS with Apple Silicon only</p> <p>Initialize MLX client and load model.</p> <p>Model is loaded once and cached for fast subsequent calls.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification with model name</p> required <code>_mlx_lm_module</code> <p>MLX module (internal/testing only)</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If MLX not installed</p> <code>Exception</code> <p>If model loading fails</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def __init__(self, spec: LLMSpec, _mlx_lm_module=None):\n    \"\"\"\n    Initialize MLX client and load model.\n\n    Model is loaded once and cached for fast subsequent calls.\n\n    Args:\n        spec: LLM specification with model name\n        _mlx_lm_module: MLX module (internal/testing only)\n\n    Raises:\n        ImportError: If MLX not installed\n        Exception: If model loading fails\n    \"\"\"\n    super().__init__(spec)\n\n    # Load mlx_lm module (or use injected module for testing)\n    if _mlx_lm_module is None:\n        try:\n            import mlx_lm as _mlx_lm_module\n        except ImportError as e:\n            raise ImportError(\n                \"MLX not installed. Install with:\\n\"\n                \"  pip install ondine[mlx]\\n\"\n                \"or:\\n\"\n                \"  pip install mlx mlx-lm\\n\\n\"\n                \"Note: MLX only works on Apple Silicon (M1/M2/M3/M4 chips)\"\n            ) from e\n\n    # Store mlx_lm module for later use\n    self.mlx_lm = _mlx_lm_module\n\n    # Load model once (expensive operation, ~1-2 seconds)\n    print(f\"\ud83d\udd04 Loading MLX model: {spec.model}...\")\n    try:\n        self.mlx_model, self.mlx_tokenizer = self.mlx_lm.load(spec.model)\n        print(\"\u2705 Model loaded successfully\")\n    except Exception as e:\n        raise Exception(\n            f\"Failed to load MLX model '{spec.model}'. \"\n            f\"Ensure the model exists on HuggingFace and you have access. \"\n            f\"Error: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.MLXClient.invoke","title":"invoke","text":"<pre><code>invoke(prompt: str, **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Invoke MLX model for inference.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with result and metadata</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def invoke(self, prompt: str, **kwargs: Any) -&gt; LLMResponse:\n    \"\"\"\n    Invoke MLX model for inference.\n\n    Args:\n        prompt: Text prompt\n        **kwargs: Additional generation parameters\n\n    Returns:\n        LLMResponse with result and metadata\n    \"\"\"\n    start_time = time.time()\n\n    # Generate response using cached model\n    max_tokens = kwargs.get(\"max_tokens\", self.max_tokens)\n\n    response_text = self.mlx_lm.generate(\n        self.mlx_model,\n        self.mlx_tokenizer,\n        prompt=prompt,\n        max_tokens=max_tokens,\n        verbose=False,\n    )\n\n    latency_ms = (time.time() - start_time) * 1000\n\n    # Estimate token usage using MLX tokenizer\n    try:\n        tokens_in = len(self.mlx_tokenizer.encode(prompt))\n        tokens_out = len(self.mlx_tokenizer.encode(response_text))\n    except Exception:\n        # Fallback to simple estimation if encoding fails\n        tokens_in = len(prompt.split())\n        tokens_out = len(response_text.split())\n\n    # Calculate cost (typically $0 for local models)\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    return LLMResponse(\n        text=response_text,\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        model=f\"MLX/{self.model}\",\n        cost=cost,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.MLXClient.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(text: str) -&gt; int\n</code></pre> <p>Estimate token count using MLX tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text</p> required <p>Returns:</p> Type Description <code>int</code> <p>Estimated token count</p> Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def estimate_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    Estimate token count using MLX tokenizer.\n\n    Args:\n        text: Input text\n\n    Returns:\n        Estimated token count\n    \"\"\"\n    try:\n        return len(self.mlx_tokenizer.encode(text))\n    except Exception:\n        # Fallback to simple word count\n        return len(text.split())\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.create_llm_client","title":"create_llm_client","text":"<pre><code>create_llm_client(spec: LLMSpec) -&gt; LLMClient\n</code></pre> <p>Factory function to create appropriate LLM client using ProviderRegistry.</p> <p>Supports both built-in providers (via LLMProvider enum) and custom providers (registered via ProviderRegistry).</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification</p> required <p>Returns:</p> Type Description <code>LLMClient</code> <p>Configured LLM client</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not supported</p> Example Source code in <code>ondine/adapters/llm_client.py</code> <pre><code>def create_llm_client(spec: LLMSpec) -&gt; LLMClient:\n    \"\"\"\n    Factory function to create appropriate LLM client using ProviderRegistry.\n\n    Supports both built-in providers (via LLMProvider enum) and custom\n    providers (registered via ProviderRegistry).\n\n    Args:\n        spec: LLM specification\n\n    Returns:\n        Configured LLM client\n\n    Raises:\n        ValueError: If provider not supported\n\n    Example:\n        # Built-in provider\n        spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\")\n        client = create_llm_client(spec)\n\n        # Custom provider (registered via @provider decorator)\n        spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\")\n        client = create_llm_client(spec)\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Check if custom provider ID is specified (from PipelineBuilder.with_llm)\n    custom_provider_id = getattr(spec, \"_custom_provider_id\", None)\n    if custom_provider_id:\n        provider_id = custom_provider_id\n    else:\n        # Convert enum to string for registry lookup\n        provider_id = (\n            spec.provider.value\n            if isinstance(spec.provider, LLMProvider)\n            else spec.provider\n        )\n\n    # Get provider class from registry\n    provider_class = ProviderRegistry.get(provider_id)\n\n    # Instantiate and return\n    return provider_class(spec)\n</code></pre>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.create_llm_client--built-in-provider","title":"Built-in provider","text":"<p>spec = LLMSpec(provider=LLMProvider.OPENAI, model=\"gpt-4o-mini\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/llm_client/#ondine.adapters.llm_client.create_llm_client--custom-provider-registered-via-provider-decorator","title":"Custom provider (registered via @provider decorator)","text":"<p>spec = LLMSpec(provider=\"my_custom_llm\", model=\"my-model\") client = create_llm_client(spec)</p>"},{"location":"api/adapters/provider_registry/","title":"provider_registry","text":""},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry","title":"provider_registry","text":"<p>Provider registry for extensible LLM client plugins.</p> <p>Enables custom LLM providers to be registered and discovered without modifying core code.</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.LLMClient","title":"LLMClient","text":"<p>Protocol for LLM client implementations (imported to avoid circular dependency).</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry","title":"ProviderRegistry","text":"<p>Global registry for LLM provider plugins.</p> <p>Enables registration and discovery of custom LLM providers without modifying core code. Uses lazy initialization for built-in providers.</p> Example"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry--register-custom-provider","title":"Register custom provider","text":"<p>@ProviderRegistry.register(\"my_llm\") class MyLLMClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         ...</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline.with_llm(provider=\"my_llm\", model=\"my-model\")</p>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(provider_id: str, client_class: type) -&gt; type\n</code></pre> <p>Register an LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")</p> required <code>client_class</code> <code>type</code> <p>LLM client class implementing LLMClient interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered client class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider_id already registered</p> Example <p>@ProviderRegistry.register(\"replicate\") class ReplicateClient(LLMClient):     ...</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef register(cls, provider_id: str, client_class: type) -&gt; type:\n    \"\"\"\n    Register an LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier (e.g., \"openai\", \"my_custom_llm\")\n        client_class: LLM client class implementing LLMClient interface\n\n    Returns:\n        The registered client class (enables use as decorator)\n\n    Raises:\n        ValueError: If provider_id already registered\n\n    Example:\n        @ProviderRegistry.register(\"replicate\")\n        class ReplicateClient(LLMClient):\n            ...\n    \"\"\"\n    if provider_id in cls._providers:\n        raise ValueError(\n            f\"Provider '{provider_id}' already registered. \"\n            f\"Use a different provider_id or unregister first.\"\n        )\n\n    cls._providers[provider_id] = client_class\n    return client_class\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(provider_id: str) -&gt; type\n</code></pre> <p>Get provider class by ID.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>LLM client class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Example <p>client_class = ProviderRegistry.get(\"openai\") client = client_class(spec)</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef get(cls, provider_id: str) -&gt; type:\n    \"\"\"\n    Get provider class by ID.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        LLM client class\n\n    Raises:\n        ValueError: If provider not found\n\n    Example:\n        client_class = ProviderRegistry.get(\"openai\")\n        client = client_class(spec)\n    \"\"\"\n    cls._ensure_builtins_registered()\n\n    if provider_id not in cls._providers:\n        available = \", \".join(sorted(cls._providers.keys()))\n        raise ValueError(\n            f\"Unknown provider: '{provider_id}'. Available providers: {available}\"\n        )\n\n    return cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.list_providers","title":"list_providers  <code>classmethod</code>","text":"<pre><code>list_providers() -&gt; dict[str, type]\n</code></pre> <p>List all registered providers.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping provider IDs to client classes</p> Example <p>providers = ProviderRegistry.list_providers() print(f\"Available: {list(providers.keys())}\")</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef list_providers(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered providers.\n\n    Returns:\n        Dictionary mapping provider IDs to client classes\n\n    Example:\n        providers = ProviderRegistry.list_providers()\n        print(f\"Available: {list(providers.keys())}\")\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return cls._providers.copy()\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(provider_id: str) -&gt; bool\n</code></pre> <p>Check if provider is registered.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, provider_id: str) -&gt; bool:\n    \"\"\"\n    Check if provider is registered.\n\n    Args:\n        provider_id: Provider identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    cls._ensure_builtins_registered()\n    return provider_id in cls._providers\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.ProviderRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(provider_id: str) -&gt; None\n</code></pre> <p>Unregister a provider (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Provider identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider not found</p> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, provider_id: str) -&gt; None:\n    \"\"\"\n    Unregister a provider (mainly for testing).\n\n    Args:\n        provider_id: Provider identifier\n\n    Raises:\n        ValueError: If provider not found\n    \"\"\"\n    if provider_id not in cls._providers:\n        raise ValueError(f\"Provider '{provider_id}' not registered\")\n\n    del cls._providers[provider_id]\n</code></pre>"},{"location":"api/adapters/provider_registry/#ondine.adapters.provider_registry.provider","title":"provider","text":"<pre><code>provider(provider_id: str)\n</code></pre> <p>Decorator to register a custom LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider_id</code> <code>str</code> <p>Unique provider identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@provider(\"replicate\") class ReplicateClient(LLMClient):     def init(self, spec: LLMSpec):         super().init(spec)         import replicate         self.client = replicate.Client(api_token=spec.api_key)</p> <pre><code>def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n    output = self.client.run(self.model, input={\"prompt\": prompt})\n    return LLMResponse(\n        text=output,\n        tokens_in=self.estimate_tokens(prompt),\n        tokens_out=self.estimate_tokens(output),\n        model=self.model,\n        cost=self.calculate_cost(...),\n        latency_ms=...\n    )\n\ndef estimate_tokens(self, text: str) -&gt; int:\n    return len(text.split())\n</code></pre> Source code in <code>ondine/adapters/provider_registry.py</code> <pre><code>def provider(provider_id: str):\n    \"\"\"\n    Decorator to register a custom LLM provider.\n\n    Args:\n        provider_id: Unique provider identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @provider(\"replicate\")\n        class ReplicateClient(LLMClient):\n            def __init__(self, spec: LLMSpec):\n                super().__init__(spec)\n                import replicate\n                self.client = replicate.Client(api_token=spec.api_key)\n\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                output = self.client.run(self.model, input={\"prompt\": prompt})\n                return LLMResponse(\n                    text=output,\n                    tokens_in=self.estimate_tokens(prompt),\n                    tokens_out=self.estimate_tokens(output),\n                    model=self.model,\n                    cost=self.calculate_cost(...),\n                    latency_ms=...\n                )\n\n            def estimate_tokens(self, text: str) -&gt; int:\n                return len(text.split())\n    \"\"\"\n\n    def decorator(cls):\n        ProviderRegistry.register(provider_id, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/api/","title":"api","text":""},{"location":"api/api/#ondine.api","title":"api","text":"<p>High-level API for pipeline construction and execution.</p>"},{"location":"api/api/#ondine.api.DatasetProcessor","title":"DatasetProcessor","text":"<pre><code>DatasetProcessor(data: str | DataFrame, input_column: str, output_column: str, prompt: str, llm_config: dict[str, any])\n</code></pre> <p>Simplified API for single-prompt, single-column use cases.</p> <p>This is a convenience wrapper around PipelineBuilder for users who don't need fine-grained control.</p> Example <p>processor = DatasetProcessor(     data=\"data.csv\",     input_column=\"description\",     output_column=\"cleaned\",     prompt=\"Clean this text: {description}\",     llm_config={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\"} ) result = processor.run()</p> <p>Initialize dataset processor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>CSV file path or DataFrame</p> required <code>input_column</code> <code>str</code> <p>Input column name</p> required <code>output_column</code> <code>str</code> <p>Output column name</p> required <code>prompt</code> <code>str</code> <p>Prompt template</p> required <code>llm_config</code> <code>dict[str, any]</code> <p>LLM configuration dict</p> required Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    input_column: str,\n    output_column: str,\n    prompt: str,\n    llm_config: dict[str, any],\n):\n    \"\"\"\n    Initialize dataset processor.\n\n    Args:\n        data: CSV file path or DataFrame\n        input_column: Input column name\n        output_column: Output column name\n        prompt: Prompt template\n        llm_config: LLM configuration dict\n    \"\"\"\n    self.data = data\n    self.input_column = input_column\n    self.output_column = output_column\n    self.prompt = prompt\n    self.llm_config = llm_config\n\n    # Build pipeline internally\n    builder = PipelineBuilder.create()\n\n    # Configure data source\n    if isinstance(data, str):\n        builder.from_csv(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    elif isinstance(data, pd.DataFrame):\n        builder.from_dataframe(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    else:\n        raise ValueError(\"data must be file path or DataFrame\")\n\n    # Configure prompt\n    builder.with_prompt(prompt)\n\n    # Configure LLM\n    provider = llm_config.get(\"provider\", \"openai\")\n    model = llm_config.get(\"model\", \"gpt-4o-mini\")\n    api_key = llm_config.get(\"api_key\")\n    temperature = llm_config.get(\"temperature\", 0.0)\n    max_tokens = llm_config.get(\"max_tokens\")\n\n    builder.with_llm(\n        provider=provider,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    # Build pipeline\n    self.pipeline = builder.build()\n</code></pre>"},{"location":"api/api/#ondine.api.DatasetProcessor.run","title":"run","text":"<pre><code>run() -&gt; pd.DataFrame\n</code></pre> <p>Execute processing and return results.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute processing and return results.\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    result = self.pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/#ondine.api.DatasetProcessor.run_sample","title":"run_sample","text":"<pre><code>run_sample(n: int = 10) -&gt; pd.DataFrame\n</code></pre> <p>Test on first N rows.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of rows to process</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with sample results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run_sample(self, n: int = 10) -&gt; pd.DataFrame:\n    \"\"\"\n    Test on first N rows.\n\n    Args:\n        n: Number of rows to process\n\n    Returns:\n        DataFrame with sample results\n    \"\"\"\n    # Create sample pipeline\n    if isinstance(self.data, str):\n        df = pd.read_csv(self.data).head(n)\n    else:\n        df = self.data.head(n)\n\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df,\n            input_columns=[self.input_column],\n            output_columns=[self.output_column],\n        )\n        .with_prompt(self.prompt)\n        .with_llm(\n            provider=self.llm_config.get(\"provider\", \"openai\"),\n            model=self.llm_config.get(\"model\", \"gpt-4o-mini\"),\n            api_key=self.llm_config.get(\"api_key\"),\n            temperature=self.llm_config.get(\"temperature\", 0.0),\n        )\n    )\n\n    sample_pipeline = builder.build()\n    result = sample_pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/#ondine.api.DatasetProcessor.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; float\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost in USD</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def estimate_cost(self) -&gt; float:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Estimated cost in USD\n    \"\"\"\n    estimate = self.pipeline.estimate_cost()\n    return float(estimate.total_cost)\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck","title":"HealthCheck","text":"<pre><code>HealthCheck(pipeline: Pipeline)\n</code></pre> <p>Health check API for monitoring pipeline status.</p> <p>Provides information about pipeline health and readiness.</p> <p>Initialize health check.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>Pipeline instance to monitor</p> required Source code in <code>ondine/api/health_check.py</code> <pre><code>def __init__(self, pipeline: Pipeline):\n    \"\"\"\n    Initialize health check.\n\n    Args:\n        pipeline: Pipeline instance to monitor\n    \"\"\"\n    self.pipeline = pipeline\n    self.last_check: datetime | None = None\n    self.last_status: dict[str, Any] = {}\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck.check","title":"check","text":"<pre><code>check() -&gt; dict[str, Any]\n</code></pre> <p>Perform health check.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Health status dictionary</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def check(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Perform health check.\n\n    Returns:\n        Health status dictionary\n    \"\"\"\n    self.last_check = datetime.now()\n\n    status = {\n        \"status\": \"healthy\",\n        \"timestamp\": self.last_check.isoformat(),\n        \"pipeline_id\": str(self.pipeline.id),\n        \"checks\": {},\n    }\n\n    # Check LLM provider configuration\n    try:\n        llm_spec = self.pipeline.specifications.llm\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"ok\",\n            \"provider\": llm_spec.provider.value,\n            \"model\": llm_spec.model,\n        }\n    except Exception as e:\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check data source configuration\n    try:\n        dataset_spec = self.pipeline.specifications.dataset\n        source_exists = True\n\n        if dataset_spec.source_path:\n            source_exists = dataset_spec.source_path.exists()\n\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"ok\" if source_exists else \"warning\",\n            \"source_type\": dataset_spec.source_type.value,\n            \"exists\": source_exists,\n        }\n    except Exception as e:\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check checkpoint storage\n    try:\n        checkpoint_dir = self.pipeline.specifications.processing.checkpoint_dir\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"ok\",\n            \"directory\": str(checkpoint_dir),\n            \"exists\": checkpoint_dir.exists(),\n        }\n    except Exception as e:\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"warning\",\n            \"error\": str(e),\n        }\n\n    # Store last status\n    self.last_status = status\n\n    return status\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck.is_healthy","title":"is_healthy","text":"<pre><code>is_healthy() -&gt; bool\n</code></pre> <p>Check if pipeline is healthy.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if healthy</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def is_healthy(self) -&gt; bool:\n    \"\"\"\n    Check if pipeline is healthy.\n\n    Returns:\n        True if healthy\n    \"\"\"\n    status = self.check()\n    return status[\"status\"] == \"healthy\"\n</code></pre>"},{"location":"api/api/#ondine.api.HealthCheck.get_readiness","title":"get_readiness","text":"<pre><code>get_readiness() -&gt; dict[str, Any]\n</code></pre> <p>Get readiness status.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Readiness information</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def get_readiness(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get readiness status.\n\n    Returns:\n        Readiness information\n    \"\"\"\n    validation = self.pipeline.validate()\n\n    return {\n        \"ready\": validation.is_valid,\n        \"errors\": validation.errors,\n        \"warnings\": validation.warnings,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(specifications: PipelineSpecifications, dataframe: DataFrame | None = None, executor: ExecutionStrategy | None = None)\n</code></pre> <p>Main pipeline class - Facade for dataset processing.</p> <p>Provides high-level interface for building and executing LLM-powered data transformations.</p> Example <p>pipeline = Pipeline(specifications) result = pipeline.execute()</p> <p>Initialize pipeline with specifications.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Complete pipeline configuration</p> required <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded DataFrame</p> <code>None</code> <code>executor</code> <code>ExecutionStrategy | None</code> <p>Optional execution strategy (default: SyncExecutor)</p> <code>None</code> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def __init__(\n    self,\n    specifications: PipelineSpecifications,\n    dataframe: pd.DataFrame | None = None,\n    executor: ExecutionStrategy | None = None,\n):\n    \"\"\"\n    Initialize pipeline with specifications.\n\n    Args:\n        specifications: Complete pipeline configuration\n        dataframe: Optional pre-loaded DataFrame\n        executor: Optional execution strategy (default: SyncExecutor)\n    \"\"\"\n    self.id = uuid4()\n    self.specifications = specifications\n    self.dataframe = dataframe\n    self.executor = executor or SyncExecutor()\n    self.observers: list[ExecutionObserver] = []\n    self.logger = get_logger(f\"{__name__}.{self.id}\")\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; Pipeline\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"Pipeline\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.validate","title":"validate","text":"<pre><code>validate() -&gt; ValidationResult\n</code></pre> <p>Validate pipeline configuration.</p> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with any errors/warnings</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def validate(self) -&gt; ValidationResult:\n    \"\"\"\n    Validate pipeline configuration.\n\n    Returns:\n        ValidationResult with any errors/warnings\n    \"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Validate dataset spec\n    if not self.specifications.dataset.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    if not self.specifications.dataset.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    # Validate that input columns exist in dataframe (if dataframe is provided)\n    if self.dataframe is not None and self.specifications.dataset.input_columns:\n        df_cols = set(self.dataframe.columns)\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_cols = input_cols - df_cols\n        if missing_cols:\n            result.add_error(\n                f\"Input columns not found in dataframe: {missing_cols}\"\n            )\n\n    # Validate prompt spec\n    if not self.specifications.prompt.template:\n        result.add_error(\"No prompt template specified\")\n    else:\n        # Check that template variables match input columns\n        import re\n\n        template_vars = set(\n            re.findall(r\"\\{(\\w+)\\}\", self.specifications.prompt.template)\n        )\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_vars = template_vars - input_cols\n        if missing_vars:\n            result.add_error(\n                f\"Template variables not in input columns: {missing_vars}\"\n            )\n\n    # Validate LLM spec\n    if not self.specifications.llm.model:\n        result.add_error(\"No LLM model specified\")\n\n    return result\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; CostEstimate\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def estimate_cost(self) -&gt; CostEstimate:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    # Create stages\n    loader = DataLoaderStage(self.dataframe)\n\n    # Load first few rows for estimation\n    df = loader.process(self.specifications.dataset, ExecutionContext())\n    sample_size = min(10, len(df))\n    sample_df = df.head(sample_size)\n\n    # Create formatter and get prompts\n    formatter = PromptFormatterStage(self.specifications.processing.batch_size)\n    batches = formatter.process(\n        (sample_df, self.specifications.prompt), ExecutionContext()\n    )\n\n    # Create LLM client and estimate\n    llm_client = create_llm_client(self.specifications.llm)\n    llm_stage = LLMInvocationStage(llm_client)\n\n    sample_estimate = llm_stage.estimate_cost(batches)\n\n    # Scale to full dataset\n    scale_factor = Decimal(len(df)) / Decimal(sample_size)\n\n    return CostEstimate(\n        total_cost=sample_estimate.total_cost * scale_factor,\n        total_tokens=int(sample_estimate.total_tokens * float(scale_factor)),\n        input_tokens=int(sample_estimate.input_tokens * float(scale_factor)),\n        output_tokens=int(sample_estimate.output_tokens * float(scale_factor)),\n        rows=len(df),\n        confidence=\"sample-based\",\n    )\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.execute","title":"execute","text":"<pre><code>execute(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    # Validate first\n    validation = self.validate()\n    if not validation.is_valid:\n        raise ValueError(f\"Pipeline validation failed: {validation.errors}\")\n\n    # Create or restore execution context\n    state_manager = StateManager(\n        storage=LocalFileCheckpointStorage(\n            self.specifications.processing.checkpoint_dir\n        ),\n        checkpoint_interval=self.specifications.processing.checkpoint_interval,\n    )\n\n    if resume_from:\n        # Resume from checkpoint\n        context = state_manager.load_checkpoint(resume_from)\n        if not context:\n            raise ValueError(f\"No checkpoint found for session {resume_from}\")\n        self.logger.info(\n            f\"Resuming from checkpoint at row {context.last_processed_row}\"\n        )\n    else:\n        # Create new context\n        context = ExecutionContext(pipeline_id=self.id)\n\n    # Add default observers if none specified\n    if not self.observers:\n        self.observers = [\n            ProgressBarObserver(),\n            LoggingObserver(),\n            CostTrackingObserver(),\n        ]\n\n    # Attach observers to context for progress notifications\n    context.observers = self.observers\n\n    # Notify observers of start\n    for observer in self.observers:\n        observer.on_pipeline_start(self, context)\n\n    try:\n        # Execute stages (preprocessing happens inside if enabled)\n        result_df = self._execute_stages(context, state_manager)\n\n        # Mark completion\n        context.end_time = datetime.now()\n\n        # Create execution result\n        result = ExecutionResult(\n            data=result_df,\n            metrics=context.get_stats(),\n            costs=CostEstimate(\n                total_cost=context.accumulated_cost,\n                total_tokens=context.accumulated_tokens,\n                input_tokens=0,\n                output_tokens=0,\n                rows=context.total_rows,\n                confidence=\"actual\",\n            ),\n            execution_id=context.session_id,\n            start_time=context.start_time,\n            end_time=context.end_time,\n            success=True,\n        )\n\n        # Optional: Auto-retry failed rows\n        if self.specifications.processing.auto_retry_failed:\n            # Get preprocessed data from context (or loaded data if no preprocessing)\n            retry_source_df = context.intermediate_data.get(\"preprocessed_data\")\n            if retry_source_df is None:\n                retry_source_df = context.intermediate_data.get(\"loaded_data\")\n            result = self._auto_retry_failed_rows(result, retry_source_df)\n\n        # Cleanup checkpoints on success\n        state_manager.cleanup_checkpoints(context.session_id)\n\n        # Notify observers of completion\n        for observer in self.observers:\n            observer.on_pipeline_complete(context, result)\n\n        return result\n\n    except Exception as e:\n        # Save checkpoint on error\n        state_manager.save_checkpoint(context)\n        self.logger.error(\n            f\"Pipeline failed. Checkpoint saved. \"\n            f\"Resume with: pipeline.execute(resume_from=UUID('{context.session_id}'))\"\n        )\n\n        # Notify observers of error\n        for observer in self.observers:\n            observer.on_pipeline_error(context, e)\n        raise\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline asynchronously.</p> <p>Uses AsyncExecutor for non-blocking execution. Ideal for integration with FastAPI, aiohttp, and other async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support async</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>async def execute_async(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline asynchronously.\n\n    Uses AsyncExecutor for non-blocking execution. Ideal for integration\n    with FastAPI, aiohttp, and other async frameworks.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        ValueError: If executor doesn't support async\n    \"\"\"\n    if not self.executor.supports_async():\n        raise ValueError(\n            \"Current executor doesn't support async. \"\n            \"Use AsyncExecutor: Pipeline(specs, executor=AsyncExecutor())\"\n        )\n\n    # For now, wrap synchronous execution in async\n    # TODO: Implement fully async execution pipeline\n    import asyncio\n\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, self.execute, resume_from)\n</code></pre>"},{"location":"api/api/#ondine.api.Pipeline.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(chunk_size: int | None = None) -&gt; Iterator[ExecutionResult]\n</code></pre> <p>Execute pipeline in streaming mode.</p> <p>Processes data in chunks for memory-efficient handling of large datasets. Ideal for datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int | None</code> <p>Number of rows per chunk (uses executor's chunk_size if None)</p> <code>None</code> <p>Yields:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult objects for each processed chunk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support streaming</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute_stream(\n    self, chunk_size: int | None = None\n) -&gt; Iterator[ExecutionResult]:\n    \"\"\"\n    Execute pipeline in streaming mode.\n\n    Processes data in chunks for memory-efficient handling of large datasets.\n    Ideal for datasets that don't fit in memory.\n\n    Args:\n        chunk_size: Number of rows per chunk (uses executor's chunk_size if None)\n\n    Yields:\n        ExecutionResult objects for each processed chunk\n\n    Raises:\n        ValueError: If executor doesn't support streaming\n    \"\"\"\n    if not self.executor.supports_streaming():\n        raise ValueError(\n            \"Current executor doesn't support streaming. \"\n            \"Use StreamingExecutor: Pipeline(specs, executor=StreamingExecutor())\"\n        )\n\n    # Use executor's chunk_size if not provided\n    if chunk_size is None and isinstance(self.executor, StreamingExecutor):\n        chunk_size = self.executor.chunk_size\n    elif chunk_size is None:\n        chunk_size = 1000  # Default fallback\n\n    # For now, execute the full pipeline and split result into chunks\n    # TODO: Implement proper streaming execution that processes chunks independently\n    result = self.execute()\n\n    # Split the result data into chunks and yield as separate ExecutionResults\n    total_rows = len(result.data)\n    for start_idx in range(0, total_rows, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_data = result.data.iloc[start_idx:end_idx].copy()\n\n        # Create a chunk result with proportional metrics\n        chunk_rows = len(chunk_data)\n        chunk_result = ExecutionResult(\n            data=chunk_data,\n            metrics=ProcessingStats(\n                total_rows=chunk_rows,\n                processed_rows=chunk_rows,\n                failed_rows=0,\n                skipped_rows=0,\n                rows_per_second=result.metrics.rows_per_second,\n                total_duration_seconds=result.metrics.total_duration_seconds\n                * (chunk_rows / total_rows),\n                stage_durations=result.metrics.stage_durations,\n            ),\n            costs=CostEstimate(\n                total_cost=result.costs.total_cost\n                * Decimal(chunk_rows / total_rows),\n                total_tokens=int(\n                    result.costs.total_tokens * (chunk_rows / total_rows)\n                ),\n                input_tokens=int(\n                    result.costs.input_tokens * (chunk_rows / total_rows)\n                ),\n                output_tokens=int(\n                    result.costs.output_tokens * (chunk_rows / total_rows)\n                ),\n                rows=chunk_rows,\n                confidence=result.costs.confidence,\n            ),\n            execution_id=result.execution_id,\n            start_time=result.start_time,\n            end_time=result.end_time,\n            success=True,\n        )\n        yield chunk_result\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder","title":"PipelineBuilder","text":"<pre><code>PipelineBuilder()\n</code></pre> <p>Fluent builder for constructing pipelines.</p> <p>Provides an intuitive, chainable API for common use cases.</p> Example <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p> <p>Initialize builder with None values.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with None values.\"\"\"\n    self._dataset_spec: DatasetSpec | None = None\n    self._prompt_spec: PromptSpec | None = None\n    self._llm_spec: LLMSpec | None = None\n    self._processing_spec: ProcessingSpec = ProcessingSpec()\n    self._output_spec: OutputSpec | None = None\n    self._dataframe: pd.DataFrame | None = None\n    self._executor: ExecutionStrategy | None = None\n    self._custom_parser: any | None = None\n    self._custom_llm_client: any | None = None\n    self._custom_stages: list[dict] = []  # For custom stage injection\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create() -&gt; PipelineBuilder\n</code></pre> <p>Start builder chain.</p> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>New PipelineBuilder instance</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef create() -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Start builder chain.\n\n    Returns:\n        New PipelineBuilder instance\n    \"\"\"\n    return PipelineBuilder()\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_specifications","title":"from_specifications  <code>staticmethod</code>","text":"<pre><code>from_specifications(specs: PipelineSpecifications) -&gt; PipelineBuilder\n</code></pre> <p>Create builder from existing specifications.</p> <p>Useful for loading from YAML and modifying programmatically.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>PipelineSpecifications</code> <p>Complete pipeline specifications</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>PipelineBuilder pre-configured with specs</p> Example <p>specs = load_pipeline_config(\"config.yaml\") builder = PipelineBuilder.from_specifications(specs) pipeline = builder.build()</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef from_specifications(specs: PipelineSpecifications) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Create builder from existing specifications.\n\n    Useful for loading from YAML and modifying programmatically.\n\n    Args:\n        specs: Complete pipeline specifications\n\n    Returns:\n        PipelineBuilder pre-configured with specs\n\n    Example:\n        specs = load_pipeline_config(\"config.yaml\")\n        builder = PipelineBuilder.from_specifications(specs)\n        pipeline = builder.build()\n    \"\"\"\n    builder = PipelineBuilder()\n    builder._dataset_spec = specs.dataset\n    builder._prompt_spec = specs.prompt\n    builder._llm_spec = specs.llm\n    builder._processing_spec = specs.processing\n    builder._output_spec = specs.output\n    return builder\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_csv","title":"from_csv","text":"<pre><code>from_csv(path: str, input_columns: list[str], output_columns: list[str], delimiter: str = ',', encoding: str = 'utf-8') -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_csv(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV data source.\n\n    Args:\n        path: Path to CSV file\n        input_columns: Input column names\n        output_columns: Output column names\n        delimiter: CSV delimiter\n        encoding: File encoding\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.CSV,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        delimiter=delimiter,\n        encoding=encoding,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_excel","title":"from_excel","text":"<pre><code>from_excel(path: str, input_columns: list[str], output_columns: list[str], sheet_name: str | int = 0) -&gt; PipelineBuilder\n</code></pre> <p>Configure Excel data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Excel file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_excel(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    sheet_name: str | int = 0,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Excel data source.\n\n    Args:\n        path: Path to Excel file\n        input_columns: Input column names\n        output_columns: Output column names\n        sheet_name: Sheet name or index\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.EXCEL,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        sheet_name=sheet_name,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_parquet","title":"from_parquet","text":"<pre><code>from_parquet(path: str, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure Parquet data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_parquet(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Parquet data source.\n\n    Args:\n        path: Path to Parquet file\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.PARQUET,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.from_dataframe","title":"from_dataframe","text":"<pre><code>from_dataframe(df: DataFrame, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure DataFrame source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_dataframe(\n    self,\n    df: pd.DataFrame,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure DataFrame source.\n\n    Args:\n        df: Pandas DataFrame\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.DATAFRAME,\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    self._dataframe = df\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_prompt","title":"with_prompt","text":"<pre><code>with_prompt(template: str, system_message: str | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Configure prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {variable} placeholders</p> required <code>system_message</code> <code>str | None</code> <p>Optional system message</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_prompt(\n    self,\n    template: str,\n    system_message: str | None = None,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure prompt template.\n\n    Args:\n        template: Prompt template with {variable} placeholders\n        system_message: Optional system message\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._prompt_spec = PromptSpec(\n        template=template,\n        system_message=system_message,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm","title":"with_llm","text":"<pre><code>with_llm(provider: str, model: str, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, **kwargs: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (openai, azure_openai, anthropic) or custom provider ID</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>api_key</code> <code>str | None</code> <p>API key (or from env)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm(\n    self,\n    provider: str,\n    model: str,\n    api_key: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    **kwargs: any,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM provider.\n\n    Args:\n        provider: Provider name (openai, azure_openai, anthropic) or custom provider ID\n        model: Model identifier\n        api_key: API key (or from env)\n        temperature: Sampling temperature\n        max_tokens: Max output tokens\n        **kwargs: Provider-specific parameters\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Try to convert to enum for built-in providers\n    try:\n        provider_enum = LLMProvider(provider.lower())\n    except ValueError:\n        # Not a built-in provider - check if it's a custom provider\n        if ProviderRegistry.is_registered(provider):\n            # Use a dummy enum value for validation, but store the actual provider string\n            provider_enum = LLMProvider.OPENAI  # Dummy for Pydantic validation\n            kwargs[\"_custom_provider_id\"] = provider\n        else:\n            raise ValueError(\n                f\"Unknown provider: {provider}. \"\n                f\"Available providers: {', '.join(ProviderRegistry.list_providers())}\"\n            )\n\n    self._llm_spec = LLMSpec(\n        provider=provider_enum,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec","title":"with_llm_spec","text":"<pre><code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM using a pre-built LLMSpec object.</p> <p>This method allows using LLMSpec objects directly, enabling: - Reusable provider configurations - Use of LLMProviderPresets for common providers - Custom LLMSpec instances for advanced use cases</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification object</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If spec is not an LLMSpec instance</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm_spec(self, spec: LLMSpec) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM using a pre-built LLMSpec object.\n\n    This method allows using LLMSpec objects directly, enabling:\n    - Reusable provider configurations\n    - Use of LLMProviderPresets for common providers\n    - Custom LLMSpec instances for advanced use cases\n\n    Args:\n        spec: LLM specification object\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        TypeError: If spec is not an LLMSpec instance\n\n    Example:\n        # Use preset\n        from ondine.core.specifications import LLMProviderPresets\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Process: {text}\")\n            .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n            .build()\n        )\n\n        # Custom spec\n        custom = LLMSpec(\n            provider=LLMProvider.OPENAI,\n            model=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n        pipeline.with_llm_spec(custom)\n\n        # Override preset\n        spec = LLMProviderPresets.GPT4O_MINI.model_copy(\n            update={\"temperature\": 0.9}\n        )\n        pipeline.with_llm_spec(spec)\n    \"\"\"\n    if not isinstance(spec, LLMSpec):\n        raise TypeError(\n            f\"Expected LLMSpec, got {type(spec).__name__}. \"\n            f\"Use with_llm() for parameter-based configuration.\"\n        )\n\n    self._llm_spec = spec\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec--use-preset","title":"Use preset","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec--custom-spec","title":"Custom spec","text":"<p>custom = LLMSpec(     provider=LLMProvider.OPENAI,     model=\"gpt-4o-mini\",     temperature=0.7 ) pipeline.with_llm_spec(custom)</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_llm_spec--override-preset","title":"Override preset","text":"<p>spec = LLMProviderPresets.GPT4O_MINI.model_copy(     update={\"temperature\": 0.9} ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_custom_llm_client","title":"with_custom_llm_client","text":"<pre><code>with_custom_llm_client(client: any) -&gt; PipelineBuilder\n</code></pre> <p>Provide a custom LLM client instance directly.</p> <p>This allows advanced users to create their own LLM client implementations by extending the LLMClient base class. The custom client will be used instead of the factory-created client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>any</code> <p>Custom LLM client instance (must inherit from LLMClient)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <p>class MyCustomClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         # Custom implementation         ...</p> <p>pipeline = (     PipelineBuilder.create()     .from_dataframe(df, ...)     .with_prompt(\"...\")     .with_custom_llm_client(MyCustomClient(spec))     .build() )</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_custom_llm_client(self, client: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Provide a custom LLM client instance directly.\n\n    This allows advanced users to create their own LLM client implementations\n    by extending the LLMClient base class. The custom client will be used\n    instead of the factory-created client.\n\n    Args:\n        client: Custom LLM client instance (must inherit from LLMClient)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        class MyCustomClient(LLMClient):\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                # Custom implementation\n                ...\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_dataframe(df, ...)\n            .with_prompt(\"...\")\n            .with_custom_llm_client(MyCustomClient(spec))\n            .build()\n        )\n    \"\"\"\n    from ondine.adapters.llm_client import LLMClient\n\n    if not isinstance(client, LLMClient):\n        raise TypeError(\n            f\"Custom client must inherit from LLMClient, got {type(client).__name__}\"\n        )\n\n    self._custom_llm_client = client\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_batch_size","title":"with_batch_size","text":"<pre><code>with_batch_size(size: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure batch size.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Rows per batch</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_size(self, size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure batch size.\n\n    Args:\n        size: Rows per batch\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.batch_size = size\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_concurrency","title":"with_concurrency","text":"<pre><code>with_concurrency(threads: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure concurrent requests.</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int</code> <p>Number of concurrent threads</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_concurrency(self, threads: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure concurrent requests.\n\n    Args:\n        threads: Number of concurrent threads\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.concurrency = threads\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_checkpoint_interval","title":"with_checkpoint_interval","text":"<pre><code>with_checkpoint_interval(rows: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint frequency.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Rows between checkpoints</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_interval(self, rows: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint frequency.\n\n    Args:\n        rows: Rows between checkpoints\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_interval = rows\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_rate_limit","title":"with_rate_limit","text":"<pre><code>with_rate_limit(rpm: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure rate limiting.</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_rate_limit(self, rpm: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure rate limiting.\n\n    Args:\n        rpm: Requests per minute\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.rate_limit_rpm = rpm\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_max_retries","title":"with_max_retries","text":"<pre><code>with_max_retries(retries: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>Maximum number of retry attempts</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_retries(self, retries: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum retry attempts.\n\n    Args:\n        retries: Maximum number of retry attempts\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_retries = retries\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_max_budget","title":"with_max_budget","text":"<pre><code>with_max_budget(budget: float) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget</code> <code>float</code> <p>Maximum budget in USD</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_budget(self, budget: float) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum budget.\n\n    Args:\n        budget: Maximum budget in USD\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_budget = Decimal(str(budget))\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_error_policy","title":"with_error_policy","text":"<pre><code>with_error_policy(policy: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure error handling policy.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>Error policy ('skip', 'fail', 'retry', 'use_default')</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_error_policy(self, policy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure error handling policy.\n\n    Args:\n        policy: Error policy ('skip', 'fail', 'retry', 'use_default')\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.core.specifications import ErrorPolicy\n\n    self._processing_spec.error_policy = ErrorPolicy(policy.lower())\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_checkpoint_dir","title":"with_checkpoint_dir","text":"<pre><code>with_checkpoint_dir(directory: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to checkpoint directory</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_dir(self, directory: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint directory.\n\n    Args:\n        directory: Path to checkpoint directory\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_dir = Path(directory)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_parser","title":"with_parser","text":"<pre><code>with_parser(parser: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure response parser.</p> <p>This method allows setting a custom parser. The parser type determines the response_format in the prompt spec.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>any</code> <p>Parser instance (JSONParser, RegexParser, PydanticParser, etc.)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_parser(self, parser: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure response parser.\n\n    This method allows setting a custom parser. The parser type\n    determines the response_format in the prompt spec.\n\n    Args:\n        parser: Parser instance (JSONParser, RegexParser, PydanticParser, etc.)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    # Store the parser for later use in the pipeline\n    # We'll configure response_format based on parser type\n    if hasattr(parser, \"__class__\"):\n        parser_name = parser.__class__.__name__\n        if \"JSON\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            # Update the existing prompt spec's response_format\n            self._prompt_spec.response_format = \"json\"\n        elif \"Regex\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            self._prompt_spec.response_format = \"regex\"\n            if hasattr(parser, \"patterns\"):\n                self._prompt_spec.regex_patterns = parser.patterns\n\n    # Store the parser instance in metadata for the pipeline to use\n    if not hasattr(self, \"_custom_parser\"):\n        self._custom_parser = parser\n\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.to_csv","title":"to_csv","text":"<pre><code>to_csv(path: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV output destination.</p> <p>Alias for with_output(path, format='csv').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output CSV file path</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def to_csv(self, path: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV output destination.\n\n    Alias for with_output(path, format='csv').\n\n    Args:\n        path: Output CSV file path\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    return self.with_output(path, format=\"csv\")\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_output","title":"with_output","text":"<pre><code>with_output(path: str, format: str = 'csv', merge_strategy: str = 'replace') -&gt; PipelineBuilder\n</code></pre> <p>Configure output destination.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path</p> required <code>format</code> <code>str</code> <p>Output format (csv, excel, parquet)</p> <code>'csv'</code> <code>merge_strategy</code> <code>str</code> <p>Merge strategy (replace, append, update)</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_output(\n    self,\n    path: str,\n    format: str = \"csv\",\n    merge_strategy: str = \"replace\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure output destination.\n\n    Args:\n        path: Output file path\n        format: Output format (csv, excel, parquet)\n        merge_strategy: Merge strategy (replace, append, update)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    format_map = {\n        \"csv\": DataSourceType.CSV,\n        \"excel\": DataSourceType.EXCEL,\n        \"parquet\": DataSourceType.PARQUET,\n    }\n\n    merge_map = {\n        \"replace\": MergeStrategy.REPLACE,\n        \"append\": MergeStrategy.APPEND,\n        \"update\": MergeStrategy.UPDATE,\n    }\n\n    self._output_spec = OutputSpec(\n        destination_type=format_map[format.lower()],\n        destination_path=Path(path),\n        merge_strategy=merge_map[merge_strategy.lower()],\n    )\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_executor","title":"with_executor","text":"<pre><code>with_executor(executor: ExecutionStrategy) -&gt; PipelineBuilder\n</code></pre> <p>Set custom execution strategy.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ExecutionStrategy</code> <p>ExecutionStrategy instance</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_executor(self, executor: ExecutionStrategy) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set custom execution strategy.\n\n    Args:\n        executor: ExecutionStrategy instance\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = executor\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_async_execution","title":"with_async_execution","text":"<pre><code>with_async_execution(max_concurrency: int = 10) -&gt; PipelineBuilder\n</code></pre> <p>Use async execution strategy.</p> <p>Enables async/await for non-blocking execution. Ideal for FastAPI, aiohttp, and async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_async_execution(self, max_concurrency: int = 10) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use async execution strategy.\n\n    Enables async/await for non-blocking execution.\n    Ideal for FastAPI, aiohttp, and async frameworks.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = AsyncExecutor(max_concurrency=max_concurrency)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_streaming","title":"with_streaming","text":"<pre><code>with_streaming(chunk_size: int = 1000) -&gt; PipelineBuilder\n</code></pre> <p>Use streaming execution strategy.</p> <p>Processes data in chunks for memory-efficient handling. Ideal for large datasets (100K+ rows).</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_streaming(self, chunk_size: int = 1000) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use streaming execution strategy.\n\n    Processes data in chunks for memory-efficient handling.\n    Ideal for large datasets (100K+ rows).\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = StreamingExecutor(chunk_size=chunk_size)\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_stage","title":"with_stage","text":"<pre><code>with_stage(stage_name: str, position: str = 'before_prompt', **stage_kwargs) -&gt; PipelineBuilder\n</code></pre> <p>Add a custom pipeline stage by name.</p> <p>Enables injection of custom processing stages at specific points in the pipeline. Stages must be registered via StageRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Registered stage name (e.g., \"rag_retrieval\")</p> required <code>position</code> <code>str</code> <p>Where to inject the stage. Options: - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing - \"after_parser\": After response parsing</p> <code>'before_prompt'</code> <code>**stage_kwargs</code> <p>Arguments to pass to stage constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name not registered or position invalid</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_stage(\n    self,\n    stage_name: str,\n    position: str = \"before_prompt\",\n    **stage_kwargs,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add a custom pipeline stage by name.\n\n    Enables injection of custom processing stages at specific points\n    in the pipeline. Stages must be registered via StageRegistry.\n\n    Args:\n        stage_name: Registered stage name (e.g., \"rag_retrieval\")\n        position: Where to inject the stage. Options:\n            - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting\n            - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation\n            - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing\n            - \"after_parser\": After response parsing\n        **stage_kwargs: Arguments to pass to stage constructor\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If stage_name not registered or position invalid\n\n    Example:\n        # RAG retrieval example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])\n            .with_stage(\n                \"rag_retrieval\",\n                position=\"before_prompt\",\n                vector_store=\"pinecone\",\n                index_name=\"my-docs\",\n                top_k=5\n            )\n            .with_prompt(\"Context: {retrieved_context}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o\")\n            .build()\n        )\n\n        # Content moderation example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])\n            .with_stage(\n                \"content_moderation\",\n                position=\"before_llm\",\n                block_patterns=[\"spam\", \"offensive\"]\n            )\n            .with_prompt(\"Moderate: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n    \"\"\"\n    from ondine.stages.stage_registry import StageRegistry\n\n    # Validate position\n    valid_positions = [\n        \"after_loader\",\n        \"before_prompt\",\n        \"after_prompt\",\n        \"before_llm\",\n        \"after_llm\",\n        \"before_parser\",\n        \"after_parser\",\n    ]\n    if position not in valid_positions:\n        raise ValueError(\n            f\"Invalid position '{position}'. Must be one of: {', '.join(valid_positions)}\"\n        )\n\n    # Get stage class from registry (this will raise ValueError if not found)\n    stage_class = StageRegistry.get(stage_name)\n\n    # Store stage config for later instantiation\n    self._custom_stages.append(\n        {\n            \"name\": stage_name,\n            \"class\": stage_class,\n            \"position\": position,\n            \"kwargs\": stage_kwargs,\n        }\n    )\n\n    return self\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_stage--rag-retrieval-example","title":"RAG retrieval example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])     .with_stage(         \"rag_retrieval\",         position=\"before_prompt\",         vector_store=\"pinecone\",         index_name=\"my-docs\",         top_k=5     )     .with_prompt(\"Context: {retrieved_context}\\n\\nQuestion: {question}\\n\\nAnswer:\")     .with_llm(provider=\"openai\", model=\"gpt-4o\")     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.with_stage--content-moderation-example","title":"Content moderation example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])     .with_stage(         \"content_moderation\",         position=\"before_llm\",         block_patterns=[\"spam\", \"offensive\"]     )     .with_prompt(\"Moderate: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p>"},{"location":"api/api/#ondine.api.PipelineBuilder.build","title":"build","text":"<pre><code>build() -&gt; Pipeline\n</code></pre> <p>Build final Pipeline.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required specifications missing</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def build(self) -&gt; Pipeline:\n    \"\"\"\n    Build final Pipeline.\n\n    Returns:\n        Configured Pipeline\n\n    Raises:\n        ValueError: If required specifications missing\n    \"\"\"\n    # Validate required specs\n    if not self._dataset_spec:\n        raise ValueError(\"Dataset specification required\")\n    if not self._prompt_spec:\n        raise ValueError(\"Prompt specification required\")\n\n    # LLM spec is optional if custom client is provided\n    if not self._llm_spec and not self._custom_llm_client:\n        raise ValueError(\"Either LLM specification or custom LLM client required\")\n\n    # Prepare metadata with custom parser, custom client, and/or custom stages if provided\n    metadata = {}\n    if self._custom_parser is not None:\n        metadata[\"custom_parser\"] = self._custom_parser\n    if self._custom_llm_client is not None:\n        metadata[\"custom_llm_client\"] = self._custom_llm_client\n    if self._custom_stages:\n        metadata[\"custom_stages\"] = self._custom_stages\n\n    # Create specifications bundle\n    # If custom client provided but no llm_spec, create a dummy spec\n    llm_spec = self._llm_spec\n    if llm_spec is None and self._custom_llm_client is not None:\n        # Create minimal spec using custom client's attributes\n        llm_spec = LLMSpec(\n            provider=LLMProvider.OPENAI,  # Dummy provider\n            model=self._custom_llm_client.model,\n            temperature=self._custom_llm_client.temperature,\n            max_tokens=self._custom_llm_client.max_tokens,\n        )\n\n    specifications = PipelineSpecifications(\n        dataset=self._dataset_spec,\n        prompt=self._prompt_spec,\n        llm=llm_spec,\n        processing=self._processing_spec,\n        output=self._output_spec,\n        metadata=metadata,\n    )\n\n    # Create and return pipeline\n    return Pipeline(\n        specifications,\n        dataframe=self._dataframe,\n        executor=self._executor,\n    )\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer","title":"PipelineComposer","text":"<pre><code>PipelineComposer(input_data: str | Path | DataFrame)\n</code></pre> <p>Composes multiple pipelines to process independent columns.</p> <p>Each pipeline processes one output column. Pipelines can depend on outputs from previous pipelines, enabling sequential processing.</p> <p>Design Philosophy: - Keep individual pipelines simple (single responsibility) - Compose complex workflows from simple building blocks - Make dependencies explicit (no hidden coupling) - Fail fast with clear error messages</p> <p>Initialize pipeline composer.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str | Path | DataFrame</code> <p>Either a file path (CSV/Excel) or DataFrame</p> required Design Note <p>We accept both paths and DataFrames to support different workflows: - Path: Lazy loading, memory efficient - DataFrame: Already loaded, faster iteration</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def __init__(self, input_data: str | Path | pd.DataFrame):\n    \"\"\"\n    Initialize pipeline composer.\n\n    Args:\n        input_data: Either a file path (CSV/Excel) or DataFrame\n\n    Design Note:\n        We accept both paths and DataFrames to support different workflows:\n        - Path: Lazy loading, memory efficient\n        - DataFrame: Already loaded, faster iteration\n    \"\"\"\n    if isinstance(input_data, str | Path):\n        self.input_path = str(input_data)\n        self.input_df = None  # Lazy load\n    elif isinstance(input_data, pd.DataFrame):\n        self.input_path = None\n        self.input_df = input_data.copy()\n    else:\n        raise TypeError(\n            f\"input_data must be str, Path, or DataFrame, got {type(input_data)}\"\n        )\n\n    # Storage for column pipelines\n    # Format: (column_name, pipeline, dependencies)\n    self.column_pipelines: list[tuple[str, Pipeline, list[str]]] = []\n\n    logger.info(\"PipelineComposer initialized\")\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer.add_column","title":"add_column","text":"<pre><code>add_column(column_name: str, pipeline: Pipeline, depends_on: list[str] | None = None) -&gt; PipelineComposer\n</code></pre> <p>Add a pipeline for processing one output column.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of output column</p> required <code>pipeline</code> <code>Pipeline</code> <p>Pipeline to generate this column</p> required <code>depends_on</code> <code>list[str] | None</code> <p>List of columns this depends on (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Self for method chaining (fluent API)</p> Example <p>composer.add_column(\"score\", score_pipeline) composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])</p> Design Note <p>Fluent API (returns self) enables readable chaining: composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def add_column(\n    self,\n    column_name: str,\n    pipeline: Pipeline,\n    depends_on: list[str] | None = None,\n) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Add a pipeline for processing one output column.\n\n    Args:\n        column_name: Name of output column\n        pipeline: Pipeline to generate this column\n        depends_on: List of columns this depends on (optional)\n\n    Returns:\n        Self for method chaining (fluent API)\n\n    Example:\n        composer.add_column(\"score\", score_pipeline)\n        composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])\n\n    Design Note:\n        Fluent API (returns self) enables readable chaining:\n        composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()\n    \"\"\"\n    dependencies = depends_on or []\n\n    # Validate column name unique\n    existing_cols = [col for col, _, _ in self.column_pipelines]\n    if column_name in existing_cols:\n        raise ValueError(f\"Column '{column_name}' already added\")\n\n    self.column_pipelines.append((column_name, pipeline, dependencies))\n\n    logger.info(\n        f\"Added column '{column_name}' with dependencies: {dependencies or 'none'}\"\n    )\n\n    return self  # Enable chaining\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer.execute","title":"execute","text":"<pre><code>execute() -&gt; ExecutionResult\n</code></pre> <p>Execute all column pipelines in dependency order.</p> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with all columns merged</p> Algorithm <ol> <li>Load input data (lazy if needed)</li> <li>Resolve execution order (topological sort)</li> <li>For each column:    a. Inject current DataFrame into pipeline    b. Execute pipeline    c. Merge result column into DataFrame</li> <li>Aggregate metrics and return final result</li> </ol> Design Note <p>Each pipeline operates on the accumulating DataFrame, so later pipelines can use earlier outputs as inputs.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def execute(self) -&gt; ExecutionResult:\n    \"\"\"\n    Execute all column pipelines in dependency order.\n\n    Returns:\n        ExecutionResult with all columns merged\n\n    Algorithm:\n        1. Load input data (lazy if needed)\n        2. Resolve execution order (topological sort)\n        3. For each column:\n           a. Inject current DataFrame into pipeline\n           b. Execute pipeline\n           c. Merge result column into DataFrame\n        4. Aggregate metrics and return final result\n\n    Design Note:\n        Each pipeline operates on the accumulating DataFrame,\n        so later pipelines can use earlier outputs as inputs.\n    \"\"\"\n    # Lazy load if needed\n    if self.input_df is None:\n        logger.info(f\"Loading input data from {self.input_path}\")\n        # Use DataReader adapter for consistent file loading\n        from ondine.adapters.data_io import create_data_reader\n        from ondine.core.specifications import DataSourceType\n\n        # Detect source type from file extension\n        if self.input_path.endswith(\".xlsx\") or self.input_path.endswith(\".xls\"):\n            source_type = DataSourceType.EXCEL\n        elif self.input_path.endswith(\".csv\"):\n            source_type = DataSourceType.CSV\n        elif self.input_path.endswith(\".parquet\"):\n            source_type = DataSourceType.PARQUET\n        else:\n            raise ValueError(f\"Unsupported file type: {self.input_path}\")\n\n        # Create reader and load data\n        reader = create_data_reader(\n            source_type=source_type, source_path=self.input_path\n        )\n        self.input_df = reader.read()\n\n    # Start with input data\n    df = self.input_df.copy()\n\n    # Resolve execution order\n    execution_order = self._get_execution_order()\n\n    logger.info(\n        f\"Executing {len(execution_order)} column pipelines in order: \"\n        f\"{[col for col, _, _ in execution_order]}\"\n    )\n\n    # Track metrics (keep Decimal precision for costs)\n    from decimal import Decimal\n\n    total_cost = Decimal(\"0.0\")\n    total_errors = []\n\n    # Execute each column pipeline\n    for col_name, pipeline, _deps in execution_order:\n        logger.info(f\"Processing column '{col_name}'...\")\n\n        # Inject current DataFrame (includes previous outputs)\n        pipeline.dataframe = df\n\n        # Execute pipeline\n        result = pipeline.execute()\n\n        # Merge new column\n        if col_name in result.data.columns:\n            df[col_name] = result.data[col_name]\n        else:\n            logger.warning(\n                f\"Pipeline for '{col_name}' didn't produce expected column\"\n            )\n\n        # Accumulate metrics (preserve Decimal precision)\n        cost_to_add = result.costs.total_cost\n        if not isinstance(cost_to_add, Decimal):\n            cost_to_add = Decimal(str(cost_to_add))\n        total_cost += cost_to_add\n        total_errors.extend(result.errors)\n\n        logger.info(\n            f\"Column '{col_name}' complete: \"\n            f\"{len(df)} rows, ${result.costs.total_cost:.4f}\"\n        )\n\n    # Create final result\n    final_result = ExecutionResult(\n        data=df,\n        metrics=ProcessingStats(\n            total_rows=len(df),\n            processed_rows=len(df),\n            failed_rows=len(total_errors),\n            skipped_rows=0,\n            rows_per_second=0.0,  # Aggregate metric not meaningful\n            total_duration_seconds=0.0,  # Would need timing\n        ),\n        costs=CostEstimate(\n            total_cost=total_cost,\n            total_tokens=0,  # Aggregate from individual pipelines\n            input_tokens=0,\n            output_tokens=0,\n            rows=len(df),\n        ),\n        errors=total_errors,\n    )\n\n    logger.info(\n        f\"Composition complete: {len(execution_order)} columns, \"\n        f\"${total_cost:.4f} total cost\"\n    )\n\n    return final_result\n</code></pre>"},{"location":"api/api/#ondine.api.PipelineComposer.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(config_path: str) -&gt; PipelineComposer\n</code></pre> <p>Load composer configuration from YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to composition config file</p> required <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Configured PipelineComposer</p> Example YAML <p>composition:   input: \"data.xlsx\"   pipelines:     - column: col1       config: pipeline1.yaml     - column: col2       depends_on: [col1]       config: pipeline2.yaml</p> Design Note <p>This enables pure YAML workflows without Python code.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>@classmethod\ndef from_yaml(cls, config_path: str) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Load composer configuration from YAML.\n\n    Args:\n        config_path: Path to composition config file\n\n    Returns:\n        Configured PipelineComposer\n\n    Example YAML:\n        composition:\n          input: \"data.xlsx\"\n          pipelines:\n            - column: col1\n              config: pipeline1.yaml\n            - column: col2\n              depends_on: [col1]\n              config: pipeline2.yaml\n\n    Design Note:\n        This enables pure YAML workflows without Python code.\n    \"\"\"\n    import yaml\n\n    from ondine.config import ConfigLoader\n\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n\n    composition = config.get(\"composition\", {})\n    input_data = composition.get(\"input\")\n\n    if not input_data:\n        raise ValueError(\"composition.input is required\")\n\n    composer = cls(input_data=input_data)\n\n    # Load each pipeline config\n    for pipeline_config in composition.get(\"pipelines\", []):\n        col_name = pipeline_config[\"column\"]\n        config_file = pipeline_config[\"config\"]\n        depends_on = pipeline_config.get(\"depends_on\", [])\n\n        # Load pipeline from its config\n        pipeline_specs = ConfigLoader.from_yaml(config_file)\n        pipeline = Pipeline(pipeline_specs)\n\n        composer.add_column(col_name, pipeline, depends_on)\n\n    return composer\n</code></pre>"},{"location":"api/api/#ondine.api.QuickPipeline","title":"QuickPipeline","text":"<p>Simplified pipeline API with smart defaults.</p> <p>Designed for rapid prototyping and common use cases. Automatically detects: - Input columns from prompt template placeholders - Provider from model name (e.g., gpt-4 \u2192 openai, claude \u2192 anthropic) - Parser type (JSON for multi-column, text for single column) - Reasonable defaults for batch size, concurrency, retries</p> <p>Examples:</p> <p>Minimal usage:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"data.csv\",\n...     prompt=\"Categorize this text: {text}\"\n... )\n&gt;&gt;&gt; result = pipeline.execute()\n</code></pre> <p>With explicit outputs:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"products.csv\",\n...     prompt=\"Extract: {description}\",\n...     output_columns=[\"brand\", \"model\", \"price\"]\n... )\n</code></pre> <p>Override defaults:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=df,\n...     prompt=\"Summarize: {content}\",\n...     model=\"gpt-4o\",\n...     temperature=0.7,\n...     max_budget=Decimal(\"5.0\")\n... )\n</code></pre>"},{"location":"api/api/#ondine.api.QuickPipeline.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(data: str | Path | DataFrame, prompt: str, model: str = 'gpt-4o-mini', output_columns: list[str] | str | None = None, provider: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, max_budget: Decimal | float | str | None = None, batch_size: int | None = None, concurrency: int | None = None, **kwargs: Any) -&gt; Pipeline\n</code></pre> <p>Create a pipeline with smart defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path | DataFrame</code> <p>CSV/Excel/Parquet file path or DataFrame</p> required <code>prompt</code> <code>str</code> <p>Prompt template with {placeholders}</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o-mini)</p> <code>'gpt-4o-mini'</code> <code>output_columns</code> <code>list[str] | str | None</code> <p>Output column name(s). If None, uses [\"output\"]</p> <code>None</code> <code>provider</code> <code>str | None</code> <p>LLM provider. If None, auto-detected from model name</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens (default: provider's default)</p> <code>None</code> <code>max_budget</code> <code>Decimal | float | str | None</code> <p>Maximum cost budget in USD</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Rows per batch (default: auto-sized based on data)</p> <code>None</code> <code>concurrency</code> <code>int | None</code> <p>Parallel requests (default: auto-sized)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to PipelineBuilder</p> <code>{}</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data cannot be loaded or prompt is invalid</p> Source code in <code>ondine/api/quick.py</code> <pre><code>@staticmethod\ndef create(\n    data: str | Path | pd.DataFrame,\n    prompt: str,\n    model: str = \"gpt-4o-mini\",\n    output_columns: list[str] | str | None = None,\n    provider: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    max_budget: Decimal | float | str | None = None,\n    batch_size: int | None = None,\n    concurrency: int | None = None,\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"\n    Create a pipeline with smart defaults.\n\n    Args:\n        data: CSV/Excel/Parquet file path or DataFrame\n        prompt: Prompt template with {placeholders}\n        model: Model name (default: gpt-4o-mini)\n        output_columns: Output column name(s). If None, uses [\"output\"]\n        provider: LLM provider. If None, auto-detected from model name\n        temperature: Sampling temperature (default: 0.0 for deterministic)\n        max_tokens: Max output tokens (default: provider's default)\n        max_budget: Maximum cost budget in USD\n        batch_size: Rows per batch (default: auto-sized based on data)\n        concurrency: Parallel requests (default: auto-sized)\n        **kwargs: Additional arguments passed to PipelineBuilder\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If input data cannot be loaded or prompt is invalid\n    \"\"\"\n    # 1. Load data\n    df = QuickPipeline._load_data(data)\n\n    # 2. Auto-detect input columns from prompt template\n    input_columns = QuickPipeline._extract_placeholders(prompt)\n    if not input_columns:\n        raise ValueError(\n            f\"No placeholders found in prompt: {prompt}\\n\"\n            \"Expected format: 'Your prompt with {{column_name}} placeholders'\"\n        )\n\n    # Validate input columns exist in data\n    missing = [col for col in input_columns if col not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Input columns {missing} not found in data. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    # 3. Normalize output columns\n    if output_columns is None:\n        output_columns = [\"output\"]\n    elif isinstance(output_columns, str):\n        output_columns = [output_columns]\n\n    # 4. Auto-detect provider from model name\n    if provider is None:\n        provider = QuickPipeline._detect_provider(model)\n\n    # 5. Auto-select parser (JSON for multi-column, text for single)\n    parser = QuickPipeline._select_parser(output_columns)\n\n    # 6. Smart defaults for batch_size and concurrency\n    if batch_size is None:\n        batch_size = QuickPipeline._default_batch_size(len(df))\n    if concurrency is None:\n        concurrency = QuickPipeline._default_concurrency(provider)\n\n    # 7. Build pipeline\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df, input_columns=input_columns, output_columns=output_columns\n        )\n        .with_prompt(template=prompt)\n        .with_llm(\n            provider=provider,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n    )\n\n    # Add optional parser if multi-column\n    if parser:\n        builder = builder.with_parser(parser)\n\n    # Add batch/concurrency settings\n    builder = builder.with_batch_size(batch_size).with_concurrency(concurrency)\n\n    # Add budget if specified\n    if max_budget is not None:\n        # Convert to float for PipelineBuilder (it expects float)\n        if isinstance(max_budget, Decimal | str):\n            max_budget = float(max_budget)\n        builder = builder.with_max_budget(budget=max_budget)\n\n    # Add sensible retry defaults\n    builder = builder.with_max_retries(3)\n\n    return builder.build()\n</code></pre>"},{"location":"api/api/dataset_processor/","title":"dataset_processor","text":""},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor","title":"dataset_processor","text":"<p>DatasetProcessor - Simplified convenience wrapper.</p> <p>For users who just want to process data with minimal configuration.</p>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor","title":"DatasetProcessor","text":"<pre><code>DatasetProcessor(data: str | DataFrame, input_column: str, output_column: str, prompt: str, llm_config: dict[str, any])\n</code></pre> <p>Simplified API for single-prompt, single-column use cases.</p> <p>This is a convenience wrapper around PipelineBuilder for users who don't need fine-grained control.</p> Example <p>processor = DatasetProcessor(     data=\"data.csv\",     input_column=\"description\",     output_column=\"cleaned\",     prompt=\"Clean this text: {description}\",     llm_config={\"provider\": \"openai\", \"model\": \"gpt-4o-mini\"} ) result = processor.run()</p> <p>Initialize dataset processor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame</code> <p>CSV file path or DataFrame</p> required <code>input_column</code> <code>str</code> <p>Input column name</p> required <code>output_column</code> <code>str</code> <p>Output column name</p> required <code>prompt</code> <code>str</code> <p>Prompt template</p> required <code>llm_config</code> <code>dict[str, any]</code> <p>LLM configuration dict</p> required Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def __init__(\n    self,\n    data: str | pd.DataFrame,\n    input_column: str,\n    output_column: str,\n    prompt: str,\n    llm_config: dict[str, any],\n):\n    \"\"\"\n    Initialize dataset processor.\n\n    Args:\n        data: CSV file path or DataFrame\n        input_column: Input column name\n        output_column: Output column name\n        prompt: Prompt template\n        llm_config: LLM configuration dict\n    \"\"\"\n    self.data = data\n    self.input_column = input_column\n    self.output_column = output_column\n    self.prompt = prompt\n    self.llm_config = llm_config\n\n    # Build pipeline internally\n    builder = PipelineBuilder.create()\n\n    # Configure data source\n    if isinstance(data, str):\n        builder.from_csv(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    elif isinstance(data, pd.DataFrame):\n        builder.from_dataframe(\n            data,\n            input_columns=[input_column],\n            output_columns=[output_column],\n        )\n    else:\n        raise ValueError(\"data must be file path or DataFrame\")\n\n    # Configure prompt\n    builder.with_prompt(prompt)\n\n    # Configure LLM\n    provider = llm_config.get(\"provider\", \"openai\")\n    model = llm_config.get(\"model\", \"gpt-4o-mini\")\n    api_key = llm_config.get(\"api_key\")\n    temperature = llm_config.get(\"temperature\", 0.0)\n    max_tokens = llm_config.get(\"max_tokens\")\n\n    builder.with_llm(\n        provider=provider,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n\n    # Build pipeline\n    self.pipeline = builder.build()\n</code></pre>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor.run","title":"run","text":"<pre><code>run() -&gt; pd.DataFrame\n</code></pre> <p>Execute processing and return results.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Execute processing and return results.\n\n    Returns:\n        DataFrame with results\n    \"\"\"\n    result = self.pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor.run_sample","title":"run_sample","text":"<pre><code>run_sample(n: int = 10) -&gt; pd.DataFrame\n</code></pre> <p>Test on first N rows.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of rows to process</p> <code>10</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with sample results</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def run_sample(self, n: int = 10) -&gt; pd.DataFrame:\n    \"\"\"\n    Test on first N rows.\n\n    Args:\n        n: Number of rows to process\n\n    Returns:\n        DataFrame with sample results\n    \"\"\"\n    # Create sample pipeline\n    if isinstance(self.data, str):\n        df = pd.read_csv(self.data).head(n)\n    else:\n        df = self.data.head(n)\n\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df,\n            input_columns=[self.input_column],\n            output_columns=[self.output_column],\n        )\n        .with_prompt(self.prompt)\n        .with_llm(\n            provider=self.llm_config.get(\"provider\", \"openai\"),\n            model=self.llm_config.get(\"model\", \"gpt-4o-mini\"),\n            api_key=self.llm_config.get(\"api_key\"),\n            temperature=self.llm_config.get(\"temperature\", 0.0),\n        )\n    )\n\n    sample_pipeline = builder.build()\n    result = sample_pipeline.execute()\n    return result.data\n</code></pre>"},{"location":"api/api/dataset_processor/#ondine.api.dataset_processor.DatasetProcessor.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; float\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost in USD</p> Source code in <code>ondine/api/dataset_processor.py</code> <pre><code>def estimate_cost(self) -&gt; float:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Estimated cost in USD\n    \"\"\"\n    estimate = self.pipeline.estimate_cost()\n    return float(estimate.total_cost)\n</code></pre>"},{"location":"api/api/health_check/","title":"health_check","text":""},{"location":"api/api/health_check/#ondine.api.health_check","title":"health_check","text":"<p>Health check API for pipeline monitoring.</p> <p>Provides status information for operational monitoring.</p>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck","title":"HealthCheck","text":"<pre><code>HealthCheck(pipeline: Pipeline)\n</code></pre> <p>Health check API for monitoring pipeline status.</p> <p>Provides information about pipeline health and readiness.</p> <p>Initialize health check.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>Pipeline instance to monitor</p> required Source code in <code>ondine/api/health_check.py</code> <pre><code>def __init__(self, pipeline: Pipeline):\n    \"\"\"\n    Initialize health check.\n\n    Args:\n        pipeline: Pipeline instance to monitor\n    \"\"\"\n    self.pipeline = pipeline\n    self.last_check: datetime | None = None\n    self.last_status: dict[str, Any] = {}\n</code></pre>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck.check","title":"check","text":"<pre><code>check() -&gt; dict[str, Any]\n</code></pre> <p>Perform health check.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Health status dictionary</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def check(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Perform health check.\n\n    Returns:\n        Health status dictionary\n    \"\"\"\n    self.last_check = datetime.now()\n\n    status = {\n        \"status\": \"healthy\",\n        \"timestamp\": self.last_check.isoformat(),\n        \"pipeline_id\": str(self.pipeline.id),\n        \"checks\": {},\n    }\n\n    # Check LLM provider configuration\n    try:\n        llm_spec = self.pipeline.specifications.llm\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"ok\",\n            \"provider\": llm_spec.provider.value,\n            \"model\": llm_spec.model,\n        }\n    except Exception as e:\n        status[\"checks\"][\"llm_provider\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check data source configuration\n    try:\n        dataset_spec = self.pipeline.specifications.dataset\n        source_exists = True\n\n        if dataset_spec.source_path:\n            source_exists = dataset_spec.source_path.exists()\n\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"ok\" if source_exists else \"warning\",\n            \"source_type\": dataset_spec.source_type.value,\n            \"exists\": source_exists,\n        }\n    except Exception as e:\n        status[\"checks\"][\"data_source\"] = {\n            \"status\": \"error\",\n            \"error\": str(e),\n        }\n        status[\"status\"] = \"unhealthy\"\n\n    # Check checkpoint storage\n    try:\n        checkpoint_dir = self.pipeline.specifications.processing.checkpoint_dir\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"ok\",\n            \"directory\": str(checkpoint_dir),\n            \"exists\": checkpoint_dir.exists(),\n        }\n    except Exception as e:\n        status[\"checks\"][\"checkpoint_storage\"] = {\n            \"status\": \"warning\",\n            \"error\": str(e),\n        }\n\n    # Store last status\n    self.last_status = status\n\n    return status\n</code></pre>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck.is_healthy","title":"is_healthy","text":"<pre><code>is_healthy() -&gt; bool\n</code></pre> <p>Check if pipeline is healthy.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if healthy</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def is_healthy(self) -&gt; bool:\n    \"\"\"\n    Check if pipeline is healthy.\n\n    Returns:\n        True if healthy\n    \"\"\"\n    status = self.check()\n    return status[\"status\"] == \"healthy\"\n</code></pre>"},{"location":"api/api/health_check/#ondine.api.health_check.HealthCheck.get_readiness","title":"get_readiness","text":"<pre><code>get_readiness() -&gt; dict[str, Any]\n</code></pre> <p>Get readiness status.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Readiness information</p> Source code in <code>ondine/api/health_check.py</code> <pre><code>def get_readiness(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get readiness status.\n\n    Returns:\n        Readiness information\n    \"\"\"\n    validation = self.pipeline.validate()\n\n    return {\n        \"ready\": validation.is_valid,\n        \"errors\": validation.errors,\n        \"warnings\": validation.warnings,\n        \"timestamp\": datetime.now().isoformat(),\n    }\n</code></pre>"},{"location":"api/api/pipeline/","title":"pipeline","text":""},{"location":"api/api/pipeline/#ondine.api.pipeline","title":"pipeline","text":"<p>Main Pipeline class - the Facade for the entire system.</p> <p>This is the primary entry point that users interact with.</p>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(specifications: PipelineSpecifications, dataframe: DataFrame | None = None, executor: ExecutionStrategy | None = None)\n</code></pre> <p>Main pipeline class - Facade for dataset processing.</p> <p>Provides high-level interface for building and executing LLM-powered data transformations.</p> Example <p>pipeline = Pipeline(specifications) result = pipeline.execute()</p> <p>Initialize pipeline with specifications.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Complete pipeline configuration</p> required <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded DataFrame</p> <code>None</code> <code>executor</code> <code>ExecutionStrategy | None</code> <p>Optional execution strategy (default: SyncExecutor)</p> <code>None</code> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def __init__(\n    self,\n    specifications: PipelineSpecifications,\n    dataframe: pd.DataFrame | None = None,\n    executor: ExecutionStrategy | None = None,\n):\n    \"\"\"\n    Initialize pipeline with specifications.\n\n    Args:\n        specifications: Complete pipeline configuration\n        dataframe: Optional pre-loaded DataFrame\n        executor: Optional execution strategy (default: SyncExecutor)\n    \"\"\"\n    self.id = uuid4()\n    self.specifications = specifications\n    self.dataframe = dataframe\n    self.executor = executor or SyncExecutor()\n    self.observers: list[ExecutionObserver] = []\n    self.logger = get_logger(f\"{__name__}.{self.id}\")\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; Pipeline\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"Pipeline\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.validate","title":"validate","text":"<pre><code>validate() -&gt; ValidationResult\n</code></pre> <p>Validate pipeline configuration.</p> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with any errors/warnings</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def validate(self) -&gt; ValidationResult:\n    \"\"\"\n    Validate pipeline configuration.\n\n    Returns:\n        ValidationResult with any errors/warnings\n    \"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Validate dataset spec\n    if not self.specifications.dataset.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    if not self.specifications.dataset.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    # Validate that input columns exist in dataframe (if dataframe is provided)\n    if self.dataframe is not None and self.specifications.dataset.input_columns:\n        df_cols = set(self.dataframe.columns)\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_cols = input_cols - df_cols\n        if missing_cols:\n            result.add_error(\n                f\"Input columns not found in dataframe: {missing_cols}\"\n            )\n\n    # Validate prompt spec\n    if not self.specifications.prompt.template:\n        result.add_error(\"No prompt template specified\")\n    else:\n        # Check that template variables match input columns\n        import re\n\n        template_vars = set(\n            re.findall(r\"\\{(\\w+)\\}\", self.specifications.prompt.template)\n        )\n        input_cols = set(self.specifications.dataset.input_columns)\n        missing_vars = template_vars - input_cols\n        if missing_vars:\n            result.add_error(\n                f\"Template variables not in input columns: {missing_vars}\"\n            )\n\n    # Validate LLM spec\n    if not self.specifications.llm.model:\n        result.add_error(\"No LLM model specified\")\n\n    return result\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost() -&gt; CostEstimate\n</code></pre> <p>Estimate total processing cost.</p> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def estimate_cost(self) -&gt; CostEstimate:\n    \"\"\"\n    Estimate total processing cost.\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    # Create stages\n    loader = DataLoaderStage(self.dataframe)\n\n    # Load first few rows for estimation\n    df = loader.process(self.specifications.dataset, ExecutionContext())\n    sample_size = min(10, len(df))\n    sample_df = df.head(sample_size)\n\n    # Create formatter and get prompts\n    formatter = PromptFormatterStage(self.specifications.processing.batch_size)\n    batches = formatter.process(\n        (sample_df, self.specifications.prompt), ExecutionContext()\n    )\n\n    # Create LLM client and estimate\n    llm_client = create_llm_client(self.specifications.llm)\n    llm_stage = LLMInvocationStage(llm_client)\n\n    sample_estimate = llm_stage.estimate_cost(batches)\n\n    # Scale to full dataset\n    scale_factor = Decimal(len(df)) / Decimal(sample_size)\n\n    return CostEstimate(\n        total_cost=sample_estimate.total_cost * scale_factor,\n        total_tokens=int(sample_estimate.total_tokens * float(scale_factor)),\n        input_tokens=int(sample_estimate.input_tokens * float(scale_factor)),\n        output_tokens=int(sample_estimate.output_tokens * float(scale_factor)),\n        rows=len(df),\n        confidence=\"sample-based\",\n    )\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.execute","title":"execute","text":"<pre><code>execute(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    # Validate first\n    validation = self.validate()\n    if not validation.is_valid:\n        raise ValueError(f\"Pipeline validation failed: {validation.errors}\")\n\n    # Create or restore execution context\n    state_manager = StateManager(\n        storage=LocalFileCheckpointStorage(\n            self.specifications.processing.checkpoint_dir\n        ),\n        checkpoint_interval=self.specifications.processing.checkpoint_interval,\n    )\n\n    if resume_from:\n        # Resume from checkpoint\n        context = state_manager.load_checkpoint(resume_from)\n        if not context:\n            raise ValueError(f\"No checkpoint found for session {resume_from}\")\n        self.logger.info(\n            f\"Resuming from checkpoint at row {context.last_processed_row}\"\n        )\n    else:\n        # Create new context\n        context = ExecutionContext(pipeline_id=self.id)\n\n    # Add default observers if none specified\n    if not self.observers:\n        self.observers = [\n            ProgressBarObserver(),\n            LoggingObserver(),\n            CostTrackingObserver(),\n        ]\n\n    # Attach observers to context for progress notifications\n    context.observers = self.observers\n\n    # Notify observers of start\n    for observer in self.observers:\n        observer.on_pipeline_start(self, context)\n\n    try:\n        # Execute stages (preprocessing happens inside if enabled)\n        result_df = self._execute_stages(context, state_manager)\n\n        # Mark completion\n        context.end_time = datetime.now()\n\n        # Create execution result\n        result = ExecutionResult(\n            data=result_df,\n            metrics=context.get_stats(),\n            costs=CostEstimate(\n                total_cost=context.accumulated_cost,\n                total_tokens=context.accumulated_tokens,\n                input_tokens=0,\n                output_tokens=0,\n                rows=context.total_rows,\n                confidence=\"actual\",\n            ),\n            execution_id=context.session_id,\n            start_time=context.start_time,\n            end_time=context.end_time,\n            success=True,\n        )\n\n        # Optional: Auto-retry failed rows\n        if self.specifications.processing.auto_retry_failed:\n            # Get preprocessed data from context (or loaded data if no preprocessing)\n            retry_source_df = context.intermediate_data.get(\"preprocessed_data\")\n            if retry_source_df is None:\n                retry_source_df = context.intermediate_data.get(\"loaded_data\")\n            result = self._auto_retry_failed_rows(result, retry_source_df)\n\n        # Cleanup checkpoints on success\n        state_manager.cleanup_checkpoints(context.session_id)\n\n        # Notify observers of completion\n        for observer in self.observers:\n            observer.on_pipeline_complete(context, result)\n\n        return result\n\n    except Exception as e:\n        # Save checkpoint on error\n        state_manager.save_checkpoint(context)\n        self.logger.error(\n            f\"Pipeline failed. Checkpoint saved. \"\n            f\"Resume with: pipeline.execute(resume_from=UUID('{context.session_id}'))\"\n        )\n\n        # Notify observers of error\n        for observer in self.observers:\n            observer.on_pipeline_error(context, e)\n        raise\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(resume_from: UUID | None = None) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline asynchronously.</p> <p>Uses AsyncExecutor for non-blocking execution. Ideal for integration with FastAPI, aiohttp, and other async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>resume_from</code> <code>UUID | None</code> <p>Optional session ID to resume from checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support async</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>async def execute_async(self, resume_from: UUID | None = None) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline asynchronously.\n\n    Uses AsyncExecutor for non-blocking execution. Ideal for integration\n    with FastAPI, aiohttp, and other async frameworks.\n\n    Args:\n        resume_from: Optional session ID to resume from checkpoint\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        ValueError: If executor doesn't support async\n    \"\"\"\n    if not self.executor.supports_async():\n        raise ValueError(\n            \"Current executor doesn't support async. \"\n            \"Use AsyncExecutor: Pipeline(specs, executor=AsyncExecutor())\"\n        )\n\n    # For now, wrap synchronous execution in async\n    # TODO: Implement fully async execution pipeline\n    import asyncio\n\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, self.execute, resume_from)\n</code></pre>"},{"location":"api/api/pipeline/#ondine.api.pipeline.Pipeline.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(chunk_size: int | None = None) -&gt; Iterator[ExecutionResult]\n</code></pre> <p>Execute pipeline in streaming mode.</p> <p>Processes data in chunks for memory-efficient handling of large datasets. Ideal for datasets that don't fit in memory.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int | None</code> <p>Number of rows per chunk (uses executor's chunk_size if None)</p> <code>None</code> <p>Yields:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult objects for each processed chunk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If executor doesn't support streaming</p> Source code in <code>ondine/api/pipeline.py</code> <pre><code>def execute_stream(\n    self, chunk_size: int | None = None\n) -&gt; Iterator[ExecutionResult]:\n    \"\"\"\n    Execute pipeline in streaming mode.\n\n    Processes data in chunks for memory-efficient handling of large datasets.\n    Ideal for datasets that don't fit in memory.\n\n    Args:\n        chunk_size: Number of rows per chunk (uses executor's chunk_size if None)\n\n    Yields:\n        ExecutionResult objects for each processed chunk\n\n    Raises:\n        ValueError: If executor doesn't support streaming\n    \"\"\"\n    if not self.executor.supports_streaming():\n        raise ValueError(\n            \"Current executor doesn't support streaming. \"\n            \"Use StreamingExecutor: Pipeline(specs, executor=StreamingExecutor())\"\n        )\n\n    # Use executor's chunk_size if not provided\n    if chunk_size is None and isinstance(self.executor, StreamingExecutor):\n        chunk_size = self.executor.chunk_size\n    elif chunk_size is None:\n        chunk_size = 1000  # Default fallback\n\n    # For now, execute the full pipeline and split result into chunks\n    # TODO: Implement proper streaming execution that processes chunks independently\n    result = self.execute()\n\n    # Split the result data into chunks and yield as separate ExecutionResults\n    total_rows = len(result.data)\n    for start_idx in range(0, total_rows, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_rows)\n        chunk_data = result.data.iloc[start_idx:end_idx].copy()\n\n        # Create a chunk result with proportional metrics\n        chunk_rows = len(chunk_data)\n        chunk_result = ExecutionResult(\n            data=chunk_data,\n            metrics=ProcessingStats(\n                total_rows=chunk_rows,\n                processed_rows=chunk_rows,\n                failed_rows=0,\n                skipped_rows=0,\n                rows_per_second=result.metrics.rows_per_second,\n                total_duration_seconds=result.metrics.total_duration_seconds\n                * (chunk_rows / total_rows),\n                stage_durations=result.metrics.stage_durations,\n            ),\n            costs=CostEstimate(\n                total_cost=result.costs.total_cost\n                * Decimal(chunk_rows / total_rows),\n                total_tokens=int(\n                    result.costs.total_tokens * (chunk_rows / total_rows)\n                ),\n                input_tokens=int(\n                    result.costs.input_tokens * (chunk_rows / total_rows)\n                ),\n                output_tokens=int(\n                    result.costs.output_tokens * (chunk_rows / total_rows)\n                ),\n                rows=chunk_rows,\n                confidence=result.costs.confidence,\n            ),\n            execution_id=result.execution_id,\n            start_time=result.start_time,\n            end_time=result.end_time,\n            success=True,\n        )\n        yield chunk_result\n</code></pre>"},{"location":"api/api/pipeline_builder/","title":"pipeline_builder","text":""},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder","title":"pipeline_builder","text":"<p>Pipeline Builder - Fluent API for constructing pipelines.</p> <p>Implements Builder pattern for intuitive pipeline creation.</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder","title":"PipelineBuilder","text":"<pre><code>PipelineBuilder()\n</code></pre> <p>Fluent builder for constructing pipelines.</p> <p>Provides an intuitive, chainable API for common use cases.</p> Example <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p> <p>Initialize builder with None values.</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize builder with None values.\"\"\"\n    self._dataset_spec: DatasetSpec | None = None\n    self._prompt_spec: PromptSpec | None = None\n    self._llm_spec: LLMSpec | None = None\n    self._processing_spec: ProcessingSpec = ProcessingSpec()\n    self._output_spec: OutputSpec | None = None\n    self._dataframe: pd.DataFrame | None = None\n    self._executor: ExecutionStrategy | None = None\n    self._custom_parser: any | None = None\n    self._custom_llm_client: any | None = None\n    self._custom_stages: list[dict] = []  # For custom stage injection\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create() -&gt; PipelineBuilder\n</code></pre> <p>Start builder chain.</p> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>New PipelineBuilder instance</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef create() -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Start builder chain.\n\n    Returns:\n        New PipelineBuilder instance\n    \"\"\"\n    return PipelineBuilder()\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_specifications","title":"from_specifications  <code>staticmethod</code>","text":"<pre><code>from_specifications(specs: PipelineSpecifications) -&gt; PipelineBuilder\n</code></pre> <p>Create builder from existing specifications.</p> <p>Useful for loading from YAML and modifying programmatically.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>PipelineSpecifications</code> <p>Complete pipeline specifications</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>PipelineBuilder pre-configured with specs</p> Example <p>specs = load_pipeline_config(\"config.yaml\") builder = PipelineBuilder.from_specifications(specs) pipeline = builder.build()</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>@staticmethod\ndef from_specifications(specs: PipelineSpecifications) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Create builder from existing specifications.\n\n    Useful for loading from YAML and modifying programmatically.\n\n    Args:\n        specs: Complete pipeline specifications\n\n    Returns:\n        PipelineBuilder pre-configured with specs\n\n    Example:\n        specs = load_pipeline_config(\"config.yaml\")\n        builder = PipelineBuilder.from_specifications(specs)\n        pipeline = builder.build()\n    \"\"\"\n    builder = PipelineBuilder()\n    builder._dataset_spec = specs.dataset\n    builder._prompt_spec = specs.prompt\n    builder._llm_spec = specs.llm\n    builder._processing_spec = specs.processing\n    builder._output_spec = specs.output\n    return builder\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_csv","title":"from_csv","text":"<pre><code>from_csv(path: str, input_columns: list[str], output_columns: list[str], delimiter: str = ',', encoding: str = 'utf-8') -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>delimiter</code> <code>str</code> <p>CSV delimiter</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_csv(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    delimiter: str = \",\",\n    encoding: str = \"utf-8\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV data source.\n\n    Args:\n        path: Path to CSV file\n        input_columns: Input column names\n        output_columns: Output column names\n        delimiter: CSV delimiter\n        encoding: File encoding\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.CSV,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        delimiter=delimiter,\n        encoding=encoding,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_excel","title":"from_excel","text":"<pre><code>from_excel(path: str, input_columns: list[str], output_columns: list[str], sheet_name: str | int = 0) -&gt; PipelineBuilder\n</code></pre> <p>Configure Excel data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Excel file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <code>sheet_name</code> <code>str | int</code> <p>Sheet name or index</p> <code>0</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_excel(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n    sheet_name: str | int = 0,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Excel data source.\n\n    Args:\n        path: Path to Excel file\n        input_columns: Input column names\n        output_columns: Output column names\n        sheet_name: Sheet name or index\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.EXCEL,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n        sheet_name=sheet_name,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_parquet","title":"from_parquet","text":"<pre><code>from_parquet(path: str, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure Parquet data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_parquet(\n    self,\n    path: str,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure Parquet data source.\n\n    Args:\n        path: Path to Parquet file\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.PARQUET,\n        source_path=Path(path),\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.from_dataframe","title":"from_dataframe","text":"<pre><code>from_dataframe(df: DataFrame, input_columns: list[str], output_columns: list[str]) -&gt; PipelineBuilder\n</code></pre> <p>Configure DataFrame source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required <code>input_columns</code> <code>list[str]</code> <p>Input column names</p> required <code>output_columns</code> <code>list[str]</code> <p>Output column names</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def from_dataframe(\n    self,\n    df: pd.DataFrame,\n    input_columns: list[str],\n    output_columns: list[str],\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure DataFrame source.\n\n    Args:\n        df: Pandas DataFrame\n        input_columns: Input column names\n        output_columns: Output column names\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._dataset_spec = DatasetSpec(\n        source_type=DataSourceType.DATAFRAME,\n        input_columns=input_columns,\n        output_columns=output_columns,\n    )\n    self._dataframe = df\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_prompt","title":"with_prompt","text":"<pre><code>with_prompt(template: str, system_message: str | None = None) -&gt; PipelineBuilder\n</code></pre> <p>Configure prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>Prompt template with {variable} placeholders</p> required <code>system_message</code> <code>str | None</code> <p>Optional system message</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_prompt(\n    self,\n    template: str,\n    system_message: str | None = None,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure prompt template.\n\n    Args:\n        template: Prompt template with {variable} placeholders\n        system_message: Optional system message\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._prompt_spec = PromptSpec(\n        template=template,\n        system_message=system_message,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm","title":"with_llm","text":"<pre><code>with_llm(provider: str, model: str, api_key: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, **kwargs: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (openai, azure_openai, anthropic) or custom provider ID</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>api_key</code> <code>str | None</code> <p>API key (or from env)</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens</p> <code>None</code> <code>**kwargs</code> <code>any</code> <p>Provider-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm(\n    self,\n    provider: str,\n    model: str,\n    api_key: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    **kwargs: any,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM provider.\n\n    Args:\n        provider: Provider name (openai, azure_openai, anthropic) or custom provider ID\n        model: Model identifier\n        api_key: API key (or from env)\n        temperature: Sampling temperature\n        max_tokens: Max output tokens\n        **kwargs: Provider-specific parameters\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.adapters.provider_registry import ProviderRegistry\n\n    # Try to convert to enum for built-in providers\n    try:\n        provider_enum = LLMProvider(provider.lower())\n    except ValueError:\n        # Not a built-in provider - check if it's a custom provider\n        if ProviderRegistry.is_registered(provider):\n            # Use a dummy enum value for validation, but store the actual provider string\n            provider_enum = LLMProvider.OPENAI  # Dummy for Pydantic validation\n            kwargs[\"_custom_provider_id\"] = provider\n        else:\n            raise ValueError(\n                f\"Unknown provider: {provider}. \"\n                f\"Available providers: {', '.join(ProviderRegistry.list_providers())}\"\n            )\n\n    self._llm_spec = LLMSpec(\n        provider=provider_enum,\n        model=model,\n        api_key=api_key,\n        temperature=temperature,\n        max_tokens=max_tokens,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec","title":"with_llm_spec","text":"<pre><code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder\n</code></pre> <p>Configure LLM using a pre-built LLMSpec object.</p> <p>This method allows using LLMSpec objects directly, enabling: - Reusable provider configurations - Use of LLMProviderPresets for common providers - Custom LLMSpec instances for advanced use cases</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>LLMSpec</code> <p>LLM specification object</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If spec is not an LLMSpec instance</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_llm_spec(self, spec: LLMSpec) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure LLM using a pre-built LLMSpec object.\n\n    This method allows using LLMSpec objects directly, enabling:\n    - Reusable provider configurations\n    - Use of LLMProviderPresets for common providers\n    - Custom LLMSpec instances for advanced use cases\n\n    Args:\n        spec: LLM specification object\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        TypeError: If spec is not an LLMSpec instance\n\n    Example:\n        # Use preset\n        from ondine.core.specifications import LLMProviderPresets\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n            .with_prompt(\"Process: {text}\")\n            .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n            .build()\n        )\n\n        # Custom spec\n        custom = LLMSpec(\n            provider=LLMProvider.OPENAI,\n            model=\"gpt-4o-mini\",\n            temperature=0.7\n        )\n        pipeline.with_llm_spec(custom)\n\n        # Override preset\n        spec = LLMProviderPresets.GPT4O_MINI.model_copy(\n            update={\"temperature\": 0.9}\n        )\n        pipeline.with_llm_spec(spec)\n    \"\"\"\n    if not isinstance(spec, LLMSpec):\n        raise TypeError(\n            f\"Expected LLMSpec, got {type(spec).__name__}. \"\n            f\"Use with_llm() for parameter-based configuration.\"\n        )\n\n    self._llm_spec = spec\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec--use-preset","title":"Use preset","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec--custom-spec","title":"Custom spec","text":"<p>custom = LLMSpec(     provider=LLMProvider.OPENAI,     model=\"gpt-4o-mini\",     temperature=0.7 ) pipeline.with_llm_spec(custom)</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_llm_spec--override-preset","title":"Override preset","text":"<p>spec = LLMProviderPresets.GPT4O_MINI.model_copy(     update={\"temperature\": 0.9} ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_custom_llm_client","title":"with_custom_llm_client","text":"<pre><code>with_custom_llm_client(client: any) -&gt; PipelineBuilder\n</code></pre> <p>Provide a custom LLM client instance directly.</p> <p>This allows advanced users to create their own LLM client implementations by extending the LLMClient base class. The custom client will be used instead of the factory-created client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>any</code> <p>Custom LLM client instance (must inherit from LLMClient)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Example <p>class MyCustomClient(LLMClient):     def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:         # Custom implementation         ...</p> <p>pipeline = (     PipelineBuilder.create()     .from_dataframe(df, ...)     .with_prompt(\"...\")     .with_custom_llm_client(MyCustomClient(spec))     .build() )</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_custom_llm_client(self, client: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Provide a custom LLM client instance directly.\n\n    This allows advanced users to create their own LLM client implementations\n    by extending the LLMClient base class. The custom client will be used\n    instead of the factory-created client.\n\n    Args:\n        client: Custom LLM client instance (must inherit from LLMClient)\n\n    Returns:\n        Self for chaining\n\n    Example:\n        class MyCustomClient(LLMClient):\n            def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n                # Custom implementation\n                ...\n\n        pipeline = (\n            PipelineBuilder.create()\n            .from_dataframe(df, ...)\n            .with_prompt(\"...\")\n            .with_custom_llm_client(MyCustomClient(spec))\n            .build()\n        )\n    \"\"\"\n    from ondine.adapters.llm_client import LLMClient\n\n    if not isinstance(client, LLMClient):\n        raise TypeError(\n            f\"Custom client must inherit from LLMClient, got {type(client).__name__}\"\n        )\n\n    self._custom_llm_client = client\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_batch_size","title":"with_batch_size","text":"<pre><code>with_batch_size(size: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure batch size.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Rows per batch</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_batch_size(self, size: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure batch size.\n\n    Args:\n        size: Rows per batch\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.batch_size = size\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_concurrency","title":"with_concurrency","text":"<pre><code>with_concurrency(threads: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure concurrent requests.</p> <p>Parameters:</p> Name Type Description Default <code>threads</code> <code>int</code> <p>Number of concurrent threads</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_concurrency(self, threads: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure concurrent requests.\n\n    Args:\n        threads: Number of concurrent threads\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.concurrency = threads\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_checkpoint_interval","title":"with_checkpoint_interval","text":"<pre><code>with_checkpoint_interval(rows: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint frequency.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Rows between checkpoints</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_interval(self, rows: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint frequency.\n\n    Args:\n        rows: Rows between checkpoints\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_interval = rows\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_rate_limit","title":"with_rate_limit","text":"<pre><code>with_rate_limit(rpm: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure rate limiting.</p> <p>Parameters:</p> Name Type Description Default <code>rpm</code> <code>int</code> <p>Requests per minute</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_rate_limit(self, rpm: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure rate limiting.\n\n    Args:\n        rpm: Requests per minute\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.rate_limit_rpm = rpm\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_max_retries","title":"with_max_retries","text":"<pre><code>with_max_retries(retries: int) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum retry attempts.</p> <p>Parameters:</p> Name Type Description Default <code>retries</code> <code>int</code> <p>Maximum number of retry attempts</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_retries(self, retries: int) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum retry attempts.\n\n    Args:\n        retries: Maximum number of retry attempts\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_retries = retries\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_max_budget","title":"with_max_budget","text":"<pre><code>with_max_budget(budget: float) -&gt; PipelineBuilder\n</code></pre> <p>Configure maximum budget.</p> <p>Parameters:</p> Name Type Description Default <code>budget</code> <code>float</code> <p>Maximum budget in USD</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_max_budget(self, budget: float) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure maximum budget.\n\n    Args:\n        budget: Maximum budget in USD\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.max_budget = Decimal(str(budget))\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_error_policy","title":"with_error_policy","text":"<pre><code>with_error_policy(policy: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure error handling policy.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>str</code> <p>Error policy ('skip', 'fail', 'retry', 'use_default')</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_error_policy(self, policy: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure error handling policy.\n\n    Args:\n        policy: Error policy ('skip', 'fail', 'retry', 'use_default')\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    from ondine.core.specifications import ErrorPolicy\n\n    self._processing_spec.error_policy = ErrorPolicy(policy.lower())\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_checkpoint_dir","title":"with_checkpoint_dir","text":"<pre><code>with_checkpoint_dir(directory: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to checkpoint directory</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_checkpoint_dir(self, directory: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure checkpoint directory.\n\n    Args:\n        directory: Path to checkpoint directory\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._processing_spec.checkpoint_dir = Path(directory)\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_parser","title":"with_parser","text":"<pre><code>with_parser(parser: any) -&gt; PipelineBuilder\n</code></pre> <p>Configure response parser.</p> <p>This method allows setting a custom parser. The parser type determines the response_format in the prompt spec.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>any</code> <p>Parser instance (JSONParser, RegexParser, PydanticParser, etc.)</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_parser(self, parser: any) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure response parser.\n\n    This method allows setting a custom parser. The parser type\n    determines the response_format in the prompt spec.\n\n    Args:\n        parser: Parser instance (JSONParser, RegexParser, PydanticParser, etc.)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    # Store the parser for later use in the pipeline\n    # We'll configure response_format based on parser type\n    if hasattr(parser, \"__class__\"):\n        parser_name = parser.__class__.__name__\n        if \"JSON\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            # Update the existing prompt spec's response_format\n            self._prompt_spec.response_format = \"json\"\n        elif \"Regex\" in parser_name:\n            if not self._prompt_spec:\n                raise ValueError(\n                    \"with_prompt() must be called before with_parser()\"\n                )\n            self._prompt_spec.response_format = \"regex\"\n            if hasattr(parser, \"patterns\"):\n                self._prompt_spec.regex_patterns = parser.patterns\n\n    # Store the parser instance in metadata for the pipeline to use\n    if not hasattr(self, \"_custom_parser\"):\n        self._custom_parser = parser\n\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.to_csv","title":"to_csv","text":"<pre><code>to_csv(path: str) -&gt; PipelineBuilder\n</code></pre> <p>Configure CSV output destination.</p> <p>Alias for with_output(path, format='csv').</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output CSV file path</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def to_csv(self, path: str) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure CSV output destination.\n\n    Alias for with_output(path, format='csv').\n\n    Args:\n        path: Output CSV file path\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    return self.with_output(path, format=\"csv\")\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_output","title":"with_output","text":"<pre><code>with_output(path: str, format: str = 'csv', merge_strategy: str = 'replace') -&gt; PipelineBuilder\n</code></pre> <p>Configure output destination.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output file path</p> required <code>format</code> <code>str</code> <p>Output format (csv, excel, parquet)</p> <code>'csv'</code> <code>merge_strategy</code> <code>str</code> <p>Merge strategy (replace, append, update)</p> <code>'replace'</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_output(\n    self,\n    path: str,\n    format: str = \"csv\",\n    merge_strategy: str = \"replace\",\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Configure output destination.\n\n    Args:\n        path: Output file path\n        format: Output format (csv, excel, parquet)\n        merge_strategy: Merge strategy (replace, append, update)\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    format_map = {\n        \"csv\": DataSourceType.CSV,\n        \"excel\": DataSourceType.EXCEL,\n        \"parquet\": DataSourceType.PARQUET,\n    }\n\n    merge_map = {\n        \"replace\": MergeStrategy.REPLACE,\n        \"append\": MergeStrategy.APPEND,\n        \"update\": MergeStrategy.UPDATE,\n    }\n\n    self._output_spec = OutputSpec(\n        destination_type=format_map[format.lower()],\n        destination_path=Path(path),\n        merge_strategy=merge_map[merge_strategy.lower()],\n    )\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_executor","title":"with_executor","text":"<pre><code>with_executor(executor: ExecutionStrategy) -&gt; PipelineBuilder\n</code></pre> <p>Set custom execution strategy.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ExecutionStrategy</code> <p>ExecutionStrategy instance</p> required <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_executor(self, executor: ExecutionStrategy) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Set custom execution strategy.\n\n    Args:\n        executor: ExecutionStrategy instance\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = executor\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_async_execution","title":"with_async_execution","text":"<pre><code>with_async_execution(max_concurrency: int = 10) -&gt; PipelineBuilder\n</code></pre> <p>Use async execution strategy.</p> <p>Enables async/await for non-blocking execution. Ideal for FastAPI, aiohttp, and async frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_async_execution(self, max_concurrency: int = 10) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use async execution strategy.\n\n    Enables async/await for non-blocking execution.\n    Ideal for FastAPI, aiohttp, and async frameworks.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = AsyncExecutor(max_concurrency=max_concurrency)\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_streaming","title":"with_streaming","text":"<pre><code>with_streaming(chunk_size: int = 1000) -&gt; PipelineBuilder\n</code></pre> <p>Use streaming execution strategy.</p> <p>Processes data in chunks for memory-efficient handling. Ideal for large datasets (100K+ rows).</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_streaming(self, chunk_size: int = 1000) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Use streaming execution strategy.\n\n    Processes data in chunks for memory-efficient handling.\n    Ideal for large datasets (100K+ rows).\n\n    Args:\n        chunk_size: Number of rows per chunk\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self._executor = StreamingExecutor(chunk_size=chunk_size)\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_stage","title":"with_stage","text":"<pre><code>with_stage(stage_name: str, position: str = 'before_prompt', **stage_kwargs) -&gt; PipelineBuilder\n</code></pre> <p>Add a custom pipeline stage by name.</p> <p>Enables injection of custom processing stages at specific points in the pipeline. Stages must be registered via StageRegistry.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Registered stage name (e.g., \"rag_retrieval\")</p> required <code>position</code> <code>str</code> <p>Where to inject the stage. Options: - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing - \"after_parser\": After response parsing</p> <code>'before_prompt'</code> <code>**stage_kwargs</code> <p>Arguments to pass to stage constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PipelineBuilder</code> <p>Self for chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name not registered or position invalid</p> Example Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def with_stage(\n    self,\n    stage_name: str,\n    position: str = \"before_prompt\",\n    **stage_kwargs,\n) -&gt; \"PipelineBuilder\":\n    \"\"\"\n    Add a custom pipeline stage by name.\n\n    Enables injection of custom processing stages at specific points\n    in the pipeline. Stages must be registered via StageRegistry.\n\n    Args:\n        stage_name: Registered stage name (e.g., \"rag_retrieval\")\n        position: Where to inject the stage. Options:\n            - \"after_loader\" / \"before_prompt\": After data loading, before prompt formatting\n            - \"after_prompt\" / \"before_llm\": After prompt formatting, before LLM invocation\n            - \"after_llm\" / \"before_parser\": After LLM invocation, before parsing\n            - \"after_parser\": After response parsing\n        **stage_kwargs: Arguments to pass to stage constructor\n\n    Returns:\n        Self for chaining\n\n    Raises:\n        ValueError: If stage_name not registered or position invalid\n\n    Example:\n        # RAG retrieval example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])\n            .with_stage(\n                \"rag_retrieval\",\n                position=\"before_prompt\",\n                vector_store=\"pinecone\",\n                index_name=\"my-docs\",\n                top_k=5\n            )\n            .with_prompt(\"Context: {retrieved_context}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o\")\n            .build()\n        )\n\n        # Content moderation example\n        pipeline = (\n            PipelineBuilder.create()\n            .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])\n            .with_stage(\n                \"content_moderation\",\n                position=\"before_llm\",\n                block_patterns=[\"spam\", \"offensive\"]\n            )\n            .with_prompt(\"Moderate: {text}\")\n            .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n            .build()\n        )\n    \"\"\"\n    from ondine.stages.stage_registry import StageRegistry\n\n    # Validate position\n    valid_positions = [\n        \"after_loader\",\n        \"before_prompt\",\n        \"after_prompt\",\n        \"before_llm\",\n        \"after_llm\",\n        \"before_parser\",\n        \"after_parser\",\n    ]\n    if position not in valid_positions:\n        raise ValueError(\n            f\"Invalid position '{position}'. Must be one of: {', '.join(valid_positions)}\"\n        )\n\n    # Get stage class from registry (this will raise ValueError if not found)\n    stage_class = StageRegistry.get(stage_name)\n\n    # Store stage config for later instantiation\n    self._custom_stages.append(\n        {\n            \"name\": stage_name,\n            \"class\": stage_class,\n            \"position\": position,\n            \"kwargs\": stage_kwargs,\n        }\n    )\n\n    return self\n</code></pre>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_stage--rag-retrieval-example","title":"RAG retrieval example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"questions.csv\", input_columns=[\"question\"], output_columns=[\"answer\"])     .with_stage(         \"rag_retrieval\",         position=\"before_prompt\",         vector_store=\"pinecone\",         index_name=\"my-docs\",         top_k=5     )     .with_prompt(\"Context: {retrieved_context}\\n\\nQuestion: {question}\\n\\nAnswer:\")     .with_llm(provider=\"openai\", model=\"gpt-4o\")     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.with_stage--content-moderation-example","title":"Content moderation example","text":"<p>pipeline = (     PipelineBuilder.create()     .from_csv(\"content.csv\", input_columns=[\"text\"], output_columns=[\"moderated\"])     .with_stage(         \"content_moderation\",         position=\"before_llm\",         block_patterns=[\"spam\", \"offensive\"]     )     .with_prompt(\"Moderate: {text}\")     .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")     .build() )</p>"},{"location":"api/api/pipeline_builder/#ondine.api.pipeline_builder.PipelineBuilder.build","title":"build","text":"<pre><code>build() -&gt; Pipeline\n</code></pre> <p>Build final Pipeline.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required specifications missing</p> Source code in <code>ondine/api/pipeline_builder.py</code> <pre><code>def build(self) -&gt; Pipeline:\n    \"\"\"\n    Build final Pipeline.\n\n    Returns:\n        Configured Pipeline\n\n    Raises:\n        ValueError: If required specifications missing\n    \"\"\"\n    # Validate required specs\n    if not self._dataset_spec:\n        raise ValueError(\"Dataset specification required\")\n    if not self._prompt_spec:\n        raise ValueError(\"Prompt specification required\")\n\n    # LLM spec is optional if custom client is provided\n    if not self._llm_spec and not self._custom_llm_client:\n        raise ValueError(\"Either LLM specification or custom LLM client required\")\n\n    # Prepare metadata with custom parser, custom client, and/or custom stages if provided\n    metadata = {}\n    if self._custom_parser is not None:\n        metadata[\"custom_parser\"] = self._custom_parser\n    if self._custom_llm_client is not None:\n        metadata[\"custom_llm_client\"] = self._custom_llm_client\n    if self._custom_stages:\n        metadata[\"custom_stages\"] = self._custom_stages\n\n    # Create specifications bundle\n    # If custom client provided but no llm_spec, create a dummy spec\n    llm_spec = self._llm_spec\n    if llm_spec is None and self._custom_llm_client is not None:\n        # Create minimal spec using custom client's attributes\n        llm_spec = LLMSpec(\n            provider=LLMProvider.OPENAI,  # Dummy provider\n            model=self._custom_llm_client.model,\n            temperature=self._custom_llm_client.temperature,\n            max_tokens=self._custom_llm_client.max_tokens,\n        )\n\n    specifications = PipelineSpecifications(\n        dataset=self._dataset_spec,\n        prompt=self._prompt_spec,\n        llm=llm_spec,\n        processing=self._processing_spec,\n        output=self._output_spec,\n        metadata=metadata,\n    )\n\n    # Create and return pipeline\n    return Pipeline(\n        specifications,\n        dataframe=self._dataframe,\n        executor=self._executor,\n    )\n</code></pre>"},{"location":"api/api/pipeline_composer/","title":"pipeline_composer","text":""},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer","title":"pipeline_composer","text":"<p>Pipeline Composer for multi-column, multi-prompt processing.</p> <p>This module enables composing multiple single-prompt pipelines to generate multiple output columns, each with its own independent processing logic.</p> Example <p>composer = (     PipelineComposer(input_data=\"data.xlsx\")     .add_column(\"similarity\", similarity_pipeline)     .add_column(\"explanation\", explanation_pipeline, depends_on=[\"similarity\"])     .execute() )</p>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer","title":"PipelineComposer","text":"<pre><code>PipelineComposer(input_data: str | Path | DataFrame)\n</code></pre> <p>Composes multiple pipelines to process independent columns.</p> <p>Each pipeline processes one output column. Pipelines can depend on outputs from previous pipelines, enabling sequential processing.</p> <p>Design Philosophy: - Keep individual pipelines simple (single responsibility) - Compose complex workflows from simple building blocks - Make dependencies explicit (no hidden coupling) - Fail fast with clear error messages</p> <p>Initialize pipeline composer.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str | Path | DataFrame</code> <p>Either a file path (CSV/Excel) or DataFrame</p> required Design Note <p>We accept both paths and DataFrames to support different workflows: - Path: Lazy loading, memory efficient - DataFrame: Already loaded, faster iteration</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def __init__(self, input_data: str | Path | pd.DataFrame):\n    \"\"\"\n    Initialize pipeline composer.\n\n    Args:\n        input_data: Either a file path (CSV/Excel) or DataFrame\n\n    Design Note:\n        We accept both paths and DataFrames to support different workflows:\n        - Path: Lazy loading, memory efficient\n        - DataFrame: Already loaded, faster iteration\n    \"\"\"\n    if isinstance(input_data, str | Path):\n        self.input_path = str(input_data)\n        self.input_df = None  # Lazy load\n    elif isinstance(input_data, pd.DataFrame):\n        self.input_path = None\n        self.input_df = input_data.copy()\n    else:\n        raise TypeError(\n            f\"input_data must be str, Path, or DataFrame, got {type(input_data)}\"\n        )\n\n    # Storage for column pipelines\n    # Format: (column_name, pipeline, dependencies)\n    self.column_pipelines: list[tuple[str, Pipeline, list[str]]] = []\n\n    logger.info(\"PipelineComposer initialized\")\n</code></pre>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer.add_column","title":"add_column","text":"<pre><code>add_column(column_name: str, pipeline: Pipeline, depends_on: list[str] | None = None) -&gt; PipelineComposer\n</code></pre> <p>Add a pipeline for processing one output column.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of output column</p> required <code>pipeline</code> <code>Pipeline</code> <p>Pipeline to generate this column</p> required <code>depends_on</code> <code>list[str] | None</code> <p>List of columns this depends on (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Self for method chaining (fluent API)</p> Example <p>composer.add_column(\"score\", score_pipeline) composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])</p> Design Note <p>Fluent API (returns self) enables readable chaining: composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def add_column(\n    self,\n    column_name: str,\n    pipeline: Pipeline,\n    depends_on: list[str] | None = None,\n) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Add a pipeline for processing one output column.\n\n    Args:\n        column_name: Name of output column\n        pipeline: Pipeline to generate this column\n        depends_on: List of columns this depends on (optional)\n\n    Returns:\n        Self for method chaining (fluent API)\n\n    Example:\n        composer.add_column(\"score\", score_pipeline)\n        composer.add_column(\"explanation\", explain_pipeline, depends_on=[\"score\"])\n\n    Design Note:\n        Fluent API (returns self) enables readable chaining:\n        composer.add_column(\"a\", p1).add_column(\"b\", p2).execute()\n    \"\"\"\n    dependencies = depends_on or []\n\n    # Validate column name unique\n    existing_cols = [col for col, _, _ in self.column_pipelines]\n    if column_name in existing_cols:\n        raise ValueError(f\"Column '{column_name}' already added\")\n\n    self.column_pipelines.append((column_name, pipeline, dependencies))\n\n    logger.info(\n        f\"Added column '{column_name}' with dependencies: {dependencies or 'none'}\"\n    )\n\n    return self  # Enable chaining\n</code></pre>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer.execute","title":"execute","text":"<pre><code>execute() -&gt; ExecutionResult\n</code></pre> <p>Execute all column pipelines in dependency order.</p> <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with all columns merged</p> Algorithm <ol> <li>Load input data (lazy if needed)</li> <li>Resolve execution order (topological sort)</li> <li>For each column:    a. Inject current DataFrame into pipeline    b. Execute pipeline    c. Merge result column into DataFrame</li> <li>Aggregate metrics and return final result</li> </ol> Design Note <p>Each pipeline operates on the accumulating DataFrame, so later pipelines can use earlier outputs as inputs.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>def execute(self) -&gt; ExecutionResult:\n    \"\"\"\n    Execute all column pipelines in dependency order.\n\n    Returns:\n        ExecutionResult with all columns merged\n\n    Algorithm:\n        1. Load input data (lazy if needed)\n        2. Resolve execution order (topological sort)\n        3. For each column:\n           a. Inject current DataFrame into pipeline\n           b. Execute pipeline\n           c. Merge result column into DataFrame\n        4. Aggregate metrics and return final result\n\n    Design Note:\n        Each pipeline operates on the accumulating DataFrame,\n        so later pipelines can use earlier outputs as inputs.\n    \"\"\"\n    # Lazy load if needed\n    if self.input_df is None:\n        logger.info(f\"Loading input data from {self.input_path}\")\n        # Use DataReader adapter for consistent file loading\n        from ondine.adapters.data_io import create_data_reader\n        from ondine.core.specifications import DataSourceType\n\n        # Detect source type from file extension\n        if self.input_path.endswith(\".xlsx\") or self.input_path.endswith(\".xls\"):\n            source_type = DataSourceType.EXCEL\n        elif self.input_path.endswith(\".csv\"):\n            source_type = DataSourceType.CSV\n        elif self.input_path.endswith(\".parquet\"):\n            source_type = DataSourceType.PARQUET\n        else:\n            raise ValueError(f\"Unsupported file type: {self.input_path}\")\n\n        # Create reader and load data\n        reader = create_data_reader(\n            source_type=source_type, source_path=self.input_path\n        )\n        self.input_df = reader.read()\n\n    # Start with input data\n    df = self.input_df.copy()\n\n    # Resolve execution order\n    execution_order = self._get_execution_order()\n\n    logger.info(\n        f\"Executing {len(execution_order)} column pipelines in order: \"\n        f\"{[col for col, _, _ in execution_order]}\"\n    )\n\n    # Track metrics (keep Decimal precision for costs)\n    from decimal import Decimal\n\n    total_cost = Decimal(\"0.0\")\n    total_errors = []\n\n    # Execute each column pipeline\n    for col_name, pipeline, _deps in execution_order:\n        logger.info(f\"Processing column '{col_name}'...\")\n\n        # Inject current DataFrame (includes previous outputs)\n        pipeline.dataframe = df\n\n        # Execute pipeline\n        result = pipeline.execute()\n\n        # Merge new column\n        if col_name in result.data.columns:\n            df[col_name] = result.data[col_name]\n        else:\n            logger.warning(\n                f\"Pipeline for '{col_name}' didn't produce expected column\"\n            )\n\n        # Accumulate metrics (preserve Decimal precision)\n        cost_to_add = result.costs.total_cost\n        if not isinstance(cost_to_add, Decimal):\n            cost_to_add = Decimal(str(cost_to_add))\n        total_cost += cost_to_add\n        total_errors.extend(result.errors)\n\n        logger.info(\n            f\"Column '{col_name}' complete: \"\n            f\"{len(df)} rows, ${result.costs.total_cost:.4f}\"\n        )\n\n    # Create final result\n    final_result = ExecutionResult(\n        data=df,\n        metrics=ProcessingStats(\n            total_rows=len(df),\n            processed_rows=len(df),\n            failed_rows=len(total_errors),\n            skipped_rows=0,\n            rows_per_second=0.0,  # Aggregate metric not meaningful\n            total_duration_seconds=0.0,  # Would need timing\n        ),\n        costs=CostEstimate(\n            total_cost=total_cost,\n            total_tokens=0,  # Aggregate from individual pipelines\n            input_tokens=0,\n            output_tokens=0,\n            rows=len(df),\n        ),\n        errors=total_errors,\n    )\n\n    logger.info(\n        f\"Composition complete: {len(execution_order)} columns, \"\n        f\"${total_cost:.4f} total cost\"\n    )\n\n    return final_result\n</code></pre>"},{"location":"api/api/pipeline_composer/#ondine.api.pipeline_composer.PipelineComposer.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(config_path: str) -&gt; PipelineComposer\n</code></pre> <p>Load composer configuration from YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to composition config file</p> required <p>Returns:</p> Type Description <code>PipelineComposer</code> <p>Configured PipelineComposer</p> Example YAML <p>composition:   input: \"data.xlsx\"   pipelines:     - column: col1       config: pipeline1.yaml     - column: col2       depends_on: [col1]       config: pipeline2.yaml</p> Design Note <p>This enables pure YAML workflows without Python code.</p> Source code in <code>ondine/api/pipeline_composer.py</code> <pre><code>@classmethod\ndef from_yaml(cls, config_path: str) -&gt; \"PipelineComposer\":\n    \"\"\"\n    Load composer configuration from YAML.\n\n    Args:\n        config_path: Path to composition config file\n\n    Returns:\n        Configured PipelineComposer\n\n    Example YAML:\n        composition:\n          input: \"data.xlsx\"\n          pipelines:\n            - column: col1\n              config: pipeline1.yaml\n            - column: col2\n              depends_on: [col1]\n              config: pipeline2.yaml\n\n    Design Note:\n        This enables pure YAML workflows without Python code.\n    \"\"\"\n    import yaml\n\n    from ondine.config import ConfigLoader\n\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n\n    composition = config.get(\"composition\", {})\n    input_data = composition.get(\"input\")\n\n    if not input_data:\n        raise ValueError(\"composition.input is required\")\n\n    composer = cls(input_data=input_data)\n\n    # Load each pipeline config\n    for pipeline_config in composition.get(\"pipelines\", []):\n        col_name = pipeline_config[\"column\"]\n        config_file = pipeline_config[\"config\"]\n        depends_on = pipeline_config.get(\"depends_on\", [])\n\n        # Load pipeline from its config\n        pipeline_specs = ConfigLoader.from_yaml(config_file)\n        pipeline = Pipeline(pipeline_specs)\n\n        composer.add_column(col_name, pipeline, depends_on)\n\n    return composer\n</code></pre>"},{"location":"api/api/quick/","title":"quick","text":""},{"location":"api/api/quick/#ondine.api.quick","title":"quick","text":"<p>Simplified API with smart defaults for quick pipeline creation.</p> <p>This module provides a high-level API that reduces boilerplate and makes common use cases trivial while still providing access to full functionality.</p> <p>Design Philosophy: - Convention over configuration - Auto-detect from context when possible - Provide sensible defaults for 80% of use cases - Still allow full customization when needed</p>"},{"location":"api/api/quick/#ondine.api.quick.QuickPipeline","title":"QuickPipeline","text":"<p>Simplified pipeline API with smart defaults.</p> <p>Designed for rapid prototyping and common use cases. Automatically detects: - Input columns from prompt template placeholders - Provider from model name (e.g., gpt-4 \u2192 openai, claude \u2192 anthropic) - Parser type (JSON for multi-column, text for single column) - Reasonable defaults for batch size, concurrency, retries</p> <p>Examples:</p> <p>Minimal usage:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"data.csv\",\n...     prompt=\"Categorize this text: {text}\"\n... )\n&gt;&gt;&gt; result = pipeline.execute()\n</code></pre> <p>With explicit outputs:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=\"products.csv\",\n...     prompt=\"Extract: {description}\",\n...     output_columns=[\"brand\", \"model\", \"price\"]\n... )\n</code></pre> <p>Override defaults:</p> <pre><code>&gt;&gt;&gt; pipeline = QuickPipeline.create(\n...     data=df,\n...     prompt=\"Summarize: {content}\",\n...     model=\"gpt-4o\",\n...     temperature=0.7,\n...     max_budget=Decimal(\"5.0\")\n... )\n</code></pre>"},{"location":"api/api/quick/#ondine.api.quick.QuickPipeline.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(data: str | Path | DataFrame, prompt: str, model: str = 'gpt-4o-mini', output_columns: list[str] | str | None = None, provider: str | None = None, temperature: float = 0.0, max_tokens: int | None = None, max_budget: Decimal | float | str | None = None, batch_size: int | None = None, concurrency: int | None = None, **kwargs: Any) -&gt; Pipeline\n</code></pre> <p>Create a pipeline with smart defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path | DataFrame</code> <p>CSV/Excel/Parquet file path or DataFrame</p> required <code>prompt</code> <code>str</code> <p>Prompt template with {placeholders}</p> required <code>model</code> <code>str</code> <p>Model name (default: gpt-4o-mini)</p> <code>'gpt-4o-mini'</code> <code>output_columns</code> <code>list[str] | str | None</code> <p>Output column name(s). If None, uses [\"output\"]</p> <code>None</code> <code>provider</code> <code>str | None</code> <p>LLM provider. If None, auto-detected from model name</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature (default: 0.0 for deterministic)</p> <code>0.0</code> <code>max_tokens</code> <code>int | None</code> <p>Max output tokens (default: provider's default)</p> <code>None</code> <code>max_budget</code> <code>Decimal | float | str | None</code> <p>Maximum cost budget in USD</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Rows per batch (default: auto-sized based on data)</p> <code>None</code> <code>concurrency</code> <code>int | None</code> <p>Parallel requests (default: auto-sized)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to PipelineBuilder</p> <code>{}</code> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Configured Pipeline ready to execute</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input data cannot be loaded or prompt is invalid</p> Source code in <code>ondine/api/quick.py</code> <pre><code>@staticmethod\ndef create(\n    data: str | Path | pd.DataFrame,\n    prompt: str,\n    model: str = \"gpt-4o-mini\",\n    output_columns: list[str] | str | None = None,\n    provider: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    max_budget: Decimal | float | str | None = None,\n    batch_size: int | None = None,\n    concurrency: int | None = None,\n    **kwargs: Any,\n) -&gt; Pipeline:\n    \"\"\"\n    Create a pipeline with smart defaults.\n\n    Args:\n        data: CSV/Excel/Parquet file path or DataFrame\n        prompt: Prompt template with {placeholders}\n        model: Model name (default: gpt-4o-mini)\n        output_columns: Output column name(s). If None, uses [\"output\"]\n        provider: LLM provider. If None, auto-detected from model name\n        temperature: Sampling temperature (default: 0.0 for deterministic)\n        max_tokens: Max output tokens (default: provider's default)\n        max_budget: Maximum cost budget in USD\n        batch_size: Rows per batch (default: auto-sized based on data)\n        concurrency: Parallel requests (default: auto-sized)\n        **kwargs: Additional arguments passed to PipelineBuilder\n\n    Returns:\n        Configured Pipeline ready to execute\n\n    Raises:\n        ValueError: If input data cannot be loaded or prompt is invalid\n    \"\"\"\n    # 1. Load data\n    df = QuickPipeline._load_data(data)\n\n    # 2. Auto-detect input columns from prompt template\n    input_columns = QuickPipeline._extract_placeholders(prompt)\n    if not input_columns:\n        raise ValueError(\n            f\"No placeholders found in prompt: {prompt}\\n\"\n            \"Expected format: 'Your prompt with {{column_name}} placeholders'\"\n        )\n\n    # Validate input columns exist in data\n    missing = [col for col in input_columns if col not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Input columns {missing} not found in data. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    # 3. Normalize output columns\n    if output_columns is None:\n        output_columns = [\"output\"]\n    elif isinstance(output_columns, str):\n        output_columns = [output_columns]\n\n    # 4. Auto-detect provider from model name\n    if provider is None:\n        provider = QuickPipeline._detect_provider(model)\n\n    # 5. Auto-select parser (JSON for multi-column, text for single)\n    parser = QuickPipeline._select_parser(output_columns)\n\n    # 6. Smart defaults for batch_size and concurrency\n    if batch_size is None:\n        batch_size = QuickPipeline._default_batch_size(len(df))\n    if concurrency is None:\n        concurrency = QuickPipeline._default_concurrency(provider)\n\n    # 7. Build pipeline\n    builder = (\n        PipelineBuilder.create()\n        .from_dataframe(\n            df, input_columns=input_columns, output_columns=output_columns\n        )\n        .with_prompt(template=prompt)\n        .with_llm(\n            provider=provider,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n    )\n\n    # Add optional parser if multi-column\n    if parser:\n        builder = builder.with_parser(parser)\n\n    # Add batch/concurrency settings\n    builder = builder.with_batch_size(batch_size).with_concurrency(concurrency)\n\n    # Add budget if specified\n    if max_budget is not None:\n        # Convert to float for PipelineBuilder (it expects float)\n        if isinstance(max_budget, Decimal | str):\n            max_budget = float(max_budget)\n        builder = builder.with_max_budget(budget=max_budget)\n\n    # Add sensible retry defaults\n    builder = builder.with_max_retries(3)\n\n    return builder.build()\n</code></pre>"},{"location":"api/cli/","title":"cli","text":""},{"location":"api/cli/#ondine.cli","title":"cli","text":"<p>Command-line interface for LLM Dataset Engine.</p>"},{"location":"api/cli/#ondine.cli.cli","title":"cli","text":"<pre><code>cli(ctx)\n</code></pre> <p>\ud83c\udf0a ONDINE - LLM Dataset Engine</p> <p>Process tabular datasets using LLMs with production-grade reliability, cost control, and observability.</p> <p>Examples:</p> <pre><code># Process a dataset\nondine process --config config.yaml\n\n# Estimate cost before processing\nondine estimate --config config.yaml\n\n# Resume from checkpoint\nondine resume --session-id abc-123\n\n# Validate configuration\nondine validate --config config.yaml\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=__version__, prog_name=\"ondine\")\n@click.pass_context\ndef cli(ctx):\n    \"\"\"\n    \ud83c\udf0a ONDINE - LLM Dataset Engine\n\n    Process tabular datasets using LLMs with production-grade reliability,\n    cost control, and observability.\n\n    Examples:\n\n        # Process a dataset\n        ondine process --config config.yaml\n\n        # Estimate cost before processing\n        ondine estimate --config config.yaml\n\n        # Resume from checkpoint\n        ondine resume --session-id abc-123\n\n        # Validate configuration\n        ondine validate --config config.yaml\n    \"\"\"\n    # Show banner only for main commands (not for --help)\n    if ctx.invoked_subcommand is not None:\n        show_banner()\n</code></pre>"},{"location":"api/cli/main/","title":"main","text":""},{"location":"api/cli/main/#ondine.cli.main","title":"main","text":"<p>Main CLI entry point for LLM Dataset Engine.</p> <p>Provides command-line interface for processing datasets, estimating costs, and managing pipeline execution.</p>"},{"location":"api/cli/main/#ondine.cli.main.show_banner","title":"show_banner","text":"<pre><code>show_banner()\n</code></pre> <p>Display the Ondine banner (centered, creative, robust).</p> Source code in <code>ondine/cli/main.py</code> <pre><code>def show_banner():\n    \"\"\"Display the Ondine banner (centered, creative, robust).\"\"\"\n    # Color gradient: cyan to magenta\n    lines = ONDINE_ART.strip().split(\"\\n\")\n    colored_lines = []\n    colors = [\n        \"bright_cyan\",\n        \"cyan\",\n        \"bright_blue\",\n        \"blue\",\n        \"bright_magenta\",\n        \"magenta\",\n        \"bright_magenta\",\n        \"blue\",\n        \"bright_blue\",\n        \"cyan\",\n        \"bright_cyan\",\n    ]\n\n    for i, line in enumerate(lines):\n        color = colors[i % len(colors)]\n        colored_lines.append(Text(line, style=f\"bold {color}\"))\n\n    title = Group(*colored_lines)\n    subtitle_text = Text(\"The LLM Dataset Engine\", style=\"dim italic bright_white\")\n    content = Group(title, \"\", subtitle_text)\n\n    console.print()\n    console.print(content)\n    console.print(\"[bold bright_cyan]\" + \"\u2500\" * 80 + \"[/bold bright_cyan]\")\n    console.print()\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.cli","title":"cli","text":"<pre><code>cli(ctx)\n</code></pre> <p>\ud83c\udf0a ONDINE - LLM Dataset Engine</p> <p>Process tabular datasets using LLMs with production-grade reliability, cost control, and observability.</p> <p>Examples:</p> <pre><code># Process a dataset\nondine process --config config.yaml\n\n# Estimate cost before processing\nondine estimate --config config.yaml\n\n# Resume from checkpoint\nondine resume --session-id abc-123\n\n# Validate configuration\nondine validate --config config.yaml\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=__version__, prog_name=\"ondine\")\n@click.pass_context\ndef cli(ctx):\n    \"\"\"\n    \ud83c\udf0a ONDINE - LLM Dataset Engine\n\n    Process tabular datasets using LLMs with production-grade reliability,\n    cost control, and observability.\n\n    Examples:\n\n        # Process a dataset\n        ondine process --config config.yaml\n\n        # Estimate cost before processing\n        ondine estimate --config config.yaml\n\n        # Resume from checkpoint\n        ondine resume --session-id abc-123\n\n        # Validate configuration\n        ondine validate --config config.yaml\n    \"\"\"\n    # Show banner only for main commands (not for --help)\n    if ctx.invoked_subcommand is not None:\n        show_banner()\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.process","title":"process","text":"<pre><code>process(config: Path, input: Path | None, output: Path | None, provider: str | None, model: str | None, max_budget: float | None, batch_size: int | None, concurrency: int | None, checkpoint_dir: Path | None, dry_run: bool, verbose: bool)\n</code></pre> <p>Process a dataset using LLM transformations.</p> <p>Reads data from config file. INPUT and OUTPUT flags override config values if provided.</p> <p>Examples:</p> <pre><code># Basic usage\nllm-dataset process -c config.yaml -i data.csv -o result.csv\n\n# Override provider and model\nllm-dataset process -c config.yaml -i data.csv -o result.csv \\\n    --provider groq --model openai/gpt-oss-120b\n\n# Set budget limit\nllm-dataset process -c config.yaml -i data.csv -o result.csv \\\n    --max-budget 10.0\n\n# Dry run (estimate only)\nllm-dataset process -c config.yaml -i data.csv -o result.csv --dry-run\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--config\",\n    \"-c\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to YAML/JSON configuration file\",\n)\n@click.option(\n    \"--input\",\n    \"-i\",\n    required=False,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to input data file (overrides config)\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    required=False,\n    type=click.Path(path_type=Path),\n    help=\"Path to output file (overrides config)\",\n)\n@click.option(\n    \"--provider\",\n    type=click.Choice([p.value for p in LLMProvider]),\n    help=\"Override LLM provider from config (use 'ondine list-providers' to see all)\",\n)\n@click.option(\n    \"--model\",\n    help=\"Override model name from config\",\n)\n@click.option(\n    \"--max-budget\",\n    type=float,\n    help=\"Override maximum budget (USD) from config\",\n)\n@click.option(\n    \"--batch-size\",\n    type=int,\n    help=\"Override batch size from config\",\n)\n@click.option(\n    \"--concurrency\",\n    type=int,\n    help=\"Override concurrency from config\",\n)\n@click.option(\n    \"--checkpoint-dir\",\n    type=click.Path(path_type=Path),\n    help=\"Override checkpoint directory from config\",\n)\n@click.option(\n    \"--dry-run\",\n    is_flag=True,\n    help=\"Validate and estimate only, don't execute\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Enable verbose logging\",\n)\ndef process(\n    config: Path,\n    input: Path | None,\n    output: Path | None,\n    provider: str | None,\n    model: str | None,\n    max_budget: float | None,\n    batch_size: int | None,\n    concurrency: int | None,\n    checkpoint_dir: Path | None,\n    dry_run: bool,\n    verbose: bool,\n):\n    \"\"\"\n    Process a dataset using LLM transformations.\n\n    Reads data from config file. INPUT and OUTPUT flags override config values if provided.\n\n    Examples:\n\n        # Basic usage\n        llm-dataset process -c config.yaml -i data.csv -o result.csv\n\n        # Override provider and model\n        llm-dataset process -c config.yaml -i data.csv -o result.csv \\\\\n            --provider groq --model openai/gpt-oss-120b\n\n        # Set budget limit\n        llm-dataset process -c config.yaml -i data.csv -o result.csv \\\\\n            --max-budget 10.0\n\n        # Dry run (estimate only)\n        llm-dataset process -c config.yaml -i data.csv -o result.csv --dry-run\n    \"\"\"\n    try:\n        # Load configuration\n        console.print(f\"[cyan]Loading configuration from {config}...[/cyan]\")\n        specs = ConfigLoader.from_yaml(str(config))\n\n        # Override with CLI arguments (if provided)\n        if input:\n            specs.dataset.source_path = input\n\n        # Set output configuration (if provided)\n        if output:\n            if specs.output:\n                specs.output.destination_path = output\n            else:\n                # Create output spec if not in config\n                from ondine.core.specifications import MergeStrategy, OutputSpec\n\n                # Detect output type from extension\n                output_suffix = output.suffix.lower()\n                if output_suffix == \".csv\":\n                    output_type = DataSourceType.CSV\n                elif output_suffix in [\".xlsx\", \".xls\"]:\n                    output_type = DataSourceType.EXCEL\n                elif output_suffix == \".parquet\":\n                    output_type = DataSourceType.PARQUET\n                else:\n                    output_type = DataSourceType.CSV  # Default\n\n                specs.output = OutputSpec(\n                    destination_type=output_type,\n                    destination_path=output,\n                    merge_strategy=MergeStrategy.REPLACE,\n                )\n\n        if provider:\n            specs.llm.provider = LLMProvider(provider)\n\n        if model:\n            specs.llm.model = model\n\n        if max_budget is not None:\n            from decimal import Decimal\n\n            specs.processing.max_budget = Decimal(str(max_budget))\n\n        if batch_size is not None:\n            specs.processing.batch_size = batch_size\n\n        if concurrency is not None:\n            specs.processing.concurrency = concurrency\n\n        if checkpoint_dir is not None:\n            specs.processing.checkpoint_dir = checkpoint_dir\n\n        # Create pipeline\n        console.print(\"[cyan]Creating pipeline...[/cyan]\")\n        pipeline = Pipeline(specs)\n\n        # Validate\n        console.print(\"[cyan]Validating pipeline...[/cyan]\")\n        validation = pipeline.validate()\n\n        if not validation.is_valid:\n            console.print(\"[red]\u274c Validation failed:[/red]\")\n            for error in validation.errors:\n                console.print(f\"  [red]\u2022 {error}[/red]\")\n            sys.exit(1)\n\n        console.print(\"[green]\u2705 Validation passed[/green]\")\n\n        # Estimate cost\n        console.print(\"\\n[cyan]Estimating cost...[/cyan]\")\n        estimate = pipeline.estimate_cost()\n\n        table = Table(title=\"Cost Estimate\")\n        table.add_column(\"Metric\", style=\"cyan\")\n        table.add_column(\"Value\", style=\"green\")\n\n        table.add_row(\"Total Cost\", f\"${estimate.total_cost}\")\n        table.add_row(\"Total Tokens\", f\"{estimate.total_tokens:,}\")\n        table.add_row(\"Input Tokens\", f\"{estimate.input_tokens:,}\")\n        table.add_row(\"Output Tokens\", f\"{estimate.output_tokens:,}\")\n        table.add_row(\"Rows\", f\"{estimate.rows:,}\")\n\n        console.print(table)\n\n        if dry_run:\n            console.print(\"\\n[yellow]Dry run mode - skipping execution[/yellow]\")\n            return\n\n        # Execute\n        console.print(\"\\n[cyan]Processing dataset...[/cyan]\")\n        result = pipeline.execute()\n\n        # Display results\n        console.print(\"\\n[green]\u2705 Processing complete![/green]\")\n\n        results_table = Table(title=\"Execution Results\")\n        results_table.add_column(\"Metric\", style=\"cyan\")\n        results_table.add_column(\"Value\", style=\"green\")\n\n        results_table.add_row(\"Total Rows\", str(result.metrics.total_rows))\n        results_table.add_row(\"Processed\", str(result.metrics.processed_rows))\n        results_table.add_row(\"Failed\", str(result.metrics.failed_rows))\n        results_table.add_row(\"Skipped\", str(result.metrics.skipped_rows))\n        results_table.add_row(\"Duration\", f\"{result.duration:.2f}s\")\n        results_table.add_row(\"Total Cost\", f\"${result.costs.total_cost}\")\n        results_table.add_row(\n            \"Cost per Row\",\n            f\"${result.costs.total_cost / result.metrics.total_rows:.6f}\",\n        )\n\n        console.print(results_table)\n\n        # Validate output quality\n        console.print(\"\\n[cyan]\ud83d\udcca Validating output quality...[/cyan]\")\n        quality = result.validate_output_quality(specs.dataset.output_columns)\n\n        quality_table = Table(title=\"Quality Report\")\n        quality_table.add_column(\"Metric\", style=\"cyan\")\n        quality_table.add_column(\"Value\", style=\"green\")\n\n        quality_table.add_row(\n            \"Valid Outputs\", f\"{quality.valid_outputs}/{quality.total_rows}\"\n        )\n        quality_table.add_row(\"Success Rate\", f\"{quality.success_rate:.1f}%\")\n        quality_table.add_row(\"Null Outputs\", str(quality.null_outputs))\n        quality_table.add_row(\"Empty Outputs\", str(quality.empty_outputs))\n\n        # Color-code quality score\n        score_color = (\n            \"green\"\n            if quality.quality_score in [\"excellent\", \"good\"]\n            else \"yellow\"\n            if quality.quality_score == \"poor\"\n            else \"red\"\n        )\n        quality_table.add_row(\n            \"Quality Score\",\n            f\"[{score_color}]{quality.quality_score.upper()}[/{score_color}]\",\n        )\n\n        console.print(quality_table)\n\n        # Display warnings and issues\n        if quality.warnings:\n            console.print(\"\\n[yellow]\u26a0\ufe0f  Warnings:[/yellow]\")\n            for warning in quality.warnings:\n                console.print(f\"  [yellow]\u2022 {warning}[/yellow]\")\n\n        if quality.issues:\n            console.print(\"\\n[red]\ud83d\udea8 Issues Detected:[/red]\")\n            for issue in quality.issues:\n                console.print(f\"  [red]\u2022 {issue}[/red]\")\n            console.print(\"\\n[red]Consider:[/red]\")\n            console.print(\n                \"  [dim]\u2022 Review your prompt complexity (simpler prompts often work better)[/dim]\"\n            )\n            console.print(\"  [dim]\u2022 Check LLM provider logs for errors[/dim]\")\n            console.print(\"  [dim]\u2022 Increase max_tokens if outputs are truncated[/dim]\")\n            console.print(\"  [dim]\u2022 Verify API key and rate limits[/dim]\")\n\n        if quality.is_acceptable:\n            console.print(\n                f\"\\n[green]\u2705 Output quality is acceptable ({quality.success_rate:.1f}% success)[/green]\"\n            )\n        else:\n            console.print(\n                f\"\\n[red]\u274c Output quality is below acceptable threshold ({quality.success_rate:.1f}% &lt; 70%)[/red]\"\n            )\n\n        console.print(\n            f\"\\n[green]Output written to: {specs.output.destination_path}[/green]\"\n        )\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        if verbose:\n            console.print_exception()\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.estimate","title":"estimate","text":"<pre><code>estimate(config: Path, input: Path, provider: str | None, model: str | None)\n</code></pre> <p>Estimate processing cost without executing.</p> <p>Useful for budget planning and cost validation before running expensive operations.</p> <p>Examples:</p> <pre><code># Estimate cost\nllm-dataset estimate -c config.yaml -i data.csv\n\n# Estimate with different model\nllm-dataset estimate -c config.yaml -i data.csv --model gpt-4o\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--config\",\n    \"-c\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to YAML/JSON configuration file\",\n)\n@click.option(\n    \"--input\",\n    \"-i\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to input data file\",\n)\n@click.option(\n    \"--provider\",\n    type=click.Choice([p.value for p in LLMProvider]),\n    help=\"Override LLM provider from config (use 'ondine list-providers' to see all)\",\n)\n@click.option(\n    \"--model\",\n    help=\"Override model name from config\",\n)\ndef estimate(\n    config: Path,\n    input: Path,\n    provider: str | None,\n    model: str | None,\n):\n    \"\"\"\n    Estimate processing cost without executing.\n\n    Useful for budget planning and cost validation before running\n    expensive operations.\n\n    Examples:\n\n        # Estimate cost\n        llm-dataset estimate -c config.yaml -i data.csv\n\n        # Estimate with different model\n        llm-dataset estimate -c config.yaml -i data.csv --model gpt-4o\n    \"\"\"\n    try:\n        # Load configuration\n        console.print(f\"[cyan]Loading configuration from {config}...[/cyan]\")\n        specs = ConfigLoader.from_yaml(str(config))\n\n        # Override\n        specs.dataset.source_path = input\n\n        if provider:\n            specs.llm.provider = LLMProvider(provider)\n\n        if model:\n            specs.llm.model = model\n\n        # Create pipeline\n        pipeline = Pipeline(specs)\n\n        # Validate\n        validation = pipeline.validate()\n        if not validation.is_valid:\n            console.print(\"[red]\u274c Validation failed:[/red]\")\n            for error in validation.errors:\n                console.print(f\"  [red]\u2022 {error}[/red]\")\n            sys.exit(1)\n\n        # Estimate\n        console.print(\"[cyan]Estimating cost...[/cyan]\")\n        estimate = pipeline.estimate_cost()\n\n        # Display results\n        table = Table(title=\"Cost Estimate\", show_header=True)\n        table.add_column(\"Metric\", style=\"cyan\", width=20)\n        table.add_column(\"Value\", style=\"green\", width=20)\n\n        table.add_row(\"Total Cost\", f\"${estimate.total_cost}\")\n        table.add_row(\"Total Tokens\", f\"{estimate.total_tokens:,}\")\n        table.add_row(\"Input Tokens\", f\"{estimate.input_tokens:,}\")\n        table.add_row(\"Output Tokens\", f\"{estimate.output_tokens:,}\")\n        table.add_row(\"Rows to Process\", f\"{estimate.rows:,}\")\n        table.add_row(\"Confidence\", estimate.confidence)\n\n        console.print(\"\\n\")\n        console.print(table)\n\n        # Cost per row\n        if estimate.rows &gt; 0:\n            cost_per_row = estimate.total_cost / estimate.rows\n            console.print(f\"\\n[cyan]Cost per row: ${cost_per_row:.6f}[/cyan]\")\n\n        # Warning if expensive\n        if estimate.total_cost &gt; 10.0:\n            console.print(\n                f\"\\n[yellow]\u26a0\ufe0f  Warning: Estimated cost (${estimate.total_cost}) exceeds $10[/yellow]\"\n            )\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.resume","title":"resume","text":"<pre><code>resume(session_id: str, checkpoint_dir: Path, output: Path | None)\n</code></pre> <p>Resume pipeline execution from checkpoint.</p> <p>Useful for recovering from failures or continuing interrupted processing.</p> <p>Examples:</p> <pre><code># Resume from checkpoint\nllm-dataset resume --session-id abc-123-def\n\n# Resume with custom checkpoint directory\nllm-dataset resume -s abc-123 --checkpoint-dir /path/to/checkpoints\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--session-id\",\n    \"-s\",\n    required=True,\n    help=\"Session ID to resume (UUID)\",\n)\n@click.option(\n    \"--checkpoint-dir\",\n    type=click.Path(exists=True, path_type=Path),\n    default=\".checkpoints\",\n    help=\"Checkpoint directory (default: .checkpoints)\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    type=click.Path(path_type=Path),\n    help=\"Override output path\",\n)\ndef resume(\n    session_id: str,\n    checkpoint_dir: Path,\n    output: Path | None,\n):\n    \"\"\"\n    Resume pipeline execution from checkpoint.\n\n    Useful for recovering from failures or continuing interrupted processing.\n\n    Examples:\n\n        # Resume from checkpoint\n        llm-dataset resume --session-id abc-123-def\n\n        # Resume with custom checkpoint directory\n        llm-dataset resume -s abc-123 --checkpoint-dir /path/to/checkpoints\n    \"\"\"\n    try:\n        from ondine.adapters import LocalFileCheckpointStorage\n        from ondine.orchestration import StateManager\n\n        # Load checkpoint\n        console.print(f\"[cyan]Looking for checkpoint in {checkpoint_dir}...[/cyan]\")\n\n        storage = LocalFileCheckpointStorage(str(checkpoint_dir))\n        state_manager = StateManager(storage)\n\n        session_uuid = UUID(session_id)\n\n        if not state_manager.can_resume(session_uuid):\n            console.print(f\"[red]\u274c No checkpoint found for session {session_id}[/red]\")\n            console.print(\n                f\"[yellow]Check checkpoint directory: {checkpoint_dir}[/yellow]\"\n            )\n            sys.exit(1)\n\n        # Load checkpoint\n        checkpoint_info = state_manager.get_latest_checkpoint(session_uuid)\n        console.print(\n            f\"[green]\u2705 Found checkpoint at row {checkpoint_info.row_index}[/green]\"\n        )\n\n        # Resume execution\n        console.print(\"[cyan]Resuming execution...[/cyan]\")\n\n        # Note: Full resume implementation would load the original pipeline\n        # and continue from checkpoint. For now, we show the checkpoint info.\n        console.print(\n            \"\\n[yellow]\u26a0\ufe0f  Full resume functionality requires the original pipeline configuration[/yellow]\"\n        )\n        console.print(\n            \"[yellow]Please use Pipeline.execute(resume_from=session_id) in Python code[/yellow]\"\n        )\n\n        # Display checkpoint info\n        table = Table(title=\"Checkpoint Information\")\n        table.add_column(\"Property\", style=\"cyan\")\n        table.add_column(\"Value\", style=\"green\")\n\n        table.add_row(\"Session ID\", str(checkpoint_info.session_id))\n        table.add_row(\"Checkpoint Path\", checkpoint_info.checkpoint_path)\n        table.add_row(\"Last Row\", str(checkpoint_info.row_index))\n        table.add_row(\"Last Stage\", str(checkpoint_info.stage_index))\n        table.add_row(\"Timestamp\", str(checkpoint_info.timestamp))\n        table.add_row(\"Size\", f\"{checkpoint_info.size_bytes:,} bytes\")\n\n        console.print(table)\n\n    except ValueError:\n        console.print(f\"[red]\u274c Invalid session ID format: {session_id}[/red]\")\n        console.print(\n            \"[yellow]Session ID should be a UUID (e.g., abc-123-def-456)[/yellow]\"\n        )\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.validate","title":"validate","text":"<pre><code>validate(config: Path, verbose: bool)\n</code></pre> <p>Validate pipeline configuration.</p> <p>Checks configuration file for errors and warnings without executing the pipeline.</p> <p>Examples:</p> <pre><code># Validate configuration\nllm-dataset validate -c config.yaml\n\n# Verbose validation\nllm-dataset validate -c config.yaml --verbose\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--config\",\n    \"-c\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to YAML/JSON configuration file\",\n)\n@click.option(\n    \"--verbose\",\n    \"-v\",\n    is_flag=True,\n    help=\"Show detailed validation results\",\n)\ndef validate(config: Path, verbose: bool):\n    \"\"\"\n    Validate pipeline configuration.\n\n    Checks configuration file for errors and warnings without executing\n    the pipeline.\n\n    Examples:\n\n        # Validate configuration\n        llm-dataset validate -c config.yaml\n\n        # Verbose validation\n        llm-dataset validate -c config.yaml --verbose\n    \"\"\"\n    try:\n        # Load configuration\n        console.print(f\"[cyan]Loading configuration from {config}...[/cyan]\")\n        specs = ConfigLoader.from_yaml(str(config))\n\n        console.print(\"[green]\u2705 Configuration loaded successfully[/green]\")\n\n        # Display configuration summary\n        if verbose:\n            table = Table(title=\"Configuration Summary\")\n            table.add_column(\"Component\", style=\"cyan\")\n            table.add_column(\"Details\", style=\"green\")\n\n            table.add_row(\"Dataset\", f\"{specs.dataset.source_type.value}\")\n            table.add_row(\"Input Columns\", \", \".join(specs.dataset.input_columns))\n            table.add_row(\"Output Columns\", \", \".join(specs.dataset.output_columns))\n            table.add_row(\"LLM Provider\", specs.llm.provider.value)\n            table.add_row(\"Model\", specs.llm.model)\n            table.add_row(\"Batch Size\", str(specs.processing.batch_size))\n            table.add_row(\"Concurrency\", str(specs.processing.concurrency))\n\n            if specs.processing.max_budget:\n                table.add_row(\"Max Budget\", f\"${specs.processing.max_budget}\")\n\n            console.print(\"\\n\")\n            console.print(table)\n\n        # Create pipeline for validation\n        console.print(\"\\n[cyan]Validating pipeline...[/cyan]\")\n        pipeline = Pipeline(specs)\n        validation = pipeline.validate()\n\n        if validation.is_valid:\n            console.print(\"[green]\u2705 Pipeline configuration is valid[/green]\")\n\n            if validation.warnings:\n                console.print(\"\\n[yellow]Warnings:[/yellow]\")\n                for warning in validation.warnings:\n                    console.print(f\"  [yellow]\u2022 {warning}[/yellow]\")\n        else:\n            console.print(\"[red]\u274c Pipeline configuration is invalid[/red]\")\n            console.print(\"\\n[red]Errors:[/red]\")\n            for error in validation.errors:\n                console.print(f\"  [red]\u2022 {error}[/red]\")\n\n            if validation.warnings:\n                console.print(\"\\n[yellow]Warnings:[/yellow]\")\n                for warning in validation.warnings:\n                    console.print(f\"  [yellow]\u2022 {warning}[/yellow]\")\n\n            sys.exit(1)\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints(checkpoint_dir: Path)\n</code></pre> <p>List available checkpoints.</p> <p>Shows all saved checkpoints in the specified directory.</p> <p>Examples:</p> <pre><code># List checkpoints\nllm-dataset list-checkpoints\n\n# List from custom directory\nllm-dataset list-checkpoints --checkpoint-dir /path/to/checkpoints\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--checkpoint-dir\",\n    type=click.Path(exists=True, path_type=Path),\n    default=\".checkpoints\",\n    help=\"Checkpoint directory to list (default: .checkpoints)\",\n)\ndef list_checkpoints(checkpoint_dir: Path):\n    \"\"\"\n    List available checkpoints.\n\n    Shows all saved checkpoints in the specified directory.\n\n    Examples:\n\n        # List checkpoints\n        llm-dataset list-checkpoints\n\n        # List from custom directory\n        llm-dataset list-checkpoints --checkpoint-dir /path/to/checkpoints\n    \"\"\"\n    try:\n        from ondine.adapters import LocalFileCheckpointStorage\n\n        console.print(f\"[cyan]Scanning {checkpoint_dir} for checkpoints...[/cyan]\")\n\n        storage = LocalFileCheckpointStorage(checkpoint_dir)\n        checkpoints = storage.list_checkpoints()\n\n        if not checkpoints:\n            console.print(\"[yellow]No checkpoints found[/yellow]\")\n            return\n\n        # Display checkpoints\n        table = Table(title=f\"Checkpoints in {checkpoint_dir}\")\n        table.add_column(\"Session ID\", style=\"cyan\")\n        table.add_column(\"Row\", style=\"green\")\n        table.add_column(\"Stage\", style=\"green\")\n        table.add_column(\"Timestamp\", style=\"yellow\")\n        table.add_column(\"Size\", style=\"magenta\")\n\n        for cp in checkpoints:\n            table.add_row(\n                str(cp.session_id)[:8] + \"...\",\n                str(cp.row_index),\n                str(cp.stage_index),\n                cp.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                f\"{cp.size_bytes:,} bytes\",\n            )\n\n        console.print(\"\\n\")\n        console.print(table)\n        console.print(f\"\\n[cyan]Total checkpoints: {len(checkpoints)}[/cyan]\")\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.inspect","title":"inspect","text":"<pre><code>inspect(input: Path, head: int)\n</code></pre> <p>Inspect input data file.</p> <p>Shows file info and preview of first N rows.</p> <p>Examples:</p> <pre><code># Inspect CSV file\nllm-dataset inspect -i data.csv\n\n# Show first 10 rows\nllm-dataset inspect -i data.csv --head 10\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--input\",\n    \"-i\",\n    required=True,\n    type=click.Path(exists=True, path_type=Path),\n    help=\"Path to input file to inspect\",\n)\n@click.option(\n    \"--head\",\n    type=int,\n    default=5,\n    help=\"Number of rows to show (default: 5)\",\n)\ndef inspect(input: Path, head: int):\n    \"\"\"\n    Inspect input data file.\n\n    Shows file info and preview of first N rows.\n\n    Examples:\n\n        # Inspect CSV file\n        llm-dataset inspect -i data.csv\n\n        # Show first 10 rows\n        llm-dataset inspect -i data.csv --head 10\n    \"\"\"\n    try:\n        import pandas as pd\n\n        console.print(f\"[cyan]Inspecting {input}...[/cyan]\")\n\n        # Detect file type\n        suffix = input.suffix.lower()\n\n        if suffix == \".csv\":\n            df = pd.read_csv(input)\n        elif suffix in [\".xlsx\", \".xls\"]:\n            df = pd.read_excel(input)\n        elif suffix == \".parquet\":\n            df = pd.read_parquet(input)\n        else:\n            console.print(f\"[red]\u274c Unsupported file type: {suffix}[/red]\")\n            sys.exit(1)\n\n        # File info\n        info_table = Table(title=\"File Information\")\n        info_table.add_column(\"Property\", style=\"cyan\")\n        info_table.add_column(\"Value\", style=\"green\")\n\n        info_table.add_row(\"File Path\", str(input))\n        info_table.add_row(\"File Type\", suffix[1:].upper())\n        info_table.add_row(\"Total Rows\", f\"{len(df):,}\")\n        info_table.add_row(\"Total Columns\", str(len(df.columns)))\n        info_table.add_row(\n            \"Memory Usage\", f\"{df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\"\n        )\n\n        console.print(\"\\n\")\n        console.print(info_table)\n\n        # Columns\n        console.print(\"\\n[cyan]Columns:[/cyan]\")\n        for col in df.columns:\n            dtype = df[col].dtype\n            null_count = df[col].isnull().sum()\n            console.print(f\"  \u2022 {col} ({dtype}) - {null_count} nulls\")\n\n        # Preview\n        console.print(f\"\\n[cyan]First {head} rows:[/cyan]\")\n        console.print(df.head(head).to_string())\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/cli/main/#ondine.cli.main.list_providers","title":"list_providers","text":"<pre><code>list_providers()\n</code></pre> <p>List all available LLM providers with details.</p> <p>Shows supported providers, their platforms, costs, and requirements.</p> <p>Examples:</p> <pre><code># List all providers\nondine list-providers\n</code></pre> Source code in <code>ondine/cli/main.py</code> <pre><code>@cli.command()\ndef list_providers():\n    \"\"\"\n    List all available LLM providers with details.\n\n    Shows supported providers, their platforms, costs, and requirements.\n\n    Examples:\n\n        # List all providers\n        ondine list-providers\n    \"\"\"\n    try:\n        # Create table\n        table = Table(title=\"\ud83e\udebd Available LLM Providers\", show_header=True)\n        table.add_column(\"Provider ID\", style=\"cyan\", width=20)\n        table.add_column(\"Name\", style=\"bright_white\", width=20)\n        table.add_column(\"Platform\", style=\"yellow\", width=25)\n        table.add_column(\"Cost\", style=\"magenta\", width=12)\n        table.add_column(\"Use Case\", style=\"white\", width=35)\n\n        # Add rows for each provider\n        for provider in LLMProvider:\n            metadata = PROVIDER_METADATA[provider]\n\n            # Color-code cost\n            cost = metadata[\"cost\"]\n            if \"Free\" in cost or cost == \"Varies\":\n                cost_colored = f\"[green]{cost}[/green]\"\n            elif cost == \"$$\":\n                cost_colored = f\"[yellow]{cost}[/yellow]\"\n            else:  # $$$\n                cost_colored = f\"[red]{cost}[/red]\"\n\n            table.add_row(\n                f\"[bold]{provider.value}[/bold]\",\n                metadata[\"name\"],\n                metadata[\"platform\"],\n                cost_colored,\n                metadata[\"use_case\"],\n            )\n\n        console.print(\"\\n\")\n        console.print(table)\n\n        # Requirements section\n        console.print(\"\\n[cyan]\ud83d\udccb Requirements by Provider:[/cyan]\")\n        for provider in LLMProvider:\n            metadata = PROVIDER_METADATA[provider]\n            console.print(\n                f\"  [bold cyan]{provider.value}[/bold cyan]: {metadata['requirements']}\"\n            )\n\n        # Usage examples\n        console.print(\"\\n[cyan]\ud83d\udca1 Usage Examples:[/cyan]\")\n        console.print(\"  [dim]# Use OpenAI[/dim]\")\n        console.print(\"  ondine process --provider openai --config config.yaml\")\n        console.print(\"\\n  [dim]# Use local MLX on Apple Silicon[/dim]\")\n        console.print(\"  ondine process --provider mlx --config config.yaml\")\n        console.print(\"\\n  [dim]# Use custom API (Ollama, vLLM, Together.AI)[/dim]\")\n        console.print(\n            \"  ondine process --provider openai_compatible --config config.yaml\"\n        )\n        console.print(\n            \"\\n  [dim]\ud83d\udca1 Tip: Set provider in your YAML config file or use --provider flag[/dim]\\n\"\n        )\n\n    except Exception as e:\n        console.print(f\"[red]\u274c Error: {e}[/red]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/config/","title":"config","text":""},{"location":"api/config/#ondine.config","title":"config","text":"<p>Configuration loading from files.</p>"},{"location":"api/config/#ondine.config.ConfigLoader","title":"ConfigLoader","text":"<p>Loads pipeline configurations from YAML or JSON files.</p> <p>Follows Single Responsibility: only handles config file loading.</p>"},{"location":"api/config/#ondine.config.ConfigLoader.from_yaml","title":"from_yaml  <code>staticmethod</code>","text":"<pre><code>from_yaml(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to YAML file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid YAML or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_yaml(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from YAML file.\n\n    Args:\n        file_path: Path to YAML file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid YAML or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = yaml.safe_load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/#ondine.config.ConfigLoader.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to JSON file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid JSON or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_json(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid JSON or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = json.load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/#ondine.config.ConfigLoader.to_yaml","title":"to_yaml  <code>staticmethod</code>","text":"<pre><code>to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to YAML file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        yaml.dump(config_dict, f, default_flow_style=False, indent=2)\n</code></pre>"},{"location":"api/config/#ondine.config.ConfigLoader.to_json","title":"to_json  <code>staticmethod</code>","text":"<pre><code>to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to JSON file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        json.dump(config_dict, f, indent=2, default=str)\n</code></pre>"},{"location":"api/config/config_loader/","title":"config_loader","text":""},{"location":"api/config/config_loader/#ondine.config.config_loader","title":"config_loader","text":"<p>Configuration loader for YAML and JSON files.</p> <p>Enables loading pipeline configurations from declarative files.</p>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader","title":"ConfigLoader","text":"<p>Loads pipeline configurations from YAML or JSON files.</p> <p>Follows Single Responsibility: only handles config file loading.</p>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.from_yaml","title":"from_yaml  <code>staticmethod</code>","text":"<pre><code>from_yaml(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to YAML file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid YAML or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_yaml(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from YAML file.\n\n    Args:\n        file_path: Path to YAML file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid YAML or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = yaml.safe_load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.from_json","title":"from_json  <code>staticmethod</code>","text":"<pre><code>from_json(file_path: str | Path) -&gt; PipelineSpecifications\n</code></pre> <p>Load configuration from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to JSON file</p> required <p>Returns:</p> Type Description <code>PipelineSpecifications</code> <p>PipelineSpecifications</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>ValueError</code> <p>If invalid JSON or configuration</p> Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef from_json(file_path: str | Path) -&gt; PipelineSpecifications:\n    \"\"\"\n    Load configuration from JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        PipelineSpecifications\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        ValueError: If invalid JSON or configuration\n    \"\"\"\n    path = Path(file_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(path) as f:\n        config_dict = json.load(f)\n\n    return ConfigLoader._dict_to_specifications(config_dict)\n</code></pre>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.to_yaml","title":"to_yaml  <code>staticmethod</code>","text":"<pre><code>to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_yaml(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to YAML file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        yaml.dump(config_dict, f, default_flow_style=False, indent=2)\n</code></pre>"},{"location":"api/config/config_loader/#ondine.config.config_loader.ConfigLoader.to_json","title":"to_json  <code>staticmethod</code>","text":"<pre><code>to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None\n</code></pre> <p>Save specifications to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>specifications</code> <code>PipelineSpecifications</code> <p>Pipeline specifications</p> required <code>file_path</code> <code>str | Path</code> <p>Destination file path</p> required Source code in <code>ondine/config/config_loader.py</code> <pre><code>@staticmethod\ndef to_json(specifications: PipelineSpecifications, file_path: str | Path) -&gt; None:\n    \"\"\"\n    Save specifications to JSON file.\n\n    Args:\n        specifications: Pipeline specifications\n        file_path: Destination file path\n    \"\"\"\n    path = Path(file_path)\n\n    # Convert to dict\n    config_dict = specifications.model_dump(mode=\"json\")\n\n    with open(path, \"w\") as f:\n        json.dump(config_dict, f, indent=2, default=str)\n</code></pre>"},{"location":"api/core/","title":"core","text":""},{"location":"api/core/#ondine.core","title":"core","text":"<p>Core configuration and data models.</p>"},{"location":"api/core/#ondine.core.CheckpointInfo","title":"CheckpointInfo  <code>dataclass</code>","text":"<pre><code>CheckpointInfo(session_id: UUID, checkpoint_path: str, row_index: int, stage_index: int, timestamp: datetime, size_bytes: int)\n</code></pre> <p>Information about a checkpoint.</p>"},{"location":"api/core/#ondine.core.CostEstimate","title":"CostEstimate  <code>dataclass</code>","text":"<pre><code>CostEstimate(total_cost: Decimal, total_tokens: int, input_tokens: int, output_tokens: int, rows: int, breakdown_by_stage: dict[str, Decimal] = dict(), confidence: str = 'estimate')\n</code></pre> <p>Cost estimation for pipeline execution.</p>"},{"location":"api/core/#ondine.core.ErrorInfo","title":"ErrorInfo  <code>dataclass</code>","text":"<pre><code>ErrorInfo(row_index: int, stage_name: str, error_type: str, error_message: str, timestamp: datetime, context: dict[str, Any] = dict())\n</code></pre> <p>Information about an error during processing.</p>"},{"location":"api/core/#ondine.core.ExecutionResult","title":"ExecutionResult  <code>dataclass</code>","text":"<pre><code>ExecutionResult(data: DataFrame, metrics: ProcessingStats, costs: CostEstimate, errors: list[ErrorInfo] = list(), execution_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, success: bool = True, metadata: dict[str, Any] = dict())\n</code></pre> <p>Complete result from pipeline execution.</p>"},{"location":"api/core/#ondine.core.ExecutionResult.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Get execution duration in seconds.</p>"},{"location":"api/core/#ondine.core.ExecutionResult.error_rate","title":"error_rate  <code>property</code>","text":"<pre><code>error_rate: float\n</code></pre> <p>Get error rate as percentage.</p>"},{"location":"api/core/#ondine.core.ExecutionResult.validate_output_quality","title":"validate_output_quality","text":"<pre><code>validate_output_quality(output_columns: list[str]) -&gt; QualityReport\n</code></pre> <p>Validate the quality of output data by checking for null/empty values.</p> <p>Parameters:</p> Name Type Description Default <code>output_columns</code> <code>list[str]</code> <p>List of output column names to check</p> required <p>Returns:</p> Type Description <code>QualityReport</code> <p>QualityReport with quality metrics and warnings</p> Source code in <code>ondine/core/models.py</code> <pre><code>def validate_output_quality(self, output_columns: list[str]) -&gt; \"QualityReport\":\n    \"\"\"\n    Validate the quality of output data by checking for null/empty values.\n\n    Args:\n        output_columns: List of output column names to check\n\n    Returns:\n        QualityReport with quality metrics and warnings\n    \"\"\"\n    total = len(self.data)\n\n    # Count null and empty values across output columns\n    null_count = 0\n    empty_count = 0\n\n    for col in output_columns:\n        if col in self.data.columns:\n            # Count nulls (None, NaN, NaT)\n            null_count += self.data[col].isna().sum()\n            # Count empty strings (only for string columns)\n            if self.data[col].dtype == \"object\":\n                empty_count += (self.data[col].astype(str).str.strip() == \"\").sum()\n\n    # Calculate per-column metrics (exclude both nulls and empties)\n    valid_outputs = total - null_count - empty_count\n    success_rate = (valid_outputs / total * 100) if total &gt; 0 else 0.0\n\n    # Determine quality score\n    if success_rate &gt;= 95.0:\n        quality_score = \"excellent\"\n    elif success_rate &gt;= 80.0:\n        quality_score = \"good\"\n    elif success_rate &gt;= 50.0:\n        quality_score = \"poor\"\n    else:\n        quality_score = \"critical\"\n\n    # Generate warnings and issues\n    warnings = []\n    issues = []\n\n    if success_rate &lt; 70.0:\n        issues.append(\n            f\"\u26a0\ufe0f  LOW SUCCESS RATE: Only {success_rate:.1f}% of outputs are valid \"\n            f\"({valid_outputs}/{total} rows)\"\n        )\n\n    if null_count &gt; total * 0.3:  # &gt; 30% nulls\n        issues.append(\n            f\"\u26a0\ufe0f  HIGH NULL RATE: {null_count} null values found \"\n            f\"({null_count / total * 100:.1f}% of rows)\"\n        )\n\n    if empty_count &gt; total * 0.1:  # &gt; 10% empty\n        warnings.append(\n            f\"Empty outputs detected: {empty_count} rows \"\n            f\"({empty_count / total * 100:.1f}%)\"\n        )\n\n    # Check if reported metrics match actual data quality\n    if self.metrics.failed_rows == 0 and null_count &gt; 0:\n        issues.append(\n            f\"\u26a0\ufe0f  METRICS MISMATCH: Pipeline reported 0 failures but \"\n            f\"{null_count} rows have null outputs. This may indicate silent errors.\"\n        )\n\n    return QualityReport(\n        total_rows=total,\n        valid_outputs=valid_outputs,\n        null_outputs=null_count,\n        empty_outputs=empty_count,\n        success_rate=success_rate,\n        quality_score=quality_score,\n        warnings=warnings,\n        issues=issues,\n    )\n</code></pre>"},{"location":"api/core/#ondine.core.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(text: str, tokens_in: int, tokens_out: int, model: str, cost: Decimal, latency_ms: float, metadata: dict[str, Any] = dict())\n</code></pre> <p>Response from a single LLM invocation.</p>"},{"location":"api/core/#ondine.core.ProcessingStats","title":"ProcessingStats  <code>dataclass</code>","text":"<pre><code>ProcessingStats(total_rows: int, processed_rows: int, failed_rows: int, skipped_rows: int, rows_per_second: float, total_duration_seconds: float, stage_durations: dict[str, float] = dict())\n</code></pre> <p>Statistics from pipeline execution.</p>"},{"location":"api/core/#ondine.core.PromptBatch","title":"PromptBatch  <code>dataclass</code>","text":"<pre><code>PromptBatch(prompts: list[str], metadata: list[RowMetadata], batch_id: int)\n</code></pre> <p>Batch of prompts for processing.</p>"},{"location":"api/core/#ondine.core.ResponseBatch","title":"ResponseBatch  <code>dataclass</code>","text":"<pre><code>ResponseBatch(responses: list[str], metadata: list[RowMetadata], tokens_used: int, cost: Decimal, batch_id: int, latencies_ms: list[float] = list())\n</code></pre> <p>Batch of responses from LLM.</p>"},{"location":"api/core/#ondine.core.RowMetadata","title":"RowMetadata  <code>dataclass</code>","text":"<pre><code>RowMetadata(row_index: int, row_id: Any | None = None, batch_id: int | None = None, attempt: int = 1, custom: dict[str, Any] = dict())\n</code></pre> <p>Metadata for a single row during processing.</p>"},{"location":"api/core/#ondine.core.ValidationResult","title":"ValidationResult  <code>dataclass</code>","text":"<pre><code>ValidationResult(is_valid: bool, errors: list[str] = list(), warnings: list[str] = list())\n</code></pre> <p>Result from validation checks.</p>"},{"location":"api/core/#ondine.core.ValidationResult.add_error","title":"add_error","text":"<pre><code>add_error(error: str) -&gt; None\n</code></pre> <p>Add an error message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_error(self, error: str) -&gt; None:\n    \"\"\"Add an error message.\"\"\"\n    self.errors.append(error)\n    self.is_valid = False\n</code></pre>"},{"location":"api/core/#ondine.core.ValidationResult.add_warning","title":"add_warning","text":"<pre><code>add_warning(warning: str) -&gt; None\n</code></pre> <p>Add a warning message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_warning(self, warning: str) -&gt; None:\n    \"\"\"Add a warning message.\"\"\"\n    self.warnings.append(warning)\n</code></pre>"},{"location":"api/core/#ondine.core.WriteConfirmation","title":"WriteConfirmation  <code>dataclass</code>","text":"<pre><code>WriteConfirmation(path: str, rows_written: int, success: bool, timestamp: datetime = datetime.now(), metadata: dict[str, Any] = dict())\n</code></pre> <p>Confirmation of successful data write.</p>"},{"location":"api/core/#ondine.core.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for data source configuration.</p>"},{"location":"api/core/#ondine.core.DatasetSpec.validate_source_path","title":"validate_source_path  <code>classmethod</code>","text":"<pre><code>validate_source_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"source_path\")\n@classmethod\ndef validate_source_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/#ondine.core.DatasetSpec.validate_no_overlap","title":"validate_no_overlap  <code>classmethod</code>","text":"<pre><code>validate_no_overlap(v: list[str], info: Any) -&gt; list[str]\n</code></pre> <p>Ensure output columns don't overlap with input columns.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"output_columns\")\n@classmethod\ndef validate_no_overlap(cls, v: list[str], info: Any) -&gt; list[str]:\n    \"\"\"Ensure output columns don't overlap with input columns.\"\"\"\n    if \"input_columns\" in info.data:\n        input_cols = set(info.data[\"input_columns\"])\n        output_cols = set(v)\n        overlap = input_cols &amp; output_cols\n        if overlap:\n            raise ValueError(f\"Output columns overlap with input: {overlap}\")\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.DataSourceType","title":"DataSourceType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported data source types.</p>"},{"location":"api/core/#ondine.core.ErrorPolicy","title":"ErrorPolicy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Error handling policies for processing failures.</p>"},{"location":"api/core/#ondine.core.LLMProvider","title":"LLMProvider","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported LLM providers.</p>"},{"location":"api/core/#ondine.core.LLMSpec","title":"LLMSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for LLM provider configuration.</p>"},{"location":"api/core/#ondine.core.LLMSpec.validate_base_url_format","title":"validate_base_url_format  <code>classmethod</code>","text":"<pre><code>validate_base_url_format(v: str | None) -&gt; str | None\n</code></pre> <p>Validate base_url is a valid HTTP(S) URL with a host.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"base_url\")\n@classmethod\ndef validate_base_url_format(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate base_url is a valid HTTP(S) URL with a host.\"\"\"\n    if v is None:\n        return v\n    from urllib.parse import urlparse\n\n    parsed = urlparse(v)\n    if parsed.scheme not in {\"http\", \"https\"}:\n        raise ValueError(\"base_url must start with http:// or https://\")\n    if not parsed.netloc:\n        raise ValueError(\n            \"base_url must include a host (e.g., localhost, api.example.com)\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.LLMSpec.validate_azure_config","title":"validate_azure_config  <code>classmethod</code>","text":"<pre><code>validate_azure_config(v: str | None, info: Any) -&gt; str | None\n</code></pre> <p>Validate Azure-specific configuration.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"azure_endpoint\", \"azure_deployment\")\n@classmethod\ndef validate_azure_config(cls, v: str | None, info: Any) -&gt; str | None:\n    \"\"\"Validate Azure-specific configuration.\"\"\"\n    if info.data.get(\"provider\") == LLMProvider.AZURE_OPENAI and v is None:\n        field_name = info.field_name\n        raise ValueError(f\"{field_name} required for Azure OpenAI provider\")\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.LLMSpec.validate_provider_requirements","title":"validate_provider_requirements","text":"<pre><code>validate_provider_requirements() -&gt; LLMSpec\n</code></pre> <p>Validate provider-specific requirements.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    \"\"\"Validate provider-specific requirements.\"\"\"\n    # Check openai_compatible requires base_url\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre>"},{"location":"api/core/#ondine.core.MergeStrategy","title":"MergeStrategy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Output merge strategies.</p>"},{"location":"api/core/#ondine.core.OutputSpec","title":"OutputSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for output configuration.</p>"},{"location":"api/core/#ondine.core.OutputSpec.validate_destination_path","title":"validate_destination_path  <code>classmethod</code>","text":"<pre><code>validate_destination_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"destination_path\")\n@classmethod\ndef validate_destination_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/#ondine.core.PipelineSpecifications","title":"PipelineSpecifications","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for all pipeline specifications.</p>"},{"location":"api/core/#ondine.core.ProcessingSpec","title":"ProcessingSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for processing parameters.</p>"},{"location":"api/core/#ondine.core.ProcessingSpec.validate_checkpoint_dir","title":"validate_checkpoint_dir  <code>classmethod</code>","text":"<pre><code>validate_checkpoint_dir(v: str | Path) -&gt; Path\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"checkpoint_dir\")\n@classmethod\ndef validate_checkpoint_dir(cls, v: str | Path) -&gt; Path:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/#ondine.core.PromptSpec","title":"PromptSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for prompt template configuration.</p>"},{"location":"api/core/#ondine.core.PromptSpec.validate_template","title":"validate_template  <code>classmethod</code>","text":"<pre><code>validate_template(v: str) -&gt; str\n</code></pre> <p>Validate template has at least one variable.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"template\")\n@classmethod\ndef validate_template(cls, v: str) -&gt; str:\n    \"\"\"Validate template has at least one variable.\"\"\"\n    if \"{\" not in v or \"}\" not in v:\n        raise ValueError(\n            \"Template must contain at least one variable in {var} format\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/#ondine.core.PromptSpec.validate_response_format","title":"validate_response_format  <code>classmethod</code>","text":"<pre><code>validate_response_format(v: str) -&gt; str\n</code></pre> <p>Validate response format is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"response_format\")\n@classmethod\ndef validate_response_format(cls, v: str) -&gt; str:\n    \"\"\"Validate response format is supported.\"\"\"\n    allowed = [\"raw\", \"json\", \"regex\"]\n    if v not in allowed:\n        raise ValueError(f\"response_format must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/core/error_handler/","title":"error_handler","text":""},{"location":"api/core/error_handler/#ondine.core.error_handler","title":"error_handler","text":"<p>Error handling system with configurable policies.</p> <p>Implements Strategy pattern for different error handling approaches.</p>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorAction","title":"ErrorAction","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Actions to take on errors.</p>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorDecision","title":"ErrorDecision  <code>dataclass</code>","text":"<pre><code>ErrorDecision(action: ErrorAction, default_value: Any = None, retry_count: int = 0, context: dict[str, Any] | None = None)\n</code></pre> <p>Decision on how to handle an error.</p>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorHandler","title":"ErrorHandler","text":"<pre><code>ErrorHandler(policy: ErrorPolicy = ErrorPolicy.SKIP, max_retries: int = 3, default_value: Any = None, default_value_factory: Callable[[], Any] | None = None)\n</code></pre> <p>Policy-based error handling (orchestrates retry/skip/fail decisions).</p> <p>Scope: Stage execution errors and pipeline-level error handling Policies: SKIP, FAIL, RETRY (delegates to RetryHandler for execution) Use when: Configuring how the pipeline handles errors</p> <p>Policy Behaviors: - SKIP: Log error and skip the row (continue processing) - FAIL: Raise error and stop pipeline - RETRY: Retry the operation (delegates to RetryHandler) - DEFAULT: Return a default value on error</p> Example <p>handler = ErrorHandler(policy=ErrorPolicy.RETRY, max_retries=3) decision = handler.handle_error(exception, context)</p> <p>See Also: - RetryHandler: Executes the actual retry logic - Pipeline._auto_retry_failed_rows(): Row-level quality retry - docs/architecture/decisions/ADR-006-retry-levels.md</p> Design Note <p>ErrorHandler decides WHAT to do (policy) RetryHandler decides HOW to do it (exponential backoff)</p> <p>Initialize error handler.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>ErrorPolicy</code> <p>Error handling policy</p> <code>SKIP</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>default_value</code> <code>Any</code> <p>Static default value (or use default_value_factory)</p> <code>None</code> <code>default_value_factory</code> <code>Callable[[], Any] | None</code> <p>Function to generate default values</p> <code>None</code> Source code in <code>ondine/core/error_handler.py</code> <pre><code>def __init__(\n    self,\n    policy: ErrorPolicy = ErrorPolicy.SKIP,\n    max_retries: int = 3,\n    default_value: Any = None,\n    default_value_factory: Callable[[], Any] | None = None,\n):\n    \"\"\"\n    Initialize error handler.\n\n    Args:\n        policy: Error handling policy\n        max_retries: Maximum retry attempts\n        default_value: Static default value (or use default_value_factory)\n        default_value_factory: Function to generate default values\n    \"\"\"\n    self.policy = policy\n    self.max_retries = max_retries\n    self.default_value = default_value\n\n    # If default_value_factory is provided, use it; otherwise use lambda returning default_value\n    if default_value_factory is not None:\n        self.default_value_factory = default_value_factory\n    else:\n        self.default_value_factory = lambda: default_value\n</code></pre>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorHandler.handle_error","title":"handle_error","text":"<pre><code>handle_error(error: Exception, context: dict[str, Any], attempt: int = 1) -&gt; ErrorDecision\n</code></pre> <p>Decide how to handle an error.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>context</code> <code>dict[str, Any]</code> <p>Error context (row_index, stage, etc.)</p> required <code>attempt</code> <code>int</code> <p>Current attempt number</p> <code>1</code> <p>Returns:</p> Type Description <code>ErrorDecision</code> <p>ErrorDecision with action to take</p> Source code in <code>ondine/core/error_handler.py</code> <pre><code>def handle_error(\n    self,\n    error: Exception,\n    context: dict[str, Any],\n    attempt: int = 1,\n) -&gt; ErrorDecision:\n    \"\"\"\n    Decide how to handle an error.\n\n    Args:\n        error: The exception that occurred\n        context: Error context (row_index, stage, etc.)\n        attempt: Current attempt number\n\n    Returns:\n        ErrorDecision with action to take\n    \"\"\"\n    # Get attempt from context if available, otherwise use parameter\n    attempt = context.get(\"attempt\", attempt)\n    row_index = context.get(\"row_index\", \"unknown\")\n    stage = context.get(\"stage\", \"unknown\")\n\n    # Log the error\n    logger.error(\n        f\"Error in {stage} at row {row_index}: {error}\",\n        exc_info=True,\n    )\n\n    # Check for FATAL errors that should always fail immediately\n    if self._is_fatal_error(error):\n        logger.error(\n            f\"\u274c FATAL ERROR: {error}\\n\"\n            f\"   This error cannot be recovered. Pipeline will terminate.\"\n        )\n        return ErrorDecision(\n            action=ErrorAction.FAIL,\n            context=context,\n        )\n\n    # Apply policy\n    if self.policy == ErrorPolicy.RETRY:\n        if attempt &lt; self.max_retries:\n            logger.info(f\"Retrying (attempt {attempt + 1}/{self.max_retries})\")\n            return ErrorDecision(\n                action=ErrorAction.RETRY,\n                retry_count=attempt + 1,\n                context=context,\n            )\n        logger.warning(f\"Max retries ({self.max_retries}) exceeded, skipping\")\n        return ErrorDecision(\n            action=ErrorAction.SKIP,\n            context=context,\n        )\n\n    if self.policy == ErrorPolicy.SKIP:\n        logger.info(f\"Skipping row {row_index} due to error\")\n        return ErrorDecision(\n            action=ErrorAction.SKIP,\n            context=context,\n        )\n\n    if self.policy == ErrorPolicy.USE_DEFAULT:\n        default = self.default_value_factory()\n        logger.info(f\"Using default value for row {row_index}: {default}\")\n        return ErrorDecision(\n            action=ErrorAction.USE_DEFAULT,\n            default_value=default,\n            context=context,\n        )\n\n    if self.policy == ErrorPolicy.FAIL:\n        logger.error(\"Failing pipeline due to error\")\n        return ErrorDecision(\n            action=ErrorAction.FAIL,\n            context=context,\n        )\n\n    # Unknown policy, default to fail\n    return ErrorDecision(\n        action=ErrorAction.FAIL,\n        context=context,\n    )\n</code></pre>"},{"location":"api/core/error_handler/#ondine.core.error_handler.ErrorHandler.should_retry","title":"should_retry","text":"<pre><code>should_retry(error: Exception) -&gt; bool\n</code></pre> <p>Determine if error should be retried.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if retriable</p> Source code in <code>ondine/core/error_handler.py</code> <pre><code>def should_retry(self, error: Exception) -&gt; bool:\n    \"\"\"\n    Determine if error should be retried.\n\n    Args:\n        error: The exception\n\n    Returns:\n        True if retriable\n    \"\"\"\n    # Don't retry fatal errors\n    if self._is_fatal_error(error):\n        return False\n\n    retriable_keywords = [\n        \"rate limit\",\n        \"timeout\",\n        \"network\",\n        \"connection\",\n        \"503\",\n        \"502\",\n        \"429\",\n    ]\n\n    error_str = str(error).lower()\n    return any(keyword in error_str for keyword in retriable_keywords)\n</code></pre>"},{"location":"api/core/models/","title":"models","text":""},{"location":"api/core/models/#ondine.core.models","title":"models","text":"<p>Core data models for execution results and metadata.</p> <p>These models represent the outputs and state information from pipeline execution with type safety.</p>"},{"location":"api/core/models/#ondine.core.models.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(text: str, tokens_in: int, tokens_out: int, model: str, cost: Decimal, latency_ms: float, metadata: dict[str, Any] = dict())\n</code></pre> <p>Response from a single LLM invocation.</p>"},{"location":"api/core/models/#ondine.core.models.CostEstimate","title":"CostEstimate  <code>dataclass</code>","text":"<pre><code>CostEstimate(total_cost: Decimal, total_tokens: int, input_tokens: int, output_tokens: int, rows: int, breakdown_by_stage: dict[str, Decimal] = dict(), confidence: str = 'estimate')\n</code></pre> <p>Cost estimation for pipeline execution.</p>"},{"location":"api/core/models/#ondine.core.models.ProcessingStats","title":"ProcessingStats  <code>dataclass</code>","text":"<pre><code>ProcessingStats(total_rows: int, processed_rows: int, failed_rows: int, skipped_rows: int, rows_per_second: float, total_duration_seconds: float, stage_durations: dict[str, float] = dict())\n</code></pre> <p>Statistics from pipeline execution.</p>"},{"location":"api/core/models/#ondine.core.models.ErrorInfo","title":"ErrorInfo  <code>dataclass</code>","text":"<pre><code>ErrorInfo(row_index: int, stage_name: str, error_type: str, error_message: str, timestamp: datetime, context: dict[str, Any] = dict())\n</code></pre> <p>Information about an error during processing.</p>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult","title":"ExecutionResult  <code>dataclass</code>","text":"<pre><code>ExecutionResult(data: DataFrame, metrics: ProcessingStats, costs: CostEstimate, errors: list[ErrorInfo] = list(), execution_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, success: bool = True, metadata: dict[str, Any] = dict())\n</code></pre> <p>Complete result from pipeline execution.</p>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Get execution duration in seconds.</p>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult.error_rate","title":"error_rate  <code>property</code>","text":"<pre><code>error_rate: float\n</code></pre> <p>Get error rate as percentage.</p>"},{"location":"api/core/models/#ondine.core.models.ExecutionResult.validate_output_quality","title":"validate_output_quality","text":"<pre><code>validate_output_quality(output_columns: list[str]) -&gt; QualityReport\n</code></pre> <p>Validate the quality of output data by checking for null/empty values.</p> <p>Parameters:</p> Name Type Description Default <code>output_columns</code> <code>list[str]</code> <p>List of output column names to check</p> required <p>Returns:</p> Type Description <code>QualityReport</code> <p>QualityReport with quality metrics and warnings</p> Source code in <code>ondine/core/models.py</code> <pre><code>def validate_output_quality(self, output_columns: list[str]) -&gt; \"QualityReport\":\n    \"\"\"\n    Validate the quality of output data by checking for null/empty values.\n\n    Args:\n        output_columns: List of output column names to check\n\n    Returns:\n        QualityReport with quality metrics and warnings\n    \"\"\"\n    total = len(self.data)\n\n    # Count null and empty values across output columns\n    null_count = 0\n    empty_count = 0\n\n    for col in output_columns:\n        if col in self.data.columns:\n            # Count nulls (None, NaN, NaT)\n            null_count += self.data[col].isna().sum()\n            # Count empty strings (only for string columns)\n            if self.data[col].dtype == \"object\":\n                empty_count += (self.data[col].astype(str).str.strip() == \"\").sum()\n\n    # Calculate per-column metrics (exclude both nulls and empties)\n    valid_outputs = total - null_count - empty_count\n    success_rate = (valid_outputs / total * 100) if total &gt; 0 else 0.0\n\n    # Determine quality score\n    if success_rate &gt;= 95.0:\n        quality_score = \"excellent\"\n    elif success_rate &gt;= 80.0:\n        quality_score = \"good\"\n    elif success_rate &gt;= 50.0:\n        quality_score = \"poor\"\n    else:\n        quality_score = \"critical\"\n\n    # Generate warnings and issues\n    warnings = []\n    issues = []\n\n    if success_rate &lt; 70.0:\n        issues.append(\n            f\"\u26a0\ufe0f  LOW SUCCESS RATE: Only {success_rate:.1f}% of outputs are valid \"\n            f\"({valid_outputs}/{total} rows)\"\n        )\n\n    if null_count &gt; total * 0.3:  # &gt; 30% nulls\n        issues.append(\n            f\"\u26a0\ufe0f  HIGH NULL RATE: {null_count} null values found \"\n            f\"({null_count / total * 100:.1f}% of rows)\"\n        )\n\n    if empty_count &gt; total * 0.1:  # &gt; 10% empty\n        warnings.append(\n            f\"Empty outputs detected: {empty_count} rows \"\n            f\"({empty_count / total * 100:.1f}%)\"\n        )\n\n    # Check if reported metrics match actual data quality\n    if self.metrics.failed_rows == 0 and null_count &gt; 0:\n        issues.append(\n            f\"\u26a0\ufe0f  METRICS MISMATCH: Pipeline reported 0 failures but \"\n            f\"{null_count} rows have null outputs. This may indicate silent errors.\"\n        )\n\n    return QualityReport(\n        total_rows=total,\n        valid_outputs=valid_outputs,\n        null_outputs=null_count,\n        empty_outputs=empty_count,\n        success_rate=success_rate,\n        quality_score=quality_score,\n        warnings=warnings,\n        issues=issues,\n    )\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.ValidationResult","title":"ValidationResult  <code>dataclass</code>","text":"<pre><code>ValidationResult(is_valid: bool, errors: list[str] = list(), warnings: list[str] = list())\n</code></pre> <p>Result from validation checks.</p>"},{"location":"api/core/models/#ondine.core.models.ValidationResult.add_error","title":"add_error","text":"<pre><code>add_error(error: str) -&gt; None\n</code></pre> <p>Add an error message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_error(self, error: str) -&gt; None:\n    \"\"\"Add an error message.\"\"\"\n    self.errors.append(error)\n    self.is_valid = False\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.ValidationResult.add_warning","title":"add_warning","text":"<pre><code>add_warning(warning: str) -&gt; None\n</code></pre> <p>Add a warning message.</p> Source code in <code>ondine/core/models.py</code> <pre><code>def add_warning(self, warning: str) -&gt; None:\n    \"\"\"Add a warning message.\"\"\"\n    self.warnings.append(warning)\n</code></pre>"},{"location":"api/core/models/#ondine.core.models.QualityReport","title":"QualityReport  <code>dataclass</code>","text":"<pre><code>QualityReport(total_rows: int, valid_outputs: int, null_outputs: int, empty_outputs: int, success_rate: float, quality_score: str, warnings: list[str] = list(), issues: list[str] = list())\n</code></pre> <p>Quality assessment of pipeline output.</p>"},{"location":"api/core/models/#ondine.core.models.QualityReport.is_acceptable","title":"is_acceptable  <code>property</code>","text":"<pre><code>is_acceptable: bool\n</code></pre> <p>Check if quality is acceptable (&gt;= 70% success).</p>"},{"location":"api/core/models/#ondine.core.models.QualityReport.has_issues","title":"has_issues  <code>property</code>","text":"<pre><code>has_issues: bool\n</code></pre> <p>Check if there are any issues.</p>"},{"location":"api/core/models/#ondine.core.models.WriteConfirmation","title":"WriteConfirmation  <code>dataclass</code>","text":"<pre><code>WriteConfirmation(path: str, rows_written: int, success: bool, timestamp: datetime = datetime.now(), metadata: dict[str, Any] = dict())\n</code></pre> <p>Confirmation of successful data write.</p>"},{"location":"api/core/models/#ondine.core.models.CheckpointInfo","title":"CheckpointInfo  <code>dataclass</code>","text":"<pre><code>CheckpointInfo(session_id: UUID, checkpoint_path: str, row_index: int, stage_index: int, timestamp: datetime, size_bytes: int)\n</code></pre> <p>Information about a checkpoint.</p>"},{"location":"api/core/models/#ondine.core.models.RowMetadata","title":"RowMetadata  <code>dataclass</code>","text":"<pre><code>RowMetadata(row_index: int, row_id: Any | None = None, batch_id: int | None = None, attempt: int = 1, custom: dict[str, Any] = dict())\n</code></pre> <p>Metadata for a single row during processing.</p>"},{"location":"api/core/models/#ondine.core.models.PromptBatch","title":"PromptBatch  <code>dataclass</code>","text":"<pre><code>PromptBatch(prompts: list[str], metadata: list[RowMetadata], batch_id: int)\n</code></pre> <p>Batch of prompts for processing.</p>"},{"location":"api/core/models/#ondine.core.models.ResponseBatch","title":"ResponseBatch  <code>dataclass</code>","text":"<pre><code>ResponseBatch(responses: list[str], metadata: list[RowMetadata], tokens_used: int, cost: Decimal, batch_id: int, latencies_ms: list[float] = list())\n</code></pre> <p>Batch of responses from LLM.</p>"},{"location":"api/core/specifications/","title":"specifications","text":""},{"location":"api/core/specifications/#ondine.core.specifications","title":"specifications","text":"<p>Core specification models for pipeline configuration.</p> <p>These Pydantic models define the configuration contracts for all pipeline components, following the principle of separation between configuration (what to do) and execution (how to do it).</p>"},{"location":"api/core/specifications/#ondine.core.specifications.DataSourceType","title":"DataSourceType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported data source types.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProvider","title":"LLMProvider","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported LLM providers.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.ErrorPolicy","title":"ErrorPolicy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Error handling policies for processing failures.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.MergeStrategy","title":"MergeStrategy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Output merge strategies.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.DatasetSpec","title":"DatasetSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for data source configuration.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.DatasetSpec.validate_source_path","title":"validate_source_path  <code>classmethod</code>","text":"<pre><code>validate_source_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"source_path\")\n@classmethod\ndef validate_source_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.DatasetSpec.validate_no_overlap","title":"validate_no_overlap  <code>classmethod</code>","text":"<pre><code>validate_no_overlap(v: list[str], info: Any) -&gt; list[str]\n</code></pre> <p>Ensure output columns don't overlap with input columns.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"output_columns\")\n@classmethod\ndef validate_no_overlap(cls, v: list[str], info: Any) -&gt; list[str]:\n    \"\"\"Ensure output columns don't overlap with input columns.\"\"\"\n    if \"input_columns\" in info.data:\n        input_cols = set(info.data[\"input_columns\"])\n        output_cols = set(v)\n        overlap = input_cols &amp; output_cols\n        if overlap:\n            raise ValueError(f\"Output columns overlap with input: {overlap}\")\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.PromptSpec","title":"PromptSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for prompt template configuration.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.PromptSpec.validate_template","title":"validate_template  <code>classmethod</code>","text":"<pre><code>validate_template(v: str) -&gt; str\n</code></pre> <p>Validate template has at least one variable.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"template\")\n@classmethod\ndef validate_template(cls, v: str) -&gt; str:\n    \"\"\"Validate template has at least one variable.\"\"\"\n    if \"{\" not in v or \"}\" not in v:\n        raise ValueError(\n            \"Template must contain at least one variable in {var} format\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.PromptSpec.validate_response_format","title":"validate_response_format  <code>classmethod</code>","text":"<pre><code>validate_response_format(v: str) -&gt; str\n</code></pre> <p>Validate response format is supported.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"response_format\")\n@classmethod\ndef validate_response_format(cls, v: str) -&gt; str:\n    \"\"\"Validate response format is supported.\"\"\"\n    allowed = [\"raw\", \"json\", \"regex\"]\n    if v not in allowed:\n        raise ValueError(f\"response_format must be one of {allowed}, got '{v}'\")\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec","title":"LLMSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for LLM provider configuration.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec.validate_base_url_format","title":"validate_base_url_format  <code>classmethod</code>","text":"<pre><code>validate_base_url_format(v: str | None) -&gt; str | None\n</code></pre> <p>Validate base_url is a valid HTTP(S) URL with a host.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"base_url\")\n@classmethod\ndef validate_base_url_format(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate base_url is a valid HTTP(S) URL with a host.\"\"\"\n    if v is None:\n        return v\n    from urllib.parse import urlparse\n\n    parsed = urlparse(v)\n    if parsed.scheme not in {\"http\", \"https\"}:\n        raise ValueError(\"base_url must start with http:// or https://\")\n    if not parsed.netloc:\n        raise ValueError(\n            \"base_url must include a host (e.g., localhost, api.example.com)\"\n        )\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec.validate_azure_config","title":"validate_azure_config  <code>classmethod</code>","text":"<pre><code>validate_azure_config(v: str | None, info: Any) -&gt; str | None\n</code></pre> <p>Validate Azure-specific configuration.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"azure_endpoint\", \"azure_deployment\")\n@classmethod\ndef validate_azure_config(cls, v: str | None, info: Any) -&gt; str | None:\n    \"\"\"Validate Azure-specific configuration.\"\"\"\n    if info.data.get(\"provider\") == LLMProvider.AZURE_OPENAI and v is None:\n        field_name = info.field_name\n        raise ValueError(f\"{field_name} required for Azure OpenAI provider\")\n    return v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMSpec.validate_provider_requirements","title":"validate_provider_requirements","text":"<pre><code>validate_provider_requirements() -&gt; LLMSpec\n</code></pre> <p>Validate provider-specific requirements.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    \"\"\"Validate provider-specific requirements.\"\"\"\n    # Check openai_compatible requires base_url\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.ProcessingSpec","title":"ProcessingSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for processing parameters.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.ProcessingSpec.validate_checkpoint_dir","title":"validate_checkpoint_dir  <code>classmethod</code>","text":"<pre><code>validate_checkpoint_dir(v: str | Path) -&gt; Path\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"checkpoint_dir\")\n@classmethod\ndef validate_checkpoint_dir(cls, v: str | Path) -&gt; Path:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.OutputSpec","title":"OutputSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for output configuration.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.OutputSpec.validate_destination_path","title":"validate_destination_path  <code>classmethod</code>","text":"<pre><code>validate_destination_path(v: str | Path | None) -&gt; Path | None\n</code></pre> <p>Convert string paths to Path objects.</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@field_validator(\"destination_path\")\n@classmethod\ndef validate_destination_path(cls, v: str | Path | None) -&gt; Path | None:\n    \"\"\"Convert string paths to Path objects.\"\"\"\n    if v is None:\n        return None\n    return Path(v) if isinstance(v, str) else v\n</code></pre>"},{"location":"api/core/specifications/#ondine.core.specifications.PipelineSpecifications","title":"PipelineSpecifications","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for all pipeline specifications.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets","title":"LLMProviderPresets","text":"<p>Pre-configured LLM provider specifications for common use cases.</p> <p>These presets provide convenient access to popular LLM providers with correct base URLs, pricing, and configuration. API keys must be provided at runtime via environment variables or explicit overrides.</p> Example Security Note <p>All presets have api_key=None by default. You must provide API keys at runtime via environment variables or explicit overrides.</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets--use-preset-with-env-var-api-key","title":"Use preset with env var API key","text":"<p>from ondine.core.specifications import LLMProviderPresets</p> <p>pipeline = (     PipelineBuilder.create()     .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])     .with_prompt(\"Process: {text}\")     .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)     .build() )</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets--override-api-key","title":"Override API key","text":"<p>spec = LLMProviderPresets.TOGETHER_AI_LLAMA_70B.model_copy(     update={\"api_key\": \"your-key\"}  # pragma: allowlist secret ) pipeline.with_llm_spec(spec)</p>"},{"location":"api/core/specifications/#ondine.core.specifications.LLMProviderPresets.create_custom_openai_compatible","title":"create_custom_openai_compatible  <code>classmethod</code>","text":"<pre><code>create_custom_openai_compatible(provider_name: str, model: str, base_url: str, input_cost_per_1k: float = 0.0, output_cost_per_1k: float = 0.0, **kwargs) -&gt; LLMSpec\n</code></pre> <p>Factory method for custom OpenAI-compatible providers.</p> <p>Use this for providers like vLLM, LocalAI, Anyscale, or any custom OpenAI-compatible API endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Display name for the provider (for logging/metrics)</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>base_url</code> <code>str</code> <p>API endpoint URL (e.g., http://localhost:8000/v1)</p> required <code>input_cost_per_1k</code> <code>float</code> <p>Input token cost per 1K tokens (default: 0.0)</p> <code>0.0</code> <code>output_cost_per_1k</code> <code>float</code> <p>Output token cost per 1K tokens (default: 0.0)</p> <code>0.0</code> <code>**kwargs</code> <p>Additional LLMSpec parameters (temperature, max_tokens, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMSpec</code> <p>Configured LLMSpec for the custom provider</p> Example <p>spec = LLMProviderPresets.create_custom_openai_compatible(     provider_name=\"My vLLM Server\",     model=\"mistral-7b-instruct\",     base_url=\"http://my-server:8000/v1\",     temperature=0.7 )</p> Source code in <code>ondine/core/specifications.py</code> <pre><code>@classmethod\ndef create_custom_openai_compatible(\n    cls,\n    provider_name: str,\n    model: str,\n    base_url: str,\n    input_cost_per_1k: float = 0.0,\n    output_cost_per_1k: float = 0.0,\n    **kwargs,\n) -&gt; LLMSpec:\n    \"\"\"\n    Factory method for custom OpenAI-compatible providers.\n\n    Use this for providers like vLLM, LocalAI, Anyscale, or any custom\n    OpenAI-compatible API endpoint.\n\n    Args:\n        provider_name: Display name for the provider (for logging/metrics)\n        model: Model identifier\n        base_url: API endpoint URL (e.g., http://localhost:8000/v1)\n        input_cost_per_1k: Input token cost per 1K tokens (default: 0.0)\n        output_cost_per_1k: Output token cost per 1K tokens (default: 0.0)\n        **kwargs: Additional LLMSpec parameters (temperature, max_tokens, etc.)\n\n    Returns:\n        Configured LLMSpec for the custom provider\n\n    Example:\n        spec = LLMProviderPresets.create_custom_openai_compatible(\n            provider_name=\"My vLLM Server\",\n            model=\"mistral-7b-instruct\",\n            base_url=\"http://my-server:8000/v1\",\n            temperature=0.7\n        )\n    \"\"\"\n    return LLMSpec(\n        provider=LLMProvider.OPENAI_COMPATIBLE,\n        provider_name=provider_name,\n        model=model,\n        base_url=base_url,\n        input_cost_per_1k_tokens=Decimal(str(input_cost_per_1k)),\n        output_cost_per_1k_tokens=Decimal(str(output_cost_per_1k)),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/integrations/","title":"integrations","text":""},{"location":"api/integrations/#ondine.integrations","title":"integrations","text":"<p>Framework integrations for popular orchestration tools.</p>"},{"location":"api/integrations/airflow/","title":"airflow","text":""},{"location":"api/integrations/airflow/#ondine.integrations.airflow","title":"airflow","text":"<p>Airflow integration - Pre-built operators for Apache Airflow.</p> <p>Provides LLMTransformOperator for easy integration into Airflow DAGs.</p>"},{"location":"api/integrations/airflow/#ondine.integrations.airflow.LLMTransformOperator","title":"LLMTransformOperator","text":"<pre><code>LLMTransformOperator(*args, **kwargs)\n</code></pre> <p>Placeholder when Airflow not installed.</p> Source code in <code>ondine/integrations/airflow.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    raise ImportError(\n        \"Apache Airflow is required to use LLMTransformOperator. \"\n        \"Install with: pip install apache-airflow\"\n    )\n</code></pre>"},{"location":"api/integrations/prefect/","title":"prefect","text":""},{"location":"api/integrations/prefect/#ondine.integrations.prefect","title":"prefect","text":"<p>Prefect integration - Pre-built tasks for Prefect workflows.</p> <p>Provides llm_transform_task for easy integration into Prefect flows.</p>"},{"location":"api/integrations/prefect/#ondine.integrations.prefect.llm_transform_task","title":"llm_transform_task","text":"<pre><code>llm_transform_task(config_path: str, input_data: DataFrame | None = None, input_file: str | None = None, output_file: str | None = None, max_budget: float | None = None, provider_override: str | None = None, model_override: str | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Prefect task for LLM dataset transformations.</p> <p>Integrates LLM Dataset Engine into Prefect flows.</p> Example <p>from prefect import flow from ondine.integrations.prefect import llm_transform_task</p> <p>@flow def data_pipeline():     raw_data = load_data()     enriched = llm_transform_task(         config_path='configs/llm_config.yaml',         input_data=raw_data,         max_budget=10.0,     )     save_data(enriched)</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to YAML/JSON configuration</p> required <code>input_data</code> <code>DataFrame | None</code> <p>Input DataFrame (from previous task)</p> <code>None</code> <code>input_file</code> <code>str | None</code> <p>Path to input file (alternative to input_data)</p> <code>None</code> <code>output_file</code> <code>str | None</code> <p>Path to output file (optional)</p> <code>None</code> <code>max_budget</code> <code>float | None</code> <p>Override maximum budget</p> <code>None</code> <code>provider_override</code> <code>str | None</code> <p>Override LLM provider</p> <code>None</code> <code>model_override</code> <code>str | None</code> <p>Override model name</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result DataFrame</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither input_data nor input_file provided</p> Source code in <code>ondine/integrations/prefect.py</code> <pre><code>@task(name=\"llm_transform\")\ndef llm_transform_task(\n    config_path: str,\n    input_data: pd.DataFrame | None = None,\n    input_file: str | None = None,\n    output_file: str | None = None,\n    max_budget: float | None = None,\n    provider_override: str | None = None,\n    model_override: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Prefect task for LLM dataset transformations.\n\n    Integrates LLM Dataset Engine into Prefect flows.\n\n    Example:\n        from prefect import flow\n        from ondine.integrations.prefect import llm_transform_task\n\n        @flow\n        def data_pipeline():\n            raw_data = load_data()\n            enriched = llm_transform_task(\n                config_path='configs/llm_config.yaml',\n                input_data=raw_data,\n                max_budget=10.0,\n            )\n            save_data(enriched)\n\n    Args:\n        config_path: Path to YAML/JSON configuration\n        input_data: Input DataFrame (from previous task)\n        input_file: Path to input file (alternative to input_data)\n        output_file: Path to output file (optional)\n        max_budget: Override maximum budget\n        provider_override: Override LLM provider\n        model_override: Override model name\n\n    Returns:\n        Result DataFrame\n\n    Raises:\n        ValueError: If neither input_data nor input_file provided\n    \"\"\"\n    if not PREFECT_AVAILABLE:\n        raise ImportError(\n            \"Prefect is required to use llm_transform_task. \"\n            \"Install with: pip install prefect\"\n        )\n\n    # Load configuration\n    specs = ConfigLoader.from_yaml(config_path)\n\n    # Override settings\n    if max_budget is not None:\n        from decimal import Decimal\n\n        specs.processing.max_budget = Decimal(str(max_budget))\n\n    if provider_override:\n        from ondine.core.specifications import LLMProvider\n\n        specs.llm.provider = LLMProvider(provider_override)\n\n    if model_override:\n        specs.llm.model = model_override\n\n    # Get input\n    if input_data is not None:\n        pipeline = Pipeline(specs, dataframe=input_data)\n    elif input_file:\n        specs.dataset.source_path = Path(input_file)\n        pipeline = Pipeline(specs)\n    else:\n        raise ValueError(\"Either input_data or input_file required\")\n\n    # Set output if specified\n    if output_file:\n        from ondine.core.specifications import (\n            DataSourceType,\n            MergeStrategy,\n            OutputSpec,\n        )\n\n        specs.output = OutputSpec(\n            destination_type=DataSourceType.CSV,\n            destination_path=Path(output_file),\n            merge_strategy=MergeStrategy.REPLACE,\n        )\n\n    # Execute\n    result = pipeline.execute()\n\n    # Log metrics (Prefect will capture)\n    print(f\"\u2705 Processed {result.metrics.total_rows} rows\")\n    print(f\"\ud83d\udcb0 Cost: ${result.costs.total_cost}\")\n    print(f\"\u23f1\ufe0f  Duration: {result.duration:.2f}s\")\n\n    return result.data\n</code></pre>"},{"location":"api/observability/","title":"observability","text":""},{"location":"api/observability/#ondine.observability","title":"observability","text":"<p>Observability toolkit for Ondine pipelines.</p> <p>Provides distributed tracing with OpenTelemetry for production debugging and performance monitoring.</p> Usage <p>from ondine.observability import enable_tracing, TracingObserver enable_tracing(exporter=\"jaeger\", endpoint=\"http://localhost:14268\")</p>"},{"location":"api/observability/#ondine.observability--traces-will-be-exported-to-jaeger","title":"Traces will be exported to Jaeger","text":""},{"location":"api/observability/#ondine.observability.TracingObserver","title":"TracingObserver","text":"<pre><code>TracingObserver(*args, **kwargs)\n</code></pre> <p>Placeholder when observability is not installed.</p> Source code in <code>ondine/observability/__init__.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    raise ImportError(\n        \"TracingObserver requires OpenTelemetry.\\n\"\n        \"Install with: pip install ondine[observability]\"\n    )\n</code></pre>"},{"location":"api/observability/#ondine.observability.enable_tracing","title":"enable_tracing","text":"<pre><code>enable_tracing(*args, **kwargs)\n</code></pre> <p>Placeholder when observability is not installed.</p> Source code in <code>ondine/observability/__init__.py</code> <pre><code>def enable_tracing(*args, **kwargs):\n    \"\"\"Placeholder when observability is not installed.\"\"\"\n    raise ImportError(\n        \"Observability features require OpenTelemetry.\\n\"\n        \"Install with: pip install ondine[observability]\\n\"\n        \"Or: pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-jaeger\"\n    )\n</code></pre>"},{"location":"api/observability/#ondine.observability.disable_tracing","title":"disable_tracing","text":"<pre><code>disable_tracing(*args, **kwargs)\n</code></pre> <p>Placeholder when observability is not installed.</p> Source code in <code>ondine/observability/__init__.py</code> <pre><code>def disable_tracing(*args, **kwargs):\n    \"\"\"Placeholder when observability is not installed.\"\"\"\n    raise ImportError(\n        \"Observability features require: pip install ondine[observability]\"\n    )\n</code></pre>"},{"location":"api/observability/#ondine.observability.is_tracing_enabled","title":"is_tracing_enabled","text":"<pre><code>is_tracing_enabled() -&gt; bool\n</code></pre> <p>Always returns False when observability is not installed.</p> Source code in <code>ondine/observability/__init__.py</code> <pre><code>def is_tracing_enabled() -&gt; bool:\n    \"\"\"Always returns False when observability is not installed.\"\"\"\n    return False\n</code></pre>"},{"location":"api/observability/observer/","title":"observer","text":""},{"location":"api/observability/observer/#ondine.observability.observer","title":"observer","text":"<p>Tracing observer for pipeline execution.</p> <p>Implements ExecutionObserver interface to create OpenTelemetry spans for pipeline and stage execution.</p>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver","title":"TracingObserver","text":"<pre><code>TracingObserver(include_prompts: bool = False)\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that creates OpenTelemetry spans for pipeline execution.</p> <p>Single Responsibility: Create and manage trace spans for observability.</p> <p>The observer creates a hierarchical span structure: - Root span for pipeline execution - Nested spans for each stage - Attributes include metrics, errors, and metadata</p> <p>Parameters:</p> Name Type Description Default <code>include_prompts</code> <code>bool</code> <p>If True, include prompts in spans (PII risk)</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ondine.observability import TracingObserver, enable_tracing\n&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n&gt;&gt;&gt; observer = TracingObserver(include_prompts=False)\n&gt;&gt;&gt; # Attach to pipeline execution\n</code></pre> <p>Initialize tracing observer.</p> Source code in <code>ondine/observability/observer.py</code> <pre><code>def __init__(self, include_prompts: bool = False):\n    \"\"\"Initialize tracing observer.\"\"\"\n    self._include_prompts = include_prompts\n    self._spans: dict[str, trace.Span] = {}  # Track active spans\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Create root span for pipeline execution.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Any</code> <p>Pipeline instance</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context with total rows, etc.</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"\n    Create root span for pipeline execution.\n\n    Args:\n        pipeline: Pipeline instance\n        context: Execution context with total rows, etc.\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    tracer = get_tracer()\n\n    # Create root span\n    span = tracer.start_span(\"pipeline.execute\")\n\n    # Add attributes\n    span.set_attribute(\"ondine.total_rows\", context.total_rows)\n    span.set_attribute(\"ondine.session_id\", context.session_id)\n\n    # Store span for later\n    self._spans[\"pipeline\"] = span\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Create span for stage execution.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>PipelineStage</code> <p>Pipeline stage being executed</p> required <code>context</code> <code>ExecutionContext</code> <p>Current execution context</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"\n    Create span for stage execution.\n\n    Args:\n        stage: Pipeline stage being executed\n        context: Current execution context\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    tracer = get_tracer()\n    stage_name = stage.__class__.__name__\n\n    # Create nested span under pipeline span\n    # Note: OpenTelemetry automatically handles span context\n    span = tracer.start_span(f\"stage.{stage_name}\")\n\n    # Add stage-specific attributes\n    span.set_attribute(\"ondine.stage\", stage_name)\n    span.set_attribute(\"ondine.processed_rows\", context.last_processed_row + 1)\n\n    # Add prompt if stage has it (and sanitize based on flag)\n    if hasattr(stage, \"prompt_template\") and stage.prompt_template:\n        from .sanitizer import sanitize_prompt\n\n        prompt_value = sanitize_prompt(\n            stage.prompt_template, include_prompts=self._include_prompts\n        )\n        span.set_attribute(\"ondine.prompt\", prompt_value)\n\n    # Store span for completion/error handling\n    self._spans[stage_name] = span\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Close stage span with success attributes.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>PipelineStage</code> <p>Pipeline stage that completed</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context after stage</p> required <code>result</code> <code>Any</code> <p>Stage execution result</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"\n    Close stage span with success attributes.\n\n    Args:\n        stage: Pipeline stage that completed\n        context: Execution context after stage\n        result: Stage execution result\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    stage_name = stage.__class__.__name__\n    span = self._spans.get(stage_name)\n\n    if span is not None:\n        # Mark as successful\n        span.set_status(Status(StatusCode.OK))\n\n        # Add completion metrics\n        span.set_attribute(\"ondine.rows_processed\", context.last_processed_row + 1)\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(stage_name, None)\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close stage span with error details.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>PipelineStage</code> <p>Pipeline stage that failed</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context at failure</p> required <code>error</code> <code>Exception</code> <p>Exception that occurred</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"\n    Close stage span with error details.\n\n    Args:\n        stage: Pipeline stage that failed\n        context: Execution context at failure\n        error: Exception that occurred\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    stage_name = stage.__class__.__name__\n    span = self._spans.get(stage_name)\n\n    if span is not None:\n        # Record exception\n        span.record_exception(error)\n\n        # Mark as error\n        span.set_status(Status(StatusCode.ERROR, str(error)))\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(stage_name, None)\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Close root span with final metrics.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Final execution context</p> required <code>result</code> <code>ExecutionResult</code> <p>Pipeline execution result</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"\n    Close root span with final metrics.\n\n    Args:\n        context: Final execution context\n        result: Pipeline execution result\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    span = self._spans.get(\"pipeline\")\n\n    if span is not None:\n        # Add final metrics\n        span.set_attribute(\"ondine.processed_rows\", result.metrics.processed_rows)\n        span.set_attribute(\"ondine.failed_rows\", result.metrics.failed_rows)\n        span.set_attribute(\n            \"ondine.duration_seconds\", result.metrics.total_duration_seconds\n        )\n        span.set_attribute(\"ondine.total_cost\", float(result.costs.total_cost))\n\n        # Mark as successful\n        span.set_status(Status(StatusCode.OK))\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(\"pipeline\", None)\n</code></pre>"},{"location":"api/observability/observer/#ondine.observability.observer.TracingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close root span with error.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Execution context at failure</p> required <code>error</code> <code>Exception</code> <p>Exception that occurred</p> required Source code in <code>ondine/observability/observer.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"\n    Close root span with error.\n\n    Args:\n        context: Execution context at failure\n        error: Exception that occurred\n    \"\"\"\n    if not is_tracing_enabled():\n        return\n\n    span = self._spans.get(\"pipeline\")\n\n    if span is not None:\n        # Record exception\n        span.record_exception(error)\n\n        # Mark as error\n        span.set_status(Status(StatusCode.ERROR, str(error)))\n\n        # End span\n        span.end()\n\n        # Remove from active spans\n        self._spans.pop(\"pipeline\", None)\n</code></pre>"},{"location":"api/observability/sanitizer/","title":"sanitizer","text":""},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer","title":"sanitizer","text":"<p>PII sanitization for trace attributes.</p> <p>Provides utilities to sanitize sensitive data in prompts and responses before including them in distributed traces.</p>"},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer.sanitize_prompt","title":"sanitize_prompt","text":"<pre><code>sanitize_prompt(prompt: str, include_prompts: bool = False) -&gt; str\n</code></pre> <p>Sanitize prompt text for tracing (PII-safe by default).</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The original prompt text</p> required <code>include_prompts</code> <code>bool</code> <p>If True, return original prompt (opt-in)</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized prompt (hash) or original if opted in</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sanitize_prompt(\"User email: test@example.com\")\n'&lt;sanitized-1234&gt;'\n&gt;&gt;&gt; sanitize_prompt(\"Test prompt\", include_prompts=True)\n'Test prompt'\n</code></pre> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_prompt(prompt: str, include_prompts: bool = False) -&gt; str:\n    \"\"\"\n    Sanitize prompt text for tracing (PII-safe by default).\n\n    Args:\n        prompt: The original prompt text\n        include_prompts: If True, return original prompt (opt-in)\n\n    Returns:\n        Sanitized prompt (hash) or original if opted in\n\n    Examples:\n        &gt;&gt;&gt; sanitize_prompt(\"User email: test@example.com\")\n        '&lt;sanitized-1234&gt;'\n        &gt;&gt;&gt; sanitize_prompt(\"Test prompt\", include_prompts=True)\n        'Test prompt'\n    \"\"\"\n    if include_prompts:\n        return prompt\n\n    # Return hash to detect duplicates without exposing content\n    # Use modulo to keep hash short and readable\n    hash_value = hash(prompt) % 10000\n    return f\"&lt;sanitized-{hash_value}&gt;\"\n</code></pre>"},{"location":"api/observability/sanitizer/#ondine.observability.sanitizer.sanitize_response","title":"sanitize_response","text":"<pre><code>sanitize_response(response: str, include_prompts: bool = False) -&gt; str\n</code></pre> <p>Sanitize LLM response text for tracing (PII-safe by default).</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>The original response text</p> required <code>include_prompts</code> <code>bool</code> <p>If True, return original response (opt-in)</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Sanitized response (hash) or original if opted in</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sanitize_response(\"SSN: 123-45-6789\")\n'&lt;sanitized-5678&gt;'\n&gt;&gt;&gt; sanitize_response(\"Safe response\", include_prompts=True)\n'Safe response'\n</code></pre> Source code in <code>ondine/observability/sanitizer.py</code> <pre><code>def sanitize_response(response: str, include_prompts: bool = False) -&gt; str:\n    \"\"\"\n    Sanitize LLM response text for tracing (PII-safe by default).\n\n    Args:\n        response: The original response text\n        include_prompts: If True, return original response (opt-in)\n\n    Returns:\n        Sanitized response (hash) or original if opted in\n\n    Examples:\n        &gt;&gt;&gt; sanitize_response(\"SSN: 123-45-6789\")\n        '&lt;sanitized-5678&gt;'\n        &gt;&gt;&gt; sanitize_response(\"Safe response\", include_prompts=True)\n        'Safe response'\n    \"\"\"\n    # Reuse same logic as prompt sanitization (DRY principle)\n    return sanitize_prompt(response, include_prompts=include_prompts)\n</code></pre>"},{"location":"api/observability/tracer/","title":"tracer","text":""},{"location":"api/observability/tracer/#ondine.observability.tracer","title":"tracer","text":"<p>OpenTelemetry tracer setup and management.</p> <p>Provides simple API for enabling/disabling distributed tracing with console or Jaeger exporters.</p>"},{"location":"api/observability/tracer/#ondine.observability.tracer.get_tracer","title":"get_tracer","text":"<pre><code>get_tracer() -&gt; trace.Tracer\n</code></pre> <p>Get current tracer instance (or no-op if disabled).</p> <p>Returns:</p> Type Description <code>Tracer</code> <p>Active tracer or no-op tracer if tracing is disabled</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tracer = get_tracer()\n&gt;&gt;&gt; with tracer.start_as_current_span(\"my-operation\"):\n...     # Do work\n...     pass\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def get_tracer() -&gt; trace.Tracer:\n    \"\"\"\n    Get current tracer instance (or no-op if disabled).\n\n    Returns:\n        Active tracer or no-op tracer if tracing is disabled\n\n    Examples:\n        &gt;&gt;&gt; tracer = get_tracer()\n        &gt;&gt;&gt; with tracer.start_as_current_span(\"my-operation\"):\n        ...     # Do work\n        ...     pass\n    \"\"\"\n    global _TRACER\n\n    if not _TRACING_ENABLED or _TRACER is None:\n        # Return no-op tracer that does nothing\n        return trace.get_tracer(__name__)\n\n    return _TRACER\n</code></pre>"},{"location":"api/observability/tracer/#ondine.observability.tracer.enable_tracing","title":"enable_tracing","text":"<pre><code>enable_tracing(exporter: str = 'console', endpoint: str | None = None, service_name: str = 'ondine-pipeline') -&gt; None\n</code></pre> <p>Enable distributed tracing (opt-in).</p> <p>Parameters:</p> Name Type Description Default <code>exporter</code> <code>str</code> <p>Exporter type (\"console\" or \"jaeger\")</p> <code>'console'</code> <code>endpoint</code> <code>str | None</code> <p>Jaeger endpoint (required if exporter=\"jaeger\")</p> <code>None</code> <code>service_name</code> <code>str</code> <p>Service name for traces</p> <code>'ondine-pipeline'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Console exporter (for development)\n&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Jaeger exporter (for production)\n&gt;&gt;&gt; enable_tracing(\n...     exporter=\"jaeger\",\n...     endpoint=\"http://localhost:14268/api/traces\"\n... )\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def enable_tracing(\n    exporter: str = \"console\",\n    endpoint: str | None = None,\n    service_name: str = \"ondine-pipeline\",\n) -&gt; None:\n    \"\"\"\n    Enable distributed tracing (opt-in).\n\n    Args:\n        exporter: Exporter type (\"console\" or \"jaeger\")\n        endpoint: Jaeger endpoint (required if exporter=\"jaeger\")\n        service_name: Service name for traces\n\n    Examples:\n        &gt;&gt;&gt; # Console exporter (for development)\n        &gt;&gt;&gt; enable_tracing(exporter=\"console\")\n\n        &gt;&gt;&gt; # Jaeger exporter (for production)\n        &gt;&gt;&gt; enable_tracing(\n        ...     exporter=\"jaeger\",\n        ...     endpoint=\"http://localhost:14268/api/traces\"\n        ... )\n    \"\"\"\n    global _TRACING_ENABLED, _TRACER_PROVIDER, _TRACER\n\n    # Make idempotent - clean up existing tracing if already enabled\n    if _TRACING_ENABLED:\n        disable_tracing()\n\n    # Create resource with service name\n    resource = Resource.create(attributes={\"service.name\": service_name})\n\n    # Create tracer provider\n    provider = TracerProvider(resource=resource)\n\n    # Configure exporter\n    if exporter == \"console\":\n        span_exporter = ConsoleSpanExporter()\n    elif exporter == \"jaeger\":\n        if endpoint is None:\n            raise ValueError(\"endpoint is required for Jaeger exporter\")\n        span_exporter = JaegerExporter(\n            collector_endpoint=endpoint,\n        )\n    else:\n        raise ValueError(f\"Unknown exporter: {exporter}. Use 'console' or 'jaeger'\")\n\n    # Add span processor (gracefully handle export failures)\n    try:\n        processor = BatchSpanProcessor(span_exporter)\n        provider.add_span_processor(processor)\n    except Exception as e:\n        # Don't break pipeline if exporter setup fails\n        import logging\n\n        logger = logging.getLogger(__name__)\n        logger.warning(f\"Failed to setup span processor: {e}. Tracing will be disabled\")\n        return\n\n    # Set global provider\n    trace.set_tracer_provider(provider)\n\n    # Store module state\n    _TRACER_PROVIDER = provider\n    _TRACER = trace.get_tracer(__name__)\n    _TRACING_ENABLED = True\n</code></pre>"},{"location":"api/observability/tracer/#ondine.observability.tracer.disable_tracing","title":"disable_tracing","text":"<pre><code>disable_tracing() -&gt; None\n</code></pre> <p>Disable tracing and cleanup resources.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n&gt;&gt;&gt; # ... do work ...\n&gt;&gt;&gt; disable_tracing()\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def disable_tracing() -&gt; None:\n    \"\"\"\n    Disable tracing and cleanup resources.\n\n    Examples:\n        &gt;&gt;&gt; enable_tracing(exporter=\"console\")\n        &gt;&gt;&gt; # ... do work ...\n        &gt;&gt;&gt; disable_tracing()\n    \"\"\"\n    global _TRACING_ENABLED, _TRACER_PROVIDER, _TRACER\n\n    if _TRACER_PROVIDER is not None:\n        # Flush any pending spans\n        try:\n            _TRACER_PROVIDER.shutdown()\n        except Exception:  # nosec B110\n            # Silently ignore shutdown errors - tracing cleanup is non-critical\n            # Prevents exceptions during cleanup from breaking application shutdown\n            pass\n\n    _TRACING_ENABLED = False\n    _TRACER_PROVIDER = None\n    _TRACER = None\n</code></pre>"},{"location":"api/observability/tracer/#ondine.observability.tracer.is_tracing_enabled","title":"is_tracing_enabled","text":"<pre><code>is_tracing_enabled() -&gt; bool\n</code></pre> <p>Check if tracing is currently enabled.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if tracing is enabled, False otherwise</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_tracing_enabled()\nFalse\n&gt;&gt;&gt; enable_tracing(exporter=\"console\")\n&gt;&gt;&gt; is_tracing_enabled()\nTrue\n</code></pre> Source code in <code>ondine/observability/tracer.py</code> <pre><code>def is_tracing_enabled() -&gt; bool:\n    \"\"\"\n    Check if tracing is currently enabled.\n\n    Returns:\n        True if tracing is enabled, False otherwise\n\n    Examples:\n        &gt;&gt;&gt; is_tracing_enabled()\n        False\n        &gt;&gt;&gt; enable_tracing(exporter=\"console\")\n        &gt;&gt;&gt; is_tracing_enabled()\n        True\n    \"\"\"\n    return _TRACING_ENABLED\n</code></pre>"},{"location":"api/orchestration/","title":"orchestration","text":""},{"location":"api/orchestration/#ondine.orchestration","title":"orchestration","text":"<p>Orchestration engine for pipeline execution control.</p>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor","title":"AsyncExecutor","text":"<pre><code>AsyncExecutor(max_concurrency: int = 10)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Asynchronous execution strategy.</p> <p>Uses asyncio for true non-blocking execution. Leverages LlamaIndex's async methods (acomplete) for concurrent LLM calls without threads.</p> <p>Benefits: - Non-blocking (works with FastAPI, aiohttp) - Better resource utilization - Higher concurrency without thread overhead - Ideal for I/O-bound operations</p> <p>Initialize async executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def __init__(self, max_concurrency: int = 10):\n    \"\"\"\n    Initialize async executor.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n    \"\"\"\n    self.max_concurrency = max_concurrency\n    self.logger = logger\n    self.semaphore = asyncio.Semaphore(max_concurrency)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages asynchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages with async/await\n        result_data = await self._execute_stages_async(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=0,\n            output_tokens=0,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Async pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Async executor supports async.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Async executor supports async.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Async executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Async executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.AsyncExecutor.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute_async(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"Alias for execute() method.\"\"\"\n    return await self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext","title":"ExecutionContext  <code>dataclass</code>","text":"<pre><code>ExecutionContext(session_id: UUID = uuid4(), pipeline_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, current_stage_index: int = 0, last_processed_row: int = 0, total_rows: int = 0, accumulated_cost: Decimal = (lambda: Decimal('0.0'))(), accumulated_tokens: int = 0, intermediate_data: dict[str, Any] = dict(), failed_rows: int = 0, skipped_rows: int = 0, observers: list[ExecutionObserver] = list())\n</code></pre> <p>Lightweight orchestration state (passed between pipeline stages).</p> <p>Scope: Runtime execution state and progress tracking Pattern: Memento (serializable for checkpointing)</p> <p>Cost Tracking in ExecutionContext: - Simple accumulation for orchestration purposes - Used by: Executors to track overall progress - NOT for: Detailed accounting (use CostTracker for that)</p> <p>Why separate from CostTracker? - ExecutionContext = orchestration state (stage progress, session ID, timing) - CostTracker = detailed accounting (per-stage breakdowns, thread-safe entries, metrics) - Different concerns, different use cases</p> <p>ExecutionContext is: - Passed between stages in the pipeline - Serialized for checkpointing - Focused on execution orchestration</p> <p>CostTracker is: - Used within LLMInvocationStage for detailed tracking - Thread-safe for concurrent operations - Focused on cost reporting and analytics</p> <p>See Also: - CostTracker: For detailed cost accounting with breakdowns - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Carries shared state between stages and tracks progress. Immutable for most fields to prevent accidental modification.</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.update_stage","title":"update_stage","text":"<pre><code>update_stage(stage_index: int) -&gt; None\n</code></pre> <p>Update current stage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_stage(self, stage_index: int) -&gt; None:\n    \"\"\"Update current stage.\"\"\"\n    self.current_stage_index = stage_index\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.update_row","title":"update_row","text":"<pre><code>update_row(row_index: int) -&gt; None\n</code></pre> <p>Update last processed row.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_row(self, row_index: int) -&gt; None:\n    \"\"\"Update last processed row.\"\"\"\n    self.last_processed_row = row_index\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.add_cost","title":"add_cost","text":"<pre><code>add_cost(cost: Decimal, tokens: int) -&gt; None\n</code></pre> <p>Add cost and token usage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def add_cost(self, cost: Decimal, tokens: int) -&gt; None:\n    \"\"\"Add cost and token usage.\"\"\"\n    self.accumulated_cost += cost\n    self.accumulated_tokens += tokens\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.notify_progress","title":"notify_progress","text":"<pre><code>notify_progress() -&gt; None\n</code></pre> <p>Notify all observers of progress update.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def notify_progress(self) -&gt; None:\n    \"\"\"Notify all observers of progress update.\"\"\"\n    for observer in self.observers:\n        try:\n            observer.on_progress_update(self)\n        except Exception:  # nosec B110\n            # Silently ignore observer errors to not break pipeline\n            # Observers are non-critical, pipeline should continue even if they fail\n            pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.get_progress","title":"get_progress","text":"<pre><code>get_progress() -&gt; float\n</code></pre> <p>Get completion percentage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_progress(self) -&gt; float:\n    \"\"\"Get completion percentage.\"\"\"\n    if self.total_rows == 0:\n        return 0.0\n    return (self.last_processed_row / self.total_rows) * 100\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; ProcessingStats\n</code></pre> <p>Get processing statistics.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_stats(self) -&gt; ProcessingStats:\n    \"\"\"Get processing statistics.\"\"\"\n    duration = (\n        (datetime.now() - self.start_time).total_seconds()\n        if self.end_time is None\n        else (self.end_time - self.start_time).total_seconds()\n    )\n\n    # last_processed_row is 0-based index, so add 1 for count\n    actual_processed = (\n        self.last_processed_row + 1 if self.last_processed_row &gt;= 0 else 0\n    )\n\n    rows_per_second = actual_processed / duration if duration &gt; 0 else 0.0\n\n    return ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=actual_processed,\n        failed_rows=self.failed_rows,\n        skipped_rows=self.skipped_rows,\n        rows_per_second=rows_per_second,\n        total_duration_seconds=duration,\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.to_checkpoint","title":"to_checkpoint","text":"<pre><code>to_checkpoint() -&gt; dict[str, Any]\n</code></pre> <p>Serialize to checkpoint dictionary (Memento pattern).</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation for persistence</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_checkpoint(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize to checkpoint dictionary (Memento pattern).\n\n    Returns:\n        Dictionary representation for persistence\n    \"\"\"\n    return {\n        \"session_id\": str(self.session_id),\n        \"pipeline_id\": str(self.pipeline_id),\n        \"start_time\": self.start_time.isoformat(),\n        \"end_time\": self.end_time.isoformat() if self.end_time else None,\n        \"current_stage_index\": self.current_stage_index,\n        \"last_processed_row\": self.last_processed_row,\n        \"total_rows\": self.total_rows,\n        \"accumulated_cost\": str(self.accumulated_cost),\n        \"accumulated_tokens\": self.accumulated_tokens,\n        \"intermediate_data\": self.intermediate_data,\n        \"failed_rows\": self.failed_rows,\n        \"skipped_rows\": self.skipped_rows,\n    }\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.from_checkpoint","title":"from_checkpoint  <code>classmethod</code>","text":"<pre><code>from_checkpoint(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Deserialize from checkpoint dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data</p> required <p>Returns:</p> Type Description <code>ExecutionContext</code> <p>Restored ExecutionContext</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_checkpoint(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"\n    Deserialize from checkpoint dictionary.\n\n    Args:\n        data: Checkpoint data\n\n    Returns:\n        Restored ExecutionContext\n    \"\"\"\n    return cls(\n        session_id=UUID(data[\"session_id\"]),\n        pipeline_id=UUID(data[\"pipeline_id\"]),\n        start_time=datetime.fromisoformat(data[\"start_time\"]),\n        end_time=(\n            datetime.fromisoformat(data[\"end_time\"])\n            if data.get(\"end_time\")\n            else None\n        ),\n        current_stage_index=data[\"current_stage_index\"],\n        last_processed_row=data[\"last_processed_row\"],\n        total_rows=data[\"total_rows\"],\n        accumulated_cost=Decimal(data[\"accumulated_cost\"]),\n        accumulated_tokens=data[\"accumulated_tokens\"],\n        intermediate_data=data.get(\"intermediate_data\", {}),\n        failed_rows=data.get(\"failed_rows\", 0),\n        skipped_rows=data.get(\"skipped_rows\", 0),\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Alias for to_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Alias for to_checkpoint().\"\"\"\n    return self.to_checkpoint()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionContext.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Alias for from_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"Alias for from_checkpoint().\"\"\"\n    return cls.from_checkpoint(data)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy","title":"ExecutionStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution strategies.</p> <p>Follows Strategy pattern: defines interface for executing pipeline stages in different modes (sync, async, streaming).</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name for logging.</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.execute","title":"execute  <code>abstractmethod</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]\n</code></pre> <p>Execute pipeline stages.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>List of pipeline stages to execute</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context for state management</p> required <p>Returns:</p> Type Description <code>ExecutionResult | Iterator[DataFrame] | AsyncIterator[DataFrame]</code> <p>ExecutionResult or iterator for streaming</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]:\n    \"\"\"\n    Execute pipeline stages.\n\n    Args:\n        stages: List of pipeline stages to execute\n        context: Execution context for state management\n\n    Returns:\n        ExecutionResult or iterator for streaming\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.supports_async","title":"supports_async  <code>abstractmethod</code>","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Whether this strategy supports async execution.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_async(self) -&gt; bool:\n    \"\"\"Whether this strategy supports async execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionStrategy.supports_streaming","title":"supports_streaming  <code>abstractmethod</code>","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Whether this strategy supports streaming.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_streaming(self) -&gt; bool:\n    \"\"\"Whether this strategy supports streaming.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver","title":"CostTrackingObserver","text":"<pre><code>CostTrackingObserver(warning_threshold: float = 0.75)\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that tracks and warns about costs.</p> <p>Initialize cost tracking observer.</p> <p>Parameters:</p> Name Type Description Default <code>warning_threshold</code> <code>float</code> <p>Warn when this fraction of budget used</p> <code>0.75</code> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self, warning_threshold: float = 0.75):\n    \"\"\"\n    Initialize cost tracking observer.\n\n    Args:\n        warning_threshold: Warn when this fraction of budget used\n    \"\"\"\n    self.logger = get_logger(__name__)\n    self.warning_threshold = warning_threshold\n    self.max_budget: float | None = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Set max budget if available.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Set max budget if available.\"\"\"\n    # Could extract from pipeline specs\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on stage start.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Check cost after stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Check cost after stage completion.\"\"\"\n    if self.max_budget:\n        usage_ratio = float(context.accumulated_cost) / self.max_budget\n\n        if usage_ratio &gt;= self.warning_threshold:\n            self.logger.warning(\n                f\"Cost warning: {usage_ratio * 100:.1f}% of budget used \"\n                f\"(${context.accumulated_cost:.4f} / ${self.max_budget:.2f})\"\n            )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>No action on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"No action on error.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log final cost summary.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log final cost summary.\"\"\"\n    self.logger.info(\n        f\"Cost summary:\\n\"\n        f\"  Total: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Input tokens: {result.costs.input_tokens:,}\\n\"\n        f\"  Output tokens: {result.costs.output_tokens:,}\\n\"\n        f\"  Cost per row: ${float(result.costs.total_cost) / result.metrics.total_rows:.6f}\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log cost at failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log cost at failure.\"\"\"\n    self.logger.info(f\"Cost at failure: ${context.accumulated_cost:.4f}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.CostTrackingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on progress update for cost tracking.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on progress update for cost tracking.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver","title":"ExecutionObserver","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution observers.</p> <p>Observers can monitor pipeline execution without coupling to the executor implementation.</p>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_pipeline_start","title":"on_pipeline_start  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before first stage execution.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before first stage execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_stage_start","title":"on_stage_start  <code>abstractmethod</code>","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before each stage.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before each stage.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_stage_complete","title":"on_stage_complete  <code>abstractmethod</code>","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Called after successful stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Called after successful stage completion.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_stage_error","title":"on_stage_error  <code>abstractmethod</code>","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on stage failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Called on stage failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_pipeline_complete","title":"on_pipeline_complete  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Called after all stages complete.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Called after all stages complete.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_pipeline_error","title":"on_pipeline_error  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on fatal pipeline failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Called on fatal pipeline failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ExecutionObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Called periodically during execution for progress updates.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Called periodically during execution for progress updates.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver","title":"LoggingObserver","text":"<pre><code>LoggingObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that logs execution events.</p> <p>Initialize logging observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize logging observer.\"\"\"\n    self.logger = get_logger(__name__)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log pipeline start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Log pipeline start.\"\"\"\n    self.logger.info(f\"Pipeline execution started (session: {context.session_id})\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Log stage start.\"\"\"\n    self.logger.info(\n        f\"Starting stage: {stage.name} (progress: {context.get_progress():.1f}%)\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Log stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Log stage completion.\"\"\"\n    self.logger.info(\n        f\"Completed stage: {stage.name} (cost: ${context.accumulated_cost:.4f})\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log stage error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Log stage error.\"\"\"\n    self.logger.error(f\"Stage {stage.name} failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log pipeline completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log pipeline completion.\"\"\"\n    self.logger.info(\n        f\"Pipeline execution completed successfully\\n\"\n        f\"  Processed: {result.metrics.processed_rows} rows\\n\"\n        f\"  Duration: {result.metrics.total_duration_seconds:.2f}s\\n\"\n        f\"  Total cost: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Errors: {result.metrics.failed_rows}\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log pipeline error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log pipeline error.\"\"\"\n    self.logger.error(f\"Pipeline execution failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.LoggingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Log progress update.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Log progress update.\"\"\"\n    # Make progress very visible with separators\n    self.logger.info(\n        f\"\u2501\u2501\u2501\u2501\u2501\u2501 PROGRESS: {context.last_processed_row}/{context.total_rows} rows \"\n        f\"({context.get_progress():.1f}%) | Cost: ${context.accumulated_cost:.4f} \u2501\u2501\u2501\u2501\u2501\u2501\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver","title":"ProgressBarObserver","text":"<pre><code>ProgressBarObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that displays progress bar with tqdm.</p> <p>Initialize progress bar observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize progress bar observer.\"\"\"\n    self.progress_bar: tqdm | None = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Initialize progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Initialize progress bar.\"\"\"\n    if context.total_rows &gt; 0:\n        self.progress_bar = tqdm(\n            total=context.total_rows,\n            desc=\"Processing\",\n            unit=\"rows\",\n        )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar description.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar description.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Stage: {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Update progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Update progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Handle error in progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Handle error in progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Error in {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Close progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Close progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close progress bar on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Close progress bar on error.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.ProgressBarObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar with current row count.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar with current row count.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.set_postfix(\n            {\n                \"cost\": f\"${context.accumulated_cost:.4f}\",\n                \"progress\": f\"{context.get_progress():.1f}%\",\n            }\n        )\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager","title":"StateManager","text":"<pre><code>StateManager(storage: CheckpointStorage, checkpoint_interval: int = 500)\n</code></pre> <p>Manages execution state persistence and recovery.</p> <p>Follows Single Responsibility: only handles state management. Uses Strategy pattern for pluggable storage backends.</p> <p>Initialize state manager.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>CheckpointStorage</code> <p>Checkpoint storage backend</p> required <code>checkpoint_interval</code> <code>int</code> <p>Rows between checkpoints</p> <code>500</code> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def __init__(self, storage: CheckpointStorage, checkpoint_interval: int = 500):\n    \"\"\"\n    Initialize state manager.\n\n    Args:\n        storage: Checkpoint storage backend\n        checkpoint_interval: Rows between checkpoints\n    \"\"\"\n    self.storage = storage\n    self.checkpoint_interval = checkpoint_interval\n    self._last_checkpoint_row = 0\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.should_checkpoint","title":"should_checkpoint","text":"<pre><code>should_checkpoint(current_row: int) -&gt; bool\n</code></pre> <p>Check if checkpoint should be saved.</p> <p>Parameters:</p> Name Type Description Default <code>current_row</code> <code>int</code> <p>Current row index</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint due</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def should_checkpoint(self, current_row: int) -&gt; bool:\n    \"\"\"\n    Check if checkpoint should be saved.\n\n    Args:\n        current_row: Current row index\n\n    Returns:\n        True if checkpoint due\n    \"\"\"\n    return (current_row - self._last_checkpoint_row) &gt;= self.checkpoint_interval\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(context: ExecutionContext) -&gt; bool\n</code></pre> <p>Save checkpoint for execution context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Execution context to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def save_checkpoint(self, context: ExecutionContext) -&gt; bool:\n    \"\"\"\n    Save checkpoint for execution context.\n\n    Args:\n        context: Execution context to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        checkpoint_data = context.to_checkpoint()\n        success = self.storage.save(context.session_id, checkpoint_data)\n\n        if success:\n            self._last_checkpoint_row = context.last_processed_row\n            logger.info(f\"Checkpoint saved at row {context.last_processed_row}\")\n\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to save checkpoint: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(session_id: UUID) -&gt; ExecutionContext | None\n</code></pre> <p>Load checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>ExecutionContext | None</code> <p>Restored execution context or None</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def load_checkpoint(self, session_id: UUID) -&gt; ExecutionContext | None:\n    \"\"\"\n    Load checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Restored execution context or None\n    \"\"\"\n    try:\n        checkpoint_data = self.storage.load(session_id)\n\n        if checkpoint_data is None:\n            return None\n\n        context = ExecutionContext.from_checkpoint(checkpoint_data)\n        logger.info(f\"Checkpoint loaded from row {context.last_processed_row}\")\n\n        return context\n    except Exception as e:\n        logger.error(f\"Failed to load checkpoint: {e}\")\n        return None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.can_resume","title":"can_resume","text":"<pre><code>can_resume(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if session can be resumed.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint exists</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def can_resume(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if session can be resumed.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if checkpoint exists\n    \"\"\"\n    return self.storage.exists(session_id)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.cleanup_checkpoints","title":"cleanup_checkpoints","text":"<pre><code>cleanup_checkpoints(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoints for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def cleanup_checkpoints(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoints for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    try:\n        success = self.storage.delete(session_id)\n        if success:\n            logger.info(f\"Checkpoints cleaned up for session {session_id}\")\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to cleanup checkpoints: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StateManager.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    return self.storage.list_checkpoints()\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor","title":"StreamingExecutor","text":"<pre><code>StreamingExecutor(chunk_size: int = 1000)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Streaming execution strategy.</p> <p>Processes data in chunks to maintain constant memory usage. Ideal for very large datasets (100K+ rows) that don't fit in memory.</p> <p>Benefits: - Constant memory footprint - Can process unlimited dataset sizes - Checkpoints at chunk boundaries - Early results available</p> <p>Initialize streaming executor.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self, chunk_size: int = 1000):\n    \"\"\"\n    Initialize streaming executor.\n\n    Args:\n        chunk_size: Number of rows per chunk\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Execute stages in streaming mode.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrames with processed chunks</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Execute stages in streaming mode.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Yields:\n        DataFrames with processed chunks\n    \"\"\"\n    self.logger.info(f\"Starting streaming execution (chunk_size={self.chunk_size})\")\n\n    # Get data loader stage\n    data_loader = stages[0]\n\n    # Stream data in chunks\n    chunk_index = 0\n    total_rows_processed = 0\n\n    # Read data in chunks\n    for chunk in self._read_chunks(data_loader, context):\n        self.logger.info(f\"Processing chunk {chunk_index} ({len(chunk)} rows)\")\n\n        # Process chunk through remaining stages\n        result_chunk = self._process_chunk(chunk, stages[1:], context)\n\n        # Update context\n        total_rows_processed += len(result_chunk)\n        context.update_row(total_rows_processed - 1)\n\n        # Yield result\n        yield result_chunk\n\n        chunk_index += 1\n\n    self.logger.info(\n        f\"Streaming execution complete: {total_rows_processed} rows, \"\n        f\"{chunk_index} chunks\"\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Streaming executor doesn't support async.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Streaming executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Streaming executor supports streaming.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Streaming executor supports streaming.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingExecutor.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute_stream(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Alias for execute() method.\"\"\"\n    return self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingResult","title":"StreamingResult","text":"<pre><code>StreamingResult()\n</code></pre> <p>Result container for streaming execution.</p> <p>Provides access to metrics after consuming the stream.</p> <p>Initialize streaming result.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize streaming result.\"\"\"\n    self.chunks_processed = 0\n    self.total_rows = 0\n    self.total_cost = Decimal(\"0.0\")\n    self.start_time = datetime.now()\n    self.end_time = None\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingResult.add_chunk","title":"add_chunk","text":"<pre><code>add_chunk(chunk: DataFrame, cost: Decimal)\n</code></pre> <p>Add chunk statistics.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def add_chunk(self, chunk: pd.DataFrame, cost: Decimal):\n    \"\"\"Add chunk statistics.\"\"\"\n    self.chunks_processed += 1\n    self.total_rows += len(chunk)\n    self.total_cost += cost\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.StreamingResult.finalize","title":"finalize","text":"<pre><code>finalize() -&gt; ExecutionResult\n</code></pre> <p>Create final ExecutionResult.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def finalize(self) -&gt; ExecutionResult:\n    \"\"\"Create final ExecutionResult.\"\"\"\n    self.end_time = datetime.now()\n    duration = (self.end_time - self.start_time).total_seconds()\n\n    stats = ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=self.total_rows,\n        failed_rows=0,\n        skipped_rows=0,\n        rows_per_second=self.total_rows / duration if duration &gt; 0 else 0,\n        total_duration_seconds=duration,\n    )\n\n    costs = CostEstimate(\n        total_cost=self.total_cost,\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=self.total_rows,\n        confidence=\"actual\",\n    )\n\n    return ExecutionResult(\n        data=pd.DataFrame(),  # Streaming doesn't return full data\n        metrics=stats,\n        costs=costs,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        success=True,\n    )\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor","title":"SyncExecutor","text":"<pre><code>SyncExecutor()\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Synchronous execution strategy.</p> <p>Uses ThreadPoolExecutor for concurrent LLM calls while maintaining sequential stage execution. This is the default strategy that preserves current behavior.</p> <p>Initialize synchronous executor.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize synchronous executor.\"\"\"\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages synchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages sequentially\n        result_data = self._execute_stages(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=0,  # Would need to track separately\n            output_tokens=0,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Sync executor doesn't support async.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/#ondine.orchestration.SyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Sync executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/async_executor/","title":"async_executor","text":""},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor","title":"async_executor","text":"<p>Asynchronous execution strategy.</p> <p>Provides async/await support for non-blocking execution, ideal for integration with FastAPI, aiohttp, and other async frameworks.</p>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor","title":"AsyncExecutor","text":"<pre><code>AsyncExecutor(max_concurrency: int = 10)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Asynchronous execution strategy.</p> <p>Uses asyncio for true non-blocking execution. Leverages LlamaIndex's async methods (acomplete) for concurrent LLM calls without threads.</p> <p>Benefits: - Non-blocking (works with FastAPI, aiohttp) - Better resource utilization - Higher concurrency without thread overhead - Ideal for I/O-bound operations</p> <p>Initialize async executor.</p> <p>Parameters:</p> Name Type Description Default <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent async tasks</p> <code>10</code> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def __init__(self, max_concurrency: int = 10):\n    \"\"\"\n    Initialize async executor.\n\n    Args:\n        max_concurrency: Maximum concurrent async tasks\n    \"\"\"\n    self.max_concurrency = max_concurrency\n    self.logger = logger\n    self.semaphore = asyncio.Semaphore(max_concurrency)\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages asynchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages with async/await\n        result_data = await self._execute_stages_async(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=0,\n            output_tokens=0,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Async pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Async executor supports async.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Async executor supports async.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Async executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Async executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/async_executor/#ondine.orchestration.async_executor.AsyncExecutor.execute_async","title":"execute_async  <code>async</code>","text":"<pre><code>execute_async(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/async_executor.py</code> <pre><code>async def execute_async(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"Alias for execute() method.\"\"\"\n    return await self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/execution_context/","title":"execution_context","text":""},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context","title":"execution_context","text":"<p>Execution context for carrying runtime state between stages.</p> <p>Implements Memento pattern for checkpoint serialization.</p>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext","title":"ExecutionContext  <code>dataclass</code>","text":"<pre><code>ExecutionContext(session_id: UUID = uuid4(), pipeline_id: UUID = uuid4(), start_time: datetime = datetime.now(), end_time: datetime | None = None, current_stage_index: int = 0, last_processed_row: int = 0, total_rows: int = 0, accumulated_cost: Decimal = (lambda: Decimal('0.0'))(), accumulated_tokens: int = 0, intermediate_data: dict[str, Any] = dict(), failed_rows: int = 0, skipped_rows: int = 0, observers: list[ExecutionObserver] = list())\n</code></pre> <p>Lightweight orchestration state (passed between pipeline stages).</p> <p>Scope: Runtime execution state and progress tracking Pattern: Memento (serializable for checkpointing)</p> <p>Cost Tracking in ExecutionContext: - Simple accumulation for orchestration purposes - Used by: Executors to track overall progress - NOT for: Detailed accounting (use CostTracker for that)</p> <p>Why separate from CostTracker? - ExecutionContext = orchestration state (stage progress, session ID, timing) - CostTracker = detailed accounting (per-stage breakdowns, thread-safe entries, metrics) - Different concerns, different use cases</p> <p>ExecutionContext is: - Passed between stages in the pipeline - Serialized for checkpointing - Focused on execution orchestration</p> <p>CostTracker is: - Used within LLMInvocationStage for detailed tracking - Thread-safe for concurrent operations - Focused on cost reporting and analytics</p> <p>See Also: - CostTracker: For detailed cost accounting with breakdowns - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Carries shared state between stages and tracks progress. Immutable for most fields to prevent accidental modification.</p>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.update_stage","title":"update_stage","text":"<pre><code>update_stage(stage_index: int) -&gt; None\n</code></pre> <p>Update current stage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_stage(self, stage_index: int) -&gt; None:\n    \"\"\"Update current stage.\"\"\"\n    self.current_stage_index = stage_index\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.update_row","title":"update_row","text":"<pre><code>update_row(row_index: int) -&gt; None\n</code></pre> <p>Update last processed row.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def update_row(self, row_index: int) -&gt; None:\n    \"\"\"Update last processed row.\"\"\"\n    self.last_processed_row = row_index\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.add_cost","title":"add_cost","text":"<pre><code>add_cost(cost: Decimal, tokens: int) -&gt; None\n</code></pre> <p>Add cost and token usage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def add_cost(self, cost: Decimal, tokens: int) -&gt; None:\n    \"\"\"Add cost and token usage.\"\"\"\n    self.accumulated_cost += cost\n    self.accumulated_tokens += tokens\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.notify_progress","title":"notify_progress","text":"<pre><code>notify_progress() -&gt; None\n</code></pre> <p>Notify all observers of progress update.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def notify_progress(self) -&gt; None:\n    \"\"\"Notify all observers of progress update.\"\"\"\n    for observer in self.observers:\n        try:\n            observer.on_progress_update(self)\n        except Exception:  # nosec B110\n            # Silently ignore observer errors to not break pipeline\n            # Observers are non-critical, pipeline should continue even if they fail\n            pass\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.get_progress","title":"get_progress","text":"<pre><code>get_progress() -&gt; float\n</code></pre> <p>Get completion percentage.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_progress(self) -&gt; float:\n    \"\"\"Get completion percentage.\"\"\"\n    if self.total_rows == 0:\n        return 0.0\n    return (self.last_processed_row / self.total_rows) * 100\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; ProcessingStats\n</code></pre> <p>Get processing statistics.</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def get_stats(self) -&gt; ProcessingStats:\n    \"\"\"Get processing statistics.\"\"\"\n    duration = (\n        (datetime.now() - self.start_time).total_seconds()\n        if self.end_time is None\n        else (self.end_time - self.start_time).total_seconds()\n    )\n\n    # last_processed_row is 0-based index, so add 1 for count\n    actual_processed = (\n        self.last_processed_row + 1 if self.last_processed_row &gt;= 0 else 0\n    )\n\n    rows_per_second = actual_processed / duration if duration &gt; 0 else 0.0\n\n    return ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=actual_processed,\n        failed_rows=self.failed_rows,\n        skipped_rows=self.skipped_rows,\n        rows_per_second=rows_per_second,\n        total_duration_seconds=duration,\n    )\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.to_checkpoint","title":"to_checkpoint","text":"<pre><code>to_checkpoint() -&gt; dict[str, Any]\n</code></pre> <p>Serialize to checkpoint dictionary (Memento pattern).</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation for persistence</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_checkpoint(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Serialize to checkpoint dictionary (Memento pattern).\n\n    Returns:\n        Dictionary representation for persistence\n    \"\"\"\n    return {\n        \"session_id\": str(self.session_id),\n        \"pipeline_id\": str(self.pipeline_id),\n        \"start_time\": self.start_time.isoformat(),\n        \"end_time\": self.end_time.isoformat() if self.end_time else None,\n        \"current_stage_index\": self.current_stage_index,\n        \"last_processed_row\": self.last_processed_row,\n        \"total_rows\": self.total_rows,\n        \"accumulated_cost\": str(self.accumulated_cost),\n        \"accumulated_tokens\": self.accumulated_tokens,\n        \"intermediate_data\": self.intermediate_data,\n        \"failed_rows\": self.failed_rows,\n        \"skipped_rows\": self.skipped_rows,\n    }\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.from_checkpoint","title":"from_checkpoint  <code>classmethod</code>","text":"<pre><code>from_checkpoint(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Deserialize from checkpoint dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Checkpoint data</p> required <p>Returns:</p> Type Description <code>ExecutionContext</code> <p>Restored ExecutionContext</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_checkpoint(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"\n    Deserialize from checkpoint dictionary.\n\n    Args:\n        data: Checkpoint data\n\n    Returns:\n        Restored ExecutionContext\n    \"\"\"\n    return cls(\n        session_id=UUID(data[\"session_id\"]),\n        pipeline_id=UUID(data[\"pipeline_id\"]),\n        start_time=datetime.fromisoformat(data[\"start_time\"]),\n        end_time=(\n            datetime.fromisoformat(data[\"end_time\"])\n            if data.get(\"end_time\")\n            else None\n        ),\n        current_stage_index=data[\"current_stage_index\"],\n        last_processed_row=data[\"last_processed_row\"],\n        total_rows=data[\"total_rows\"],\n        accumulated_cost=Decimal(data[\"accumulated_cost\"]),\n        accumulated_tokens=data[\"accumulated_tokens\"],\n        intermediate_data=data.get(\"intermediate_data\", {}),\n        failed_rows=data.get(\"failed_rows\", 0),\n        skipped_rows=data.get(\"skipped_rows\", 0),\n    )\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Alias for to_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Alias for to_checkpoint().\"\"\"\n    return self.to_checkpoint()\n</code></pre>"},{"location":"api/orchestration/execution_context/#ondine.orchestration.execution_context.ExecutionContext.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; ExecutionContext\n</code></pre> <p>Alias for from_checkpoint().</p> Source code in <code>ondine/orchestration/execution_context.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict[str, Any]) -&gt; \"ExecutionContext\":\n    \"\"\"Alias for from_checkpoint().\"\"\"\n    return cls.from_checkpoint(data)\n</code></pre>"},{"location":"api/orchestration/execution_strategy/","title":"execution_strategy","text":""},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy","title":"execution_strategy","text":"<p>Execution strategy abstraction for different execution modes.</p> <p>Implements Strategy pattern to support sync, async, and streaming execution without modifying core pipeline logic.</p>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy","title":"ExecutionStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution strategies.</p> <p>Follows Strategy pattern: defines interface for executing pipeline stages in different modes (sync, async, streaming).</p>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name for logging.</p>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.execute","title":"execute  <code>abstractmethod</code>","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]\n</code></pre> <p>Execute pipeline stages.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>List of pipeline stages to execute</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context for state management</p> required <p>Returns:</p> Type Description <code>ExecutionResult | Iterator[DataFrame] | AsyncIterator[DataFrame]</code> <p>ExecutionResult or iterator for streaming</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult | Iterator[pd.DataFrame] | AsyncIterator[pd.DataFrame]:\n    \"\"\"\n    Execute pipeline stages.\n\n    Args:\n        stages: List of pipeline stages to execute\n        context: Execution context for state management\n\n    Returns:\n        ExecutionResult or iterator for streaming\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.supports_async","title":"supports_async  <code>abstractmethod</code>","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Whether this strategy supports async execution.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_async(self) -&gt; bool:\n    \"\"\"Whether this strategy supports async execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/execution_strategy/#ondine.orchestration.execution_strategy.ExecutionStrategy.supports_streaming","title":"supports_streaming  <code>abstractmethod</code>","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Whether this strategy supports streaming.</p> Source code in <code>ondine/orchestration/execution_strategy.py</code> <pre><code>@abstractmethod\ndef supports_streaming(self) -&gt; bool:\n    \"\"\"Whether this strategy supports streaming.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/","title":"observers","text":""},{"location":"api/orchestration/observers/#ondine.orchestration.observers","title":"observers","text":"<p>Execution observers for monitoring and logging.</p> <p>Implements Observer pattern for decoupled event notification.</p>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver","title":"ExecutionObserver","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for execution observers.</p> <p>Observers can monitor pipeline execution without coupling to the executor implementation.</p>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_pipeline_start","title":"on_pipeline_start  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before first stage execution.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before first stage execution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_stage_start","title":"on_stage_start  <code>abstractmethod</code>","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Called before each stage.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Called before each stage.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_stage_complete","title":"on_stage_complete  <code>abstractmethod</code>","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Called after successful stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Called after successful stage completion.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_stage_error","title":"on_stage_error  <code>abstractmethod</code>","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on stage failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Called on stage failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_pipeline_complete","title":"on_pipeline_complete  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Called after all stages complete.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Called after all stages complete.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_pipeline_error","title":"on_pipeline_error  <code>abstractmethod</code>","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Called on fatal pipeline failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>@abstractmethod\ndef on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Called on fatal pipeline failure.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ExecutionObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Called periodically during execution for progress updates.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Called periodically during execution for progress updates.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver","title":"ProgressBarObserver","text":"<pre><code>ProgressBarObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that displays progress bar with tqdm.</p> <p>Initialize progress bar observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize progress bar observer.\"\"\"\n    self.progress_bar: tqdm | None = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Initialize progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Initialize progress bar.\"\"\"\n    if context.total_rows &gt; 0:\n        self.progress_bar = tqdm(\n            total=context.total_rows,\n            desc=\"Processing\",\n            unit=\"rows\",\n        )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar description.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar description.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Stage: {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Update progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Update progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Handle error in progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Handle error in progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.set_description(f\"Error in {stage.name}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Close progress bar.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Close progress bar.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Close progress bar on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Close progress bar on error.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.close()\n        self.progress_bar = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.ProgressBarObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Update progress bar with current row count.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Update progress bar with current row count.\"\"\"\n    if self.progress_bar:\n        self.progress_bar.n = context.last_processed_row\n        self.progress_bar.set_postfix(\n            {\n                \"cost\": f\"${context.accumulated_cost:.4f}\",\n                \"progress\": f\"{context.get_progress():.1f}%\",\n            }\n        )\n        self.progress_bar.refresh()\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver","title":"LoggingObserver","text":"<pre><code>LoggingObserver()\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that logs execution events.</p> <p>Initialize logging observer.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize logging observer.\"\"\"\n    self.logger = get_logger(__name__)\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log pipeline start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Log pipeline start.\"\"\"\n    self.logger.info(f\"Pipeline execution started (session: {context.session_id})\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>Log stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"Log stage start.\"\"\"\n    self.logger.info(\n        f\"Starting stage: {stage.name} (progress: {context.get_progress():.1f}%)\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Log stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Log stage completion.\"\"\"\n    self.logger.info(\n        f\"Completed stage: {stage.name} (cost: ${context.accumulated_cost:.4f})\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log stage error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"Log stage error.\"\"\"\n    self.logger.error(f\"Stage {stage.name} failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log pipeline completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log pipeline completion.\"\"\"\n    self.logger.info(\n        f\"Pipeline execution completed successfully\\n\"\n        f\"  Processed: {result.metrics.processed_rows} rows\\n\"\n        f\"  Duration: {result.metrics.total_duration_seconds:.2f}s\\n\"\n        f\"  Total cost: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Errors: {result.metrics.failed_rows}\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log pipeline error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log pipeline error.\"\"\"\n    self.logger.error(f\"Pipeline execution failed: {error}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.LoggingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>Log progress update.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"Log progress update.\"\"\"\n    # Make progress very visible with separators\n    self.logger.info(\n        f\"\u2501\u2501\u2501\u2501\u2501\u2501 PROGRESS: {context.last_processed_row}/{context.total_rows} rows \"\n        f\"({context.get_progress():.1f}%) | Cost: ${context.accumulated_cost:.4f} \u2501\u2501\u2501\u2501\u2501\u2501\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver","title":"CostTrackingObserver","text":"<pre><code>CostTrackingObserver(warning_threshold: float = 0.75)\n</code></pre> <p>               Bases: <code>ExecutionObserver</code></p> <p>Observer that tracks and warns about costs.</p> <p>Initialize cost tracking observer.</p> <p>Parameters:</p> Name Type Description Default <code>warning_threshold</code> <code>float</code> <p>Warn when this fraction of budget used</p> <code>0.75</code> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def __init__(self, warning_threshold: float = 0.75):\n    \"\"\"\n    Initialize cost tracking observer.\n\n    Args:\n        warning_threshold: Warn when this fraction of budget used\n    \"\"\"\n    self.logger = get_logger(__name__)\n    self.warning_threshold = warning_threshold\n    self.max_budget: float | None = None\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_pipeline_start","title":"on_pipeline_start","text":"<pre><code>on_pipeline_start(pipeline: Any, context: ExecutionContext) -&gt; None\n</code></pre> <p>Set max budget if available.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_start(self, pipeline: Any, context: ExecutionContext) -&gt; None:\n    \"\"\"Set max budget if available.\"\"\"\n    # Could extract from pipeline specs\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_stage_start","title":"on_stage_start","text":"<pre><code>on_stage_start(stage: PipelineStage, context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on stage start.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_start(self, stage: PipelineStage, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on stage start.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_stage_complete","title":"on_stage_complete","text":"<pre><code>on_stage_complete(stage: PipelineStage, context: ExecutionContext, result: Any) -&gt; None\n</code></pre> <p>Check cost after stage completion.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_complete(\n    self, stage: PipelineStage, context: ExecutionContext, result: Any\n) -&gt; None:\n    \"\"\"Check cost after stage completion.\"\"\"\n    if self.max_budget:\n        usage_ratio = float(context.accumulated_cost) / self.max_budget\n\n        if usage_ratio &gt;= self.warning_threshold:\n            self.logger.warning(\n                f\"Cost warning: {usage_ratio * 100:.1f}% of budget used \"\n                f\"(${context.accumulated_cost:.4f} / ${self.max_budget:.2f})\"\n            )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_stage_error","title":"on_stage_error","text":"<pre><code>on_stage_error(stage: PipelineStage, context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>No action on error.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_stage_error(\n    self, stage: PipelineStage, context: ExecutionContext, error: Exception\n) -&gt; None:\n    \"\"\"No action on error.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_pipeline_complete","title":"on_pipeline_complete","text":"<pre><code>on_pipeline_complete(context: ExecutionContext, result: ExecutionResult) -&gt; None\n</code></pre> <p>Log final cost summary.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_complete(\n    self, context: ExecutionContext, result: ExecutionResult\n) -&gt; None:\n    \"\"\"Log final cost summary.\"\"\"\n    self.logger.info(\n        f\"Cost summary:\\n\"\n        f\"  Total: ${result.costs.total_cost:.4f}\\n\"\n        f\"  Input tokens: {result.costs.input_tokens:,}\\n\"\n        f\"  Output tokens: {result.costs.output_tokens:,}\\n\"\n        f\"  Cost per row: ${float(result.costs.total_cost) / result.metrics.total_rows:.6f}\"\n    )\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_pipeline_error","title":"on_pipeline_error","text":"<pre><code>on_pipeline_error(context: ExecutionContext, error: Exception) -&gt; None\n</code></pre> <p>Log cost at failure.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_pipeline_error(self, context: ExecutionContext, error: Exception) -&gt; None:\n    \"\"\"Log cost at failure.\"\"\"\n    self.logger.info(f\"Cost at failure: ${context.accumulated_cost:.4f}\")\n</code></pre>"},{"location":"api/orchestration/observers/#ondine.orchestration.observers.CostTrackingObserver.on_progress_update","title":"on_progress_update","text":"<pre><code>on_progress_update(context: ExecutionContext) -&gt; None\n</code></pre> <p>No action on progress update for cost tracking.</p> Source code in <code>ondine/orchestration/observers.py</code> <pre><code>def on_progress_update(self, context: ExecutionContext) -&gt; None:\n    \"\"\"No action on progress update for cost tracking.\"\"\"\n    pass\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/","title":"pipeline_executor","text":""},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor","title":"pipeline_executor","text":"<p>Pipeline executor for orchestrating stage execution.</p> <p>Implements the complete execution flow with state machine management as specified in the design document.</p>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.ExecutionState","title":"ExecutionState","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Pipeline execution states.</p>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor","title":"PipelineExecutor","text":"<pre><code>PipelineExecutor(stages: list[PipelineStage], state_manager: StateManager, observers: list[ExecutionObserver] | None = None)\n</code></pre> <p>Orchestrates pipeline execution with state management.</p> <p>Implements Command and Mediator patterns for coordinating stages, observers, and state management.</p> State Machine <p>IDLE \u2192 INITIALIZING \u2192 EXECUTING \u2192 [PAUSED \u2194 EXECUTING] \u2192 COMPLETED                          \u2193                       FAILED</p> <p>Initialize pipeline executor.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Ordered list of processing stages</p> required <code>state_manager</code> <code>StateManager</code> <p>State manager for checkpointing</p> required <code>observers</code> <code>list[ExecutionObserver] | None</code> <p>Optional execution observers</p> <code>None</code> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def __init__(\n    self,\n    stages: list[PipelineStage],\n    state_manager: StateManager,\n    observers: list[ExecutionObserver] | None = None,\n):\n    \"\"\"\n    Initialize pipeline executor.\n\n    Args:\n        stages: Ordered list of processing stages\n        state_manager: State manager for checkpointing\n        observers: Optional execution observers\n    \"\"\"\n    self.execution_id = uuid4()\n    self.stages = stages\n    self.state_manager = state_manager\n    self.observers = observers or []\n    self.state = ExecutionState.IDLE\n    self.context: ExecutionContext | None = None\n    self.logger = get_logger(f\"{__name__}.{self.execution_id}\")\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.add_observer","title":"add_observer","text":"<pre><code>add_observer(observer: ExecutionObserver) -&gt; PipelineExecutor\n</code></pre> <p>Add execution observer.</p> <p>Parameters:</p> Name Type Description Default <code>observer</code> <code>ExecutionObserver</code> <p>Observer to add</p> required <p>Returns:</p> Type Description <code>PipelineExecutor</code> <p>Self for chaining</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def add_observer(self, observer: ExecutionObserver) -&gt; \"PipelineExecutor\":\n    \"\"\"\n    Add execution observer.\n\n    Args:\n        observer: Observer to add\n\n    Returns:\n        Self for chaining\n    \"\"\"\n    self.observers.append(observer)\n    return self\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.execute","title":"execute","text":"<pre><code>execute(pipeline: Any) -&gt; ExecutionResult\n</code></pre> <p>Execute pipeline end-to-end.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Any</code> <p>Pipeline instance to execute</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If pipeline in invalid state</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def execute(self, pipeline: Any) -&gt; ExecutionResult:\n    \"\"\"\n    Execute pipeline end-to-end.\n\n    Args:\n        pipeline: Pipeline instance to execute\n\n    Returns:\n        ExecutionResult with data and metrics\n\n    Raises:\n        RuntimeError: If pipeline in invalid state\n    \"\"\"\n    if self.state not in [ExecutionState.IDLE, ExecutionState.FAILED]:\n        raise RuntimeError(f\"Cannot execute from state: {self.state}\")\n\n    try:\n        # Initialize\n        self.state = ExecutionState.INITIALIZING\n        self.context = self._initialize_context()\n\n        # Check for existing checkpoint\n        if self.state_manager.can_resume(self.context.session_id):\n            self.logger.info(\"Found existing checkpoint, resuming...\")\n            self.context = self.state_manager.load_checkpoint(\n                self.context.session_id\n            )\n\n        # Notify observers\n        self._notify_pipeline_start(pipeline)\n\n        # Execute stages\n        self.state = ExecutionState.EXECUTING\n        result_data = self._execute_all_stages(pipeline)\n\n        # Mark completion\n        self.state = ExecutionState.COMPLETED\n        self.context.end_time = datetime.now()\n\n        # Create result\n        result = self._create_execution_result(result_data)\n\n        # Cleanup checkpoints\n        self.state_manager.cleanup_checkpoints(self.context.session_id)\n\n        # Notify observers\n        self._notify_pipeline_complete(result)\n\n        return result\n\n    except Exception as e:\n        self.state = ExecutionState.FAILED\n        self._notify_pipeline_error(e)\n\n        # Save checkpoint on failure\n        if self.context:\n            self.state_manager.save_checkpoint(self.context)\n\n        raise\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.pause","title":"pause","text":"<pre><code>pause() -&gt; None\n</code></pre> <p>Gracefully pause execution.</p> <p>Finishes current batch and saves checkpoint.</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def pause(self) -&gt; None:\n    \"\"\"\n    Gracefully pause execution.\n\n    Finishes current batch and saves checkpoint.\n    \"\"\"\n    if self.state != ExecutionState.EXECUTING:\n        raise RuntimeError(f\"Cannot pause from state: {self.state}\")\n\n    self.logger.info(\"Pausing execution...\")\n    self.state = ExecutionState.PAUSED\n\n    # Save checkpoint\n    if self.context:\n        self.state_manager.save_checkpoint(self.context)\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.resume","title":"resume","text":"<pre><code>resume(session_id: UUID) -&gt; ExecutionResult\n</code></pre> <p>Resume from saved checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session ID to resume</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no checkpoint found</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def resume(self, session_id: UUID) -&gt; ExecutionResult:\n    \"\"\"\n    Resume from saved checkpoint.\n\n    Args:\n        session_id: Session ID to resume\n\n    Returns:\n        ExecutionResult\n\n    Raises:\n        ValueError: If no checkpoint found\n    \"\"\"\n    if not self.state_manager.can_resume(session_id):\n        raise ValueError(f\"No checkpoint found for session {session_id}\")\n\n    self.context = self.state_manager.load_checkpoint(session_id)\n    if not self.context:\n        raise ValueError(\"Failed to load checkpoint\")\n\n    self.logger.info(f\"Resuming from row {self.context.last_processed_row}\")\n\n    # Continue execution\n    # Note: Would need to reconstruct pipeline and skip processed rows\n    raise NotImplementedError(\"Resume functionality coming soon\")\n</code></pre>"},{"location":"api/orchestration/pipeline_executor/#ondine.orchestration.pipeline_executor.PipelineExecutor.cancel","title":"cancel","text":"<pre><code>cancel() -&gt; None\n</code></pre> <p>Immediately stop and save checkpoint.</p> Source code in <code>ondine/orchestration/pipeline_executor.py</code> <pre><code>def cancel(self) -&gt; None:\n    \"\"\"\n    Immediately stop and save checkpoint.\n    \"\"\"\n    self.logger.info(\"Cancelling execution...\")\n\n    # Save checkpoint\n    if self.context:\n        self.state_manager.save_checkpoint(self.context)\n\n    self.state = ExecutionState.IDLE\n</code></pre>"},{"location":"api/orchestration/state_manager/","title":"state_manager","text":""},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager","title":"state_manager","text":"<p>State management for checkpointing and recovery.</p>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager","title":"StateManager","text":"<pre><code>StateManager(storage: CheckpointStorage, checkpoint_interval: int = 500)\n</code></pre> <p>Manages execution state persistence and recovery.</p> <p>Follows Single Responsibility: only handles state management. Uses Strategy pattern for pluggable storage backends.</p> <p>Initialize state manager.</p> <p>Parameters:</p> Name Type Description Default <code>storage</code> <code>CheckpointStorage</code> <p>Checkpoint storage backend</p> required <code>checkpoint_interval</code> <code>int</code> <p>Rows between checkpoints</p> <code>500</code> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def __init__(self, storage: CheckpointStorage, checkpoint_interval: int = 500):\n    \"\"\"\n    Initialize state manager.\n\n    Args:\n        storage: Checkpoint storage backend\n        checkpoint_interval: Rows between checkpoints\n    \"\"\"\n    self.storage = storage\n    self.checkpoint_interval = checkpoint_interval\n    self._last_checkpoint_row = 0\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.should_checkpoint","title":"should_checkpoint","text":"<pre><code>should_checkpoint(current_row: int) -&gt; bool\n</code></pre> <p>Check if checkpoint should be saved.</p> <p>Parameters:</p> Name Type Description Default <code>current_row</code> <code>int</code> <p>Current row index</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint due</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def should_checkpoint(self, current_row: int) -&gt; bool:\n    \"\"\"\n    Check if checkpoint should be saved.\n\n    Args:\n        current_row: Current row index\n\n    Returns:\n        True if checkpoint due\n    \"\"\"\n    return (current_row - self._last_checkpoint_row) &gt;= self.checkpoint_interval\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(context: ExecutionContext) -&gt; bool\n</code></pre> <p>Save checkpoint for execution context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ExecutionContext</code> <p>Execution context to save</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def save_checkpoint(self, context: ExecutionContext) -&gt; bool:\n    \"\"\"\n    Save checkpoint for execution context.\n\n    Args:\n        context: Execution context to save\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        checkpoint_data = context.to_checkpoint()\n        success = self.storage.save(context.session_id, checkpoint_data)\n\n        if success:\n            self._last_checkpoint_row = context.last_processed_row\n            logger.info(f\"Checkpoint saved at row {context.last_processed_row}\")\n\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to save checkpoint: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(session_id: UUID) -&gt; ExecutionContext | None\n</code></pre> <p>Load checkpoint for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>ExecutionContext | None</code> <p>Restored execution context or None</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def load_checkpoint(self, session_id: UUID) -&gt; ExecutionContext | None:\n    \"\"\"\n    Load checkpoint for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        Restored execution context or None\n    \"\"\"\n    try:\n        checkpoint_data = self.storage.load(session_id)\n\n        if checkpoint_data is None:\n            return None\n\n        context = ExecutionContext.from_checkpoint(checkpoint_data)\n        logger.info(f\"Checkpoint loaded from row {context.last_processed_row}\")\n\n        return context\n    except Exception as e:\n        logger.error(f\"Failed to load checkpoint: {e}\")\n        return None\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.can_resume","title":"can_resume","text":"<pre><code>can_resume(session_id: UUID) -&gt; bool\n</code></pre> <p>Check if session can be resumed.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint exists</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def can_resume(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Check if session can be resumed.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if checkpoint exists\n    \"\"\"\n    return self.storage.exists(session_id)\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.cleanup_checkpoints","title":"cleanup_checkpoints","text":"<pre><code>cleanup_checkpoints(session_id: UUID) -&gt; bool\n</code></pre> <p>Delete checkpoints for session.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>UUID</code> <p>Session identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def cleanup_checkpoints(self, session_id: UUID) -&gt; bool:\n    \"\"\"\n    Delete checkpoints for session.\n\n    Args:\n        session_id: Session identifier\n\n    Returns:\n        True if deleted\n    \"\"\"\n    try:\n        success = self.storage.delete(session_id)\n        if success:\n            logger.info(f\"Checkpoints cleaned up for session {session_id}\")\n        return success\n    except Exception as e:\n        logger.error(f\"Failed to cleanup checkpoints: {e}\")\n        return False\n</code></pre>"},{"location":"api/orchestration/state_manager/#ondine.orchestration.state_manager.StateManager.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; list[CheckpointInfo]\n</code></pre> <p>List all available checkpoints.</p> <p>Returns:</p> Type Description <code>list[CheckpointInfo]</code> <p>List of checkpoint information</p> Source code in <code>ondine/orchestration/state_manager.py</code> <pre><code>def list_checkpoints(self) -&gt; list[CheckpointInfo]:\n    \"\"\"\n    List all available checkpoints.\n\n    Returns:\n        List of checkpoint information\n    \"\"\"\n    return self.storage.list_checkpoints()\n</code></pre>"},{"location":"api/orchestration/streaming_executor/","title":"streaming_executor","text":""},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor","title":"streaming_executor","text":"<p>Streaming execution strategy.</p> <p>Provides memory-efficient processing for large datasets by processing data in chunks.</p>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor","title":"StreamingExecutor","text":"<pre><code>StreamingExecutor(chunk_size: int = 1000)\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Streaming execution strategy.</p> <p>Processes data in chunks to maintain constant memory usage. Ideal for very large datasets (100K+ rows) that don't fit in memory.</p> <p>Benefits: - Constant memory footprint - Can process unlimited dataset sizes - Checkpoints at chunk boundaries - Early results available</p> <p>Initialize streaming executor.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self, chunk_size: int = 1000):\n    \"\"\"\n    Initialize streaming executor.\n\n    Args:\n        chunk_size: Number of rows per chunk\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Execute stages in streaming mode.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Yields:</p> Type Description <code>DataFrame</code> <p>DataFrames with processed chunks</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Execute stages in streaming mode.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Yields:\n        DataFrames with processed chunks\n    \"\"\"\n    self.logger.info(f\"Starting streaming execution (chunk_size={self.chunk_size})\")\n\n    # Get data loader stage\n    data_loader = stages[0]\n\n    # Stream data in chunks\n    chunk_index = 0\n    total_rows_processed = 0\n\n    # Read data in chunks\n    for chunk in self._read_chunks(data_loader, context):\n        self.logger.info(f\"Processing chunk {chunk_index} ({len(chunk)} rows)\")\n\n        # Process chunk through remaining stages\n        result_chunk = self._process_chunk(chunk, stages[1:], context)\n\n        # Update context\n        total_rows_processed += len(result_chunk)\n        context.update_row(total_rows_processed - 1)\n\n        # Yield result\n        yield result_chunk\n\n        chunk_index += 1\n\n    self.logger.info(\n        f\"Streaming execution complete: {total_rows_processed} rows, \"\n        f\"{chunk_index} chunks\"\n    )\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Streaming executor doesn't support async.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Streaming executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Streaming executor supports streaming.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Streaming executor supports streaming.\"\"\"\n    return True\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingExecutor.execute_stream","title":"execute_stream","text":"<pre><code>execute_stream(stages: list[PipelineStage], context: ExecutionContext) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Alias for execute() method.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def execute_stream(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Alias for execute() method.\"\"\"\n    return self.execute(stages, context)\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingResult","title":"StreamingResult","text":"<pre><code>StreamingResult()\n</code></pre> <p>Result container for streaming execution.</p> <p>Provides access to metrics after consuming the stream.</p> <p>Initialize streaming result.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize streaming result.\"\"\"\n    self.chunks_processed = 0\n    self.total_rows = 0\n    self.total_cost = Decimal(\"0.0\")\n    self.start_time = datetime.now()\n    self.end_time = None\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingResult.add_chunk","title":"add_chunk","text":"<pre><code>add_chunk(chunk: DataFrame, cost: Decimal)\n</code></pre> <p>Add chunk statistics.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def add_chunk(self, chunk: pd.DataFrame, cost: Decimal):\n    \"\"\"Add chunk statistics.\"\"\"\n    self.chunks_processed += 1\n    self.total_rows += len(chunk)\n    self.total_cost += cost\n</code></pre>"},{"location":"api/orchestration/streaming_executor/#ondine.orchestration.streaming_executor.StreamingResult.finalize","title":"finalize","text":"<pre><code>finalize() -&gt; ExecutionResult\n</code></pre> <p>Create final ExecutionResult.</p> Source code in <code>ondine/orchestration/streaming_executor.py</code> <pre><code>def finalize(self) -&gt; ExecutionResult:\n    \"\"\"Create final ExecutionResult.\"\"\"\n    self.end_time = datetime.now()\n    duration = (self.end_time - self.start_time).total_seconds()\n\n    stats = ProcessingStats(\n        total_rows=self.total_rows,\n        processed_rows=self.total_rows,\n        failed_rows=0,\n        skipped_rows=0,\n        rows_per_second=self.total_rows / duration if duration &gt; 0 else 0,\n        total_duration_seconds=duration,\n    )\n\n    costs = CostEstimate(\n        total_cost=self.total_cost,\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=self.total_rows,\n        confidence=\"actual\",\n    )\n\n    return ExecutionResult(\n        data=pd.DataFrame(),  # Streaming doesn't return full data\n        metrics=stats,\n        costs=costs,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        success=True,\n    )\n</code></pre>"},{"location":"api/orchestration/sync_executor/","title":"sync_executor","text":""},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor","title":"sync_executor","text":"<p>Synchronous execution strategy.</p> <p>Default executor that maintains current behavior using ThreadPoolExecutor for concurrent LLM calls.</p>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor","title":"SyncExecutor","text":"<pre><code>SyncExecutor()\n</code></pre> <p>               Bases: <code>ExecutionStrategy</code></p> <p>Synchronous execution strategy.</p> <p>Uses ThreadPoolExecutor for concurrent LLM calls while maintaining sequential stage execution. This is the default strategy that preserves current behavior.</p> <p>Initialize synchronous executor.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize synchronous executor.\"\"\"\n    self.logger = logger\n</code></pre>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Strategy name.</p>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.execute","title":"execute","text":"<pre><code>execute(stages: list[PipelineStage], context: ExecutionContext) -&gt; ExecutionResult\n</code></pre> <p>Execute stages synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>stages</code> <code>list[PipelineStage]</code> <p>Pipeline stages</p> required <code>context</code> <code>ExecutionContext</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>ExecutionResult</code> <p>ExecutionResult with data and metrics</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def execute(\n    self,\n    stages: list[PipelineStage],\n    context: ExecutionContext,\n) -&gt; ExecutionResult:\n    \"\"\"\n    Execute stages synchronously.\n\n    Args:\n        stages: Pipeline stages\n        context: Execution context\n\n    Returns:\n        ExecutionResult with data and metrics\n    \"\"\"\n    start_time = datetime.now()\n\n    try:\n        # Execute stages sequentially\n        result_data = self._execute_stages(stages, context)\n\n        end_time = datetime.now()\n        duration = (end_time - start_time).total_seconds()\n\n        # Calculate stats\n        stats = ProcessingStats(\n            total_rows=context.total_rows,\n            processed_rows=context.last_processed_row + 1,\n            failed_rows=context.total_rows - (context.last_processed_row + 1),\n            skipped_rows=0,\n            rows_per_second=context.total_rows / duration if duration &gt; 0 else 0,\n            total_duration_seconds=duration,\n        )\n\n        # Get cost estimate\n        cost_estimate = CostEstimate(\n            total_cost=context.accumulated_cost,\n            total_tokens=context.accumulated_tokens,\n            input_tokens=0,  # Would need to track separately\n            output_tokens=0,\n            rows=context.total_rows,\n            confidence=\"actual\",\n        )\n\n        return ExecutionResult(\n            data=result_data,\n            metrics=stats,\n            costs=cost_estimate,\n            execution_id=context.session_id,\n            start_time=start_time,\n            end_time=end_time,\n            success=True,\n        )\n\n    except Exception as e:\n        self.logger.error(f\"Pipeline execution failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.supports_async","title":"supports_async","text":"<pre><code>supports_async() -&gt; bool\n</code></pre> <p>Sync executor doesn't support async.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_async(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support async.\"\"\"\n    return False\n</code></pre>"},{"location":"api/orchestration/sync_executor/#ondine.orchestration.sync_executor.SyncExecutor.supports_streaming","title":"supports_streaming","text":"<pre><code>supports_streaming() -&gt; bool\n</code></pre> <p>Sync executor doesn't support streaming.</p> Source code in <code>ondine/orchestration/sync_executor.py</code> <pre><code>def supports_streaming(self) -&gt; bool:\n    \"\"\"Sync executor doesn't support streaming.\"\"\"\n    return False\n</code></pre>"},{"location":"api/stages/","title":"stages","text":""},{"location":"api/stages/#ondine.stages","title":"stages","text":"<p>Processing stages for data transformation.</p>"},{"location":"api/stages/#ondine.stages.DataLoaderStage","title":"DataLoaderStage","text":"<pre><code>DataLoaderStage(dataframe: DataFrame | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[DatasetSpec, DataFrame]</code></p> <p>Load data from source and validate schema.</p> <p>Responsibilities: - Read data from configured source - Validate required columns exist - Apply any filters - Update context with row count</p> <p>Initialize data loader stage.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded dataframe (for DataFrame source)</p> <code>None</code> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"\n    Initialize data loader stage.\n\n    Args:\n        dataframe: Optional pre-loaded dataframe (for DataFrame source)\n    \"\"\"\n    super().__init__(\"DataLoader\")\n    self.dataframe = dataframe\n</code></pre>"},{"location":"api/stages/#ondine.stages.DataLoaderStage.process","title":"process","text":"<pre><code>process(spec: DatasetSpec, context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Load data from source.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def process(self, spec: DatasetSpec, context: Any) -&gt; pd.DataFrame:\n    \"\"\"Load data from source.\"\"\"\n    # Create appropriate reader\n    reader = create_data_reader(\n        source_type=spec.source_type,\n        source_path=spec.source_path,\n        dataframe=self.dataframe,\n        delimiter=spec.delimiter,\n        encoding=spec.encoding,\n        sheet_name=spec.sheet_name,\n    )\n\n    # Read data\n    df = reader.read()\n\n    # Validate columns exist\n    missing_cols = set(spec.input_columns) - set(df.columns)\n    if missing_cols:\n        raise ValueError(f\"Missing columns: {missing_cols}\")\n\n    # Apply filters if specified\n    if spec.filters:\n        for column, value in spec.filters.items():\n            if column in df.columns:\n                df = df[df[column] == value]\n\n    # Update context with total rows\n    context.total_rows = len(df)\n\n    self.logger.info(f\"Loaded {len(df)} rows from {spec.source_type}\")\n\n    return df\n</code></pre>"},{"location":"api/stages/#ondine.stages.DataLoaderStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(spec: DatasetSpec) -&gt; ValidationResult\n</code></pre> <p>Validate dataset specification.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def validate_input(self, spec: DatasetSpec) -&gt; ValidationResult:\n    \"\"\"Validate dataset specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Check file exists for file sources\n    if spec.source_path and not spec.source_path.exists():\n        result.add_error(f\"Source file not found: {spec.source_path}\")\n\n    # Check input columns specified\n    if not spec.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    # Check output columns specified\n    if not spec.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.DataLoaderStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(spec: DatasetSpec) -&gt; CostEstimate\n</code></pre> <p>Data loading has no LLM cost.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def estimate_cost(self, spec: DatasetSpec) -&gt; CostEstimate:\n    \"\"\"Data loading has no LLM cost.\"\"\"\n    # Try to determine row count if dataframe is available\n    row_count = 0\n    if self.dataframe is not None:\n        row_count = len(self.dataframe)\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=row_count,\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage","title":"LLMInvocationStage","text":"<pre><code>LLMInvocationStage(llm_client: LLMClient, concurrency: int = 5, rate_limiter: RateLimiter | None = None, retry_handler: RetryHandler | None = None, error_policy: ErrorPolicy = ErrorPolicy.SKIP, max_retries: int = 3)\n</code></pre> <p>               Bases: <code>PipelineStage[list[PromptBatch], list[ResponseBatch]]</code></p> <p>Invoke LLM with prompts using concurrency and retries.</p> <p>Responsibilities: - Execute LLM calls with rate limiting - Handle retries for transient failures - Track tokens and costs - Support concurrent processing</p> <p>Initialize LLM invocation stage.</p> <p>Parameters:</p> Name Type Description Default <code>llm_client</code> <code>LLMClient</code> <p>LLM client instance</p> required <code>concurrency</code> <code>int</code> <p>Max concurrent requests</p> <code>5</code> <code>rate_limiter</code> <code>RateLimiter | None</code> <p>Optional rate limiter</p> <code>None</code> <code>retry_handler</code> <code>RetryHandler | None</code> <p>Optional retry handler</p> <code>None</code> <code>error_policy</code> <code>ErrorPolicy</code> <p>Policy for handling errors</p> <code>SKIP</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def __init__(\n    self,\n    llm_client: LLMClient,\n    concurrency: int = 5,\n    rate_limiter: RateLimiter | None = None,\n    retry_handler: RetryHandler | None = None,\n    error_policy: ErrorPolicy = ErrorPolicy.SKIP,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize LLM invocation stage.\n\n    Args:\n        llm_client: LLM client instance\n        concurrency: Max concurrent requests\n        rate_limiter: Optional rate limiter\n        retry_handler: Optional retry handler\n        error_policy: Policy for handling errors\n        max_retries: Maximum retry attempts\n    \"\"\"\n    super().__init__(\"LLMInvocation\")\n    self.llm_client = llm_client\n    self.concurrency = concurrency\n    self.rate_limiter = rate_limiter\n    self.retry_handler = retry_handler or RetryHandler()\n    self.error_handler = ErrorHandler(\n        policy=error_policy,\n        max_retries=max_retries,\n        default_value_factory=lambda: LLMResponse(\n            text=\"\",\n            tokens_in=0,\n            tokens_out=0,\n            model=llm_client.model,\n            cost=Decimal(\"0.0\"),\n            latency_ms=0.0,\n        ),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage.process","title":"process","text":"<pre><code>process(batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]\n</code></pre> <p>Execute LLM calls for all prompt batches.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def process(self, batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]:\n    \"\"\"Execute LLM calls for all prompt batches.\"\"\"\n    response_batches: list[ResponseBatch] = []\n\n    for _batch_idx, batch in enumerate(batches):\n        self.logger.info(\n            f\"Processing batch {batch.batch_id} ({len(batch.prompts)} prompts)\"\n        )\n\n        # Process batch with concurrency\n        responses = self._process_batch_concurrent(batch.prompts, context)\n\n        # Notify progress after each batch\n        if hasattr(context, \"notify_progress\"):\n            context.notify_progress()\n\n        # Calculate batch metrics\n        total_tokens = sum(r.tokens_in + r.tokens_out for r in responses)\n        total_cost = sum(r.cost for r in responses)\n        latencies = [r.latency_ms for r in responses]\n\n        # Create response batch\n        response_batch = ResponseBatch(\n            responses=[r.text for r in responses],\n            metadata=batch.metadata,\n            tokens_used=total_tokens,\n            cost=total_cost,\n            batch_id=batch.batch_id,\n            latencies_ms=latencies,\n        )\n        response_batches.append(response_batch)\n\n        # Update context\n        context.add_cost(total_cost, total_tokens)\n        context.update_row(batch.metadata[-1].row_index if batch.metadata else 0)\n\n    return response_batches\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(batches: list[PromptBatch]) -&gt; ValidationResult\n</code></pre> <p>Validate prompt batches.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def validate_input(self, batches: list[PromptBatch]) -&gt; ValidationResult:\n    \"\"\"Validate prompt batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    if not batches:\n        result.add_error(\"No prompt batches provided\")\n\n    for batch in batches:\n        if not batch.prompts:\n            result.add_error(f\"Batch {batch.batch_id} has no prompts\")\n\n        if len(batch.prompts) != len(batch.metadata):\n            result.add_error(f\"Batch {batch.batch_id} prompt/metadata mismatch\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.LLMInvocationStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(batches: list[PromptBatch]) -&gt; CostEstimate\n</code></pre> <p>Estimate LLM invocation cost.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def estimate_cost(self, batches: list[PromptBatch]) -&gt; CostEstimate:\n    \"\"\"Estimate LLM invocation cost.\"\"\"\n    total_input_tokens = 0\n    total_output_tokens = 0\n\n    # Estimate tokens for all prompts\n    for batch in batches:\n        for prompt in batch.prompts:\n            input_tokens = self.llm_client.estimate_tokens(prompt)\n            total_input_tokens += input_tokens\n\n            # Assume average output length (can be made configurable)\n            estimated_output = int(input_tokens * 0.5)\n            total_output_tokens += estimated_output\n\n    total_cost = self.llm_client.calculate_cost(\n        total_input_tokens, total_output_tokens\n    )\n\n    return CostEstimate(\n        total_cost=total_cost,\n        total_tokens=total_input_tokens + total_output_tokens,\n        input_tokens=total_input_tokens,\n        output_tokens=total_output_tokens,\n        rows=sum(len(b.prompts) for b in batches),\n        confidence=\"estimate\",\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.AggregationStrategy","title":"AggregationStrategy","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract base for aggregation strategies.</p> <p>Follows Strategy pattern for different ways to aggregate results.</p>"},{"location":"api/stages/#ondine.stages.AggregationStrategy.aggregate","title":"aggregate  <code>abstractmethod</code>","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Aggregate multiple results into one.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[T]</code> <p>List of results from multiple runs</p> required <p>Returns:</p> Type Description <code>T</code> <p>Aggregated result</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>@abstractmethod\ndef aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"\n    Aggregate multiple results into one.\n\n    Args:\n        results: List of results from multiple runs\n\n    Returns:\n        Aggregated result\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.AllStrategy","title":"AllStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns all results as list (no aggregation).</p>"},{"location":"api/stages/#ondine.stages.AllStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; list[T]\n</code></pre> <p>Return all results.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; list[T]:\n    \"\"\"Return all results.\"\"\"\n    return results\n</code></pre>"},{"location":"api/stages/#ondine.stages.AverageStrategy","title":"AverageStrategy","text":"<p>               Bases: <code>AggregationStrategy[float]</code></p> <p>Returns average of numeric results.</p>"},{"location":"api/stages/#ondine.stages.AverageStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[float]) -&gt; float\n</code></pre> <p>Return average.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[float]) -&gt; float:\n    \"\"\"Return average.\"\"\"\n    if not results:\n        return 0.0\n    return sum(results) / len(results)\n</code></pre>"},{"location":"api/stages/#ondine.stages.ConsensusStrategy","title":"ConsensusStrategy","text":"<p>               Bases: <code>AggregationStrategy[str]</code></p> <p>Returns most common result (consensus voting).</p>"},{"location":"api/stages/#ondine.stages.ConsensusStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[str]) -&gt; str\n</code></pre> <p>Return most frequent result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[str]) -&gt; str:\n    \"\"\"Return most frequent result.\"\"\"\n    if not results:\n        return \"\"\n\n    # Count occurrences\n    counter = Counter(results)\n    return counter.most_common(1)[0][0]\n</code></pre>"},{"location":"api/stages/#ondine.stages.FirstSuccessStrategy","title":"FirstSuccessStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns first successful (non-None) result.</p>"},{"location":"api/stages/#ondine.stages.FirstSuccessStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Return first non-None result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"Return first non-None result.\"\"\"\n    for result in results:\n        if result is not None:\n            return result\n    return results[0] if results else None\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage","title":"MultiRunStage","text":"<pre><code>MultiRunStage(wrapped_stage: PipelineStage[TInput, TOutput], num_runs: int = 3, aggregation_strategy: AggregationStrategy | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[TInput, TOutput]</code></p> <p>Decorator stage that runs wrapped stage multiple times.</p> <p>Use cases: - Run LLM 3 times, take consensus (reduce hallucinations) - Retry until success - Collect multiple responses for analysis</p> Example <p>multi_llm = MultiRunStage(     wrapped=LLMInvocationStage(...),     num_runs=3,     aggregation=ConsensusStrategy() )</p> <p>Initialize multi-run stage.</p> <p>Parameters:</p> Name Type Description Default <code>wrapped_stage</code> <code>PipelineStage[TInput, TOutput]</code> <p>Stage to execute multiple times</p> required <code>num_runs</code> <code>int</code> <p>Number of times to run</p> <code>3</code> <code>aggregation_strategy</code> <code>AggregationStrategy | None</code> <p>Strategy for aggregating results</p> <code>None</code> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def __init__(\n    self,\n    wrapped_stage: PipelineStage[TInput, TOutput],\n    num_runs: int = 3,\n    aggregation_strategy: AggregationStrategy | None = None,\n):\n    \"\"\"\n    Initialize multi-run stage.\n\n    Args:\n        wrapped_stage: Stage to execute multiple times\n        num_runs: Number of times to run\n        aggregation_strategy: Strategy for aggregating results\n    \"\"\"\n    super().__init__(f\"MultiRun({wrapped_stage.name})\")\n    self.wrapped_stage = wrapped_stage\n    self.num_runs = num_runs\n    self.aggregation_strategy = aggregation_strategy or ConsensusStrategy()\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage.process","title":"process","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute wrapped stage multiple times and aggregate.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"Execute wrapped stage multiple times and aggregate.\"\"\"\n    results: list[TOutput] = []\n\n    self.logger.info(f\"Running {self.wrapped_stage.name} {self.num_runs} times\")\n\n    for run_num in range(self.num_runs):\n        try:\n            result = self.wrapped_stage.process(input_data, context)\n            results.append(result)\n        except Exception as e:\n            self.logger.error(f\"Run {run_num + 1}/{self.num_runs} failed: {e}\")\n            # Continue with other runs\n            continue\n\n    if not results:\n        raise RuntimeError(\n            f\"All {self.num_runs} runs failed for {self.wrapped_stage.name}\"\n        )\n\n    # Aggregate results\n    aggregated = self.aggregation_strategy.aggregate(results)\n\n    self.logger.info(\n        f\"Aggregated {len(results)} results using \"\n        f\"{self.aggregation_strategy.__class__.__name__}\"\n    )\n\n    return aggregated\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Delegate validation to wrapped stage.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"Delegate validation to wrapped stage.\"\"\"\n    return self.wrapped_stage.validate_input(input_data)\n</code></pre>"},{"location":"api/stages/#ondine.stages.MultiRunStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate cost as num_runs \u00d7 wrapped stage cost.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"Estimate cost as num_runs \u00d7 wrapped stage cost.\"\"\"\n    single_run_cost = self.wrapped_stage.estimate_cost(input_data)\n\n    return CostEstimate(\n        total_cost=single_run_cost.total_cost * self.num_runs,\n        total_tokens=single_run_cost.total_tokens * self.num_runs,\n        input_tokens=single_run_cost.input_tokens * self.num_runs,\n        output_tokens=single_run_cost.output_tokens * self.num_runs,\n        rows=single_run_cost.rows,\n        confidence=f\"{single_run_cost.confidence} \u00d7 {self.num_runs} runs\",\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage","title":"PipelineStage","text":"<pre><code>PipelineStage(name: str)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[TInput, TOutput]</code></p> <p>Abstract base class for all pipeline stages.</p> <p>Implements Template Method pattern with hooks for extensibility. All stages follow Single Responsibility and are composable.</p> <p>Initialize pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable stage name</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize pipeline stage.\n\n    Args:\n        name: Human-readable stage name\n    \"\"\"\n    self.name = name\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.process","title":"process  <code>abstractmethod</code>","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Core processing logic (must be implemented by subclasses).</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data for this stage</p> required <code>context</code> <code>Any</code> <p>Execution context with shared state</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output data</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Core processing logic (must be implemented by subclasses).\n\n    Args:\n        input_data: Input data for this stage\n        context: Execution context with shared state\n\n    Returns:\n        Processed output data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.validate_input","title":"validate_input  <code>abstractmethod</code>","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Validate input before processing.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input to validate</p> required <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with errors/warnings</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"\n    Validate input before processing.\n\n    Args:\n        input_data: Input to validate\n\n    Returns:\n        ValidationResult with errors/warnings\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.execute","title":"execute","text":"<pre><code>execute(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute stage with pre/post hooks (Template Method).</p> <p>This method orchestrates the execution flow and should not be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input validation fails</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def execute(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Execute stage with pre/post hooks (Template Method).\n\n    This method orchestrates the execution flow and should not\n    be overridden.\n\n    Args:\n        input_data: Input data\n        context: Execution context\n\n    Returns:\n        Processed output\n\n    Raises:\n        ValueError: If input validation fails\n    \"\"\"\n    self.logger.info(f\"Starting stage: {self.name}\")\n\n    # Pre-processing hook\n    self.before_process(context)\n\n    # Validate input\n    validation = self.validate_input(input_data)\n    if not validation.is_valid:\n        error_msg = f\"Input validation failed: {validation.errors}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    if validation.warnings:\n        for warning in validation.warnings:\n            self.logger.warning(warning)\n\n    # Core processing\n    try:\n        result = self.process(input_data, context)\n        self.logger.info(f\"Completed stage: {self.name}\")\n\n        # Post-processing hook\n        self.after_process(result, context)\n\n        return result\n    except Exception as e:\n        self.logger.error(f\"Stage {self.name} failed: {e}\")\n        error_decision = self.on_error(e, context)\n        raise error_decision\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.before_process","title":"before_process","text":"<pre><code>before_process(context: Any) -&gt; None\n</code></pre> <p>Hook called before processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def before_process(self, context: Any) -&gt; None:\n    \"\"\"\n    Hook called before processing (default: no-op).\n\n    Args:\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.after_process","title":"after_process","text":"<pre><code>after_process(result: TOutput, context: Any) -&gt; None\n</code></pre> <p>Hook called after successful processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TOutput</code> <p>Processing result</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def after_process(self, result: TOutput, context: Any) -&gt; None:\n    \"\"\"\n    Hook called after successful processing (default: no-op).\n\n    Args:\n        result: Processing result\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.on_error","title":"on_error","text":"<pre><code>on_error(error: Exception, context: Any) -&gt; Exception\n</code></pre> <p>Hook called on processing error (default: re-raise).</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Exception</code> <p>Exception to raise (can transform error)</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def on_error(self, error: Exception, context: Any) -&gt; Exception:\n    \"\"\"\n    Hook called on processing error (default: re-raise).\n\n    Args:\n        error: The exception that occurred\n        context: Execution context\n\n    Returns:\n        Exception to raise (can transform error)\n    \"\"\"\n    return error\n</code></pre>"},{"location":"api/stages/#ondine.stages.PipelineStage.estimate_cost","title":"estimate_cost  <code>abstractmethod</code>","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate processing cost for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data to estimate for</p> required <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"\n    Estimate processing cost for this stage.\n\n    Args:\n        input_data: Input data to estimate for\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage","title":"PromptFormatterStage","text":"<pre><code>PromptFormatterStage(batch_size: int = 100, use_jinja2: bool = False)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, PromptSpec], list[PromptBatch]]</code></p> <p>Format prompts using template and row data.</p> <p>Responsibilities: - Extract input columns from rows - Format prompts using template - Batch prompts for efficient processing - Attach metadata for tracking</p> <p>Initialize prompt formatter stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of prompts per batch</p> <code>100</code> <code>use_jinja2</code> <code>bool</code> <p>Use Jinja2 for template rendering</p> <code>False</code> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def __init__(self, batch_size: int = 100, use_jinja2: bool = False):\n    \"\"\"\n    Initialize prompt formatter stage.\n\n    Args:\n        batch_size: Number of prompts per batch\n        use_jinja2: Use Jinja2 for template rendering\n    \"\"\"\n    super().__init__(\"PromptFormatter\")\n    self.batch_size = batch_size\n    self.use_jinja2 = use_jinja2\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, PromptSpec], context: Any) -&gt; list[PromptBatch]\n</code></pre> <p>Format prompts from DataFrame rows.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def process(\n    self, input_data: tuple[pd.DataFrame, PromptSpec], context: Any\n) -&gt; list[PromptBatch]:\n    \"\"\"Format prompts from DataFrame rows.\"\"\"\n    df, prompt_spec = input_data\n\n    prompts: list[str] = []\n    metadata_list: list[RowMetadata] = []\n\n    # Extract template variables\n    template_str = prompt_spec.template\n\n    # Create template renderer\n    if self.use_jinja2:\n        # Note: autoescape=False is intentional for LLM prompts (not HTML)\n        # We're generating text prompts, not web content, so HTML escaping\n        # would corrupt the prompt data sent to the LLM\n        template = Jinja2Template(template_str, autoescape=False)  # noqa: S701\n\n    # Format prompt for each row\n    for idx, row in df.iterrows():\n        try:\n            # Extract input columns\n            row_data = {col: row[col] for col in df.columns if col in template_str}\n\n            # Format prompt (Jinja2 or f-string)\n            if self.use_jinja2:\n                prompt = template.render(**row_data)\n            else:\n                prompt = template_str.format(**row_data)\n\n            # Add few-shot examples if specified\n            if prompt_spec.few_shot_examples:\n                examples_text = self._format_few_shot_examples(\n                    prompt_spec.few_shot_examples\n                )\n                prompt = f\"{examples_text}\\n\\n{prompt}\"\n\n            # Add system message if specified\n            if prompt_spec.system_message:\n                prompt = f\"{prompt_spec.system_message}\\n\\n{prompt}\"\n\n            prompts.append(prompt)\n\n            # Create metadata\n            metadata = RowMetadata(\n                row_index=idx,\n                row_id=row.get(\"id\", None),\n            )\n            metadata_list.append(metadata)\n\n        except KeyError as e:\n            self.logger.warning(f\"Missing template variable at row {idx}: {e}\")\n            continue\n        except Exception as e:\n            self.logger.error(f\"Error formatting prompt at row {idx}: {e}\")\n            continue\n\n    # Create batches\n    batches: list[PromptBatch] = []\n    for i in range(0, len(prompts), self.batch_size):\n        batch_prompts = prompts[i : i + self.batch_size]\n        batch_metadata = metadata_list[i : i + self.batch_size]\n\n        batch = PromptBatch(\n            prompts=batch_prompts,\n            metadata=batch_metadata,\n            batch_id=i // self.batch_size,\n        )\n        batches.append(batch)\n\n    self.logger.info(\n        f\"Formatted {len(prompts)} prompts into {len(batches)} batches\"\n    )\n\n    return batches\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, PromptSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate DataFrame and prompt specification.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; ValidationResult:\n    \"\"\"Validate DataFrame and prompt specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    df, prompt_spec = input_data\n\n    # Check DataFrame not empty\n    if df.empty:\n        result.add_error(\"DataFrame is empty\")\n\n    # Check template variables exist in DataFrame\n    template = prompt_spec.template\n    import re\n\n    variables = re.findall(r\"\\{(\\w+)\\}\", template)\n    missing_vars = set(variables) - set(df.columns)\n\n    if missing_vars:\n        result.add_error(f\"Template variables not in DataFrame: {missing_vars}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.PromptFormatterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, PromptSpec]) -&gt; CostEstimate\n</code></pre> <p>Prompt formatting has no LLM cost.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; CostEstimate:\n    \"\"\"Prompt formatting has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.JSONParser","title":"JSONParser","text":"<pre><code>JSONParser(strict: bool = False)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts JSON from response.</p> <p>Initialize JSON parser.</p> <p>Parameters:</p> Name Type Description Default <code>strict</code> <code>bool</code> <p>If True, fail on invalid JSON; if False, try to extract</p> <code>False</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, strict: bool = False):\n    \"\"\"\n    Initialize JSON parser.\n\n    Args:\n        strict: If True, fail on invalid JSON; if False, try to extract\n    \"\"\"\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/#ondine.stages.JSONParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse JSON from response.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse JSON from response.\"\"\"\n    try:\n        return json.loads(response.strip())\n    except json.JSONDecodeError:\n        if self.strict:\n            raise\n\n        # Try to extract JSON from markdown code blocks\n        if \"```json\" in response:\n            start = response.find(\"```json\") + 7\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        if \"```\" in response:\n            start = response.find(\"```\") + 3\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        # Return as raw text if can't parse\n        return {\"output\": response.strip()}\n</code></pre>"},{"location":"api/stages/#ondine.stages.PydanticParser","title":"PydanticParser","text":"<pre><code>PydanticParser(model: type[BaseModel], strict: bool = True)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that validates responses against Pydantic models.</p> <p>Provides type-safe extraction with automatic validation.</p> <p>Initialize Pydantic parser.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>type[BaseModel]</code> <p>Pydantic model class for validation</p> required <code>strict</code> <code>bool</code> <p>If True, fail on validation errors</p> <code>True</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, model: type[BaseModel], strict: bool = True):\n    \"\"\"\n    Initialize Pydantic parser.\n\n    Args:\n        model: Pydantic model class for validation\n        strict: If True, fail on validation errors\n    \"\"\"\n    self.model = model\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/#ondine.stages.PydanticParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; BaseModel\n</code></pre> <p>Parse and validate response with Pydantic model.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; BaseModel:\n    \"\"\"Parse and validate response with Pydantic model.\"\"\"\n    try:\n        # Try to parse as JSON first\n        json_parser = JSONParser(strict=False)\n        data = json_parser.parse(response)\n\n        # Validate with Pydantic and return the model instance\n        return self.model(**data)\n\n    except ValidationError as e:\n        if self.strict:\n            raise ValueError(f\"Pydantic validation failed: {e}\")\n        # Return raw data if validation fails\n        return {\"output\": response.strip(), \"validation_error\": str(e)}\n</code></pre>"},{"location":"api/stages/#ondine.stages.RawTextParser","title":"RawTextParser","text":"<p>               Bases: <code>ResponseParser</code></p> <p>Parser that returns raw text.</p>"},{"location":"api/stages/#ondine.stages.RawTextParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Return response as-is, after cleaning chat format artifacts.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Return response as-is, after cleaning chat format artifacts.\"\"\"\n    cleaned = response.strip()\n\n    # Strip common chat format prefixes (assistant:, user:, system:)\n    for prefix in [\"assistant:\", \"user:\", \"system:\"]:\n        if cleaned.lower().startswith(prefix):\n            cleaned = cleaned[len(prefix) :].strip()\n            break\n\n    return {\"output\": cleaned}\n</code></pre>"},{"location":"api/stages/#ondine.stages.RegexParser","title":"RegexParser","text":"<pre><code>RegexParser(patterns: dict[str, str])\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts data using regex patterns.</p> <p>Useful for extracting specific fields from structured text.</p> <p>Initialize regex parser.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>dict[str, str]</code> <p>Dict mapping field names to regex patterns</p> required Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, patterns: dict[str, str]):\n    \"\"\"\n    Initialize regex parser.\n\n    Args:\n        patterns: Dict mapping field names to regex patterns\n    \"\"\"\n    self.patterns = {key: re.compile(pattern) for key, pattern in patterns.items()}\n</code></pre>"},{"location":"api/stages/#ondine.stages.RegexParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Extract fields using regex patterns.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Extract fields using regex patterns.\"\"\"\n    result = {}\n\n    for field_name, pattern in self.patterns.items():\n        match = pattern.search(response)\n        if match:\n            # Use first group if groups exist, else full match\n            if match.groups():\n                result[field_name] = match.group(1)\n            else:\n                result[field_name] = match.group(0)\n        else:\n            result[field_name] = None\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParser","title":"ResponseParser","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for response parsers (Strategy pattern).</p>"},{"location":"api/stages/#ondine.stages.ResponseParser.parse","title":"parse  <code>abstractmethod</code>","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse response into structured data.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>@abstractmethod\ndef parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse response into structured data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage","title":"ResponseParserStage","text":"<pre><code>ResponseParserStage(parser: ResponseParser | None = None, output_columns: list[str] | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[list[ResponseBatch], list[str]], DataFrame]</code></p> <p>Parse LLM responses into structured DataFrame.</p> <p>Responsibilities: - Parse responses using configured parser - Map parsed data to output columns - Handle parse errors gracefully - Return DataFrame with results</p> <p>Initialize response parser stage.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ResponseParser | None</code> <p>Response parser (default: RawTextParser)</p> <code>None</code> <code>output_columns</code> <code>list[str] | None</code> <p>Output column names</p> <code>None</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(\n    self,\n    parser: ResponseParser | None = None,\n    output_columns: list[str] | None = None,\n):\n    \"\"\"\n    Initialize response parser stage.\n\n    Args:\n        parser: Response parser (default: RawTextParser)\n        output_columns: Output column names\n    \"\"\"\n    super().__init__(\"ResponseParser\")\n    self.parser = parser or RawTextParser()\n    self.output_columns = output_columns or [\"output\"]\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage.process","title":"process","text":"<pre><code>process(input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Parse responses into DataFrame.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Parse responses into DataFrame.\"\"\"\n    # Handle both tuple (batches, output_cols) and list [batches] for backward compatibility\n    if isinstance(input_data, tuple):\n        batches, output_cols = input_data\n        # Use output_cols from input_data (overrides self.output_columns if provided)\n        if not output_cols:\n            output_cols = self.output_columns\n    else:\n        # Backward compatibility: input_data is just the list of batches\n        batches = input_data\n        output_cols = self.output_columns\n\n    # Initialize result storage\n    results: dict[int, dict[str, Any]] = {}\n\n    # Parse all responses\n    for batch in batches:\n        for response, metadata in zip(\n            batch.responses, batch.metadata, strict=False\n        ):\n            try:\n                # Parse response text\n                response_text = (\n                    response.text if hasattr(response, \"text\") else str(response)\n                )\n                parsed = self.parser.parse(response_text)\n\n                # Map to output columns\n                row_data = {}\n                if len(output_cols) == 1:\n                    # Single output column\n                    if isinstance(parsed, dict) and \"output\" in parsed:\n                        row_data[output_cols[0]] = parsed[\"output\"]\n                    elif isinstance(parsed, dict):\n                        # Use first value\n                        row_data[output_cols[0]] = next(iter(parsed.values()))\n                    else:\n                        row_data[output_cols[0]] = parsed\n                else:\n                    # Multiple output columns\n                    for col in output_cols:\n                        row_data[col] = parsed.get(col, None)\n\n                results[metadata.row_index] = row_data\n\n            except Exception as e:\n                self.logger.error(\n                    f\"Failed to parse response at row {metadata.row_index}: {e}\"\n                )\n                # Store None for failed parses\n                results[metadata.row_index] = {col: None for col in output_cols}\n\n    # Create DataFrame\n    df = pd.DataFrame.from_dict(results, orient=\"index\")\n    df.index.name = \"row_index\"\n\n    self.logger.info(f\"Parsed {len(results)} responses\")\n\n    return df\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; ValidationResult\n</code></pre> <p>Validate response batches.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; ValidationResult:\n    \"\"\"Validate response batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    batches, output_cols = input_data\n\n    if not batches:\n        result.add_error(\"No response batches provided\")\n\n    if not output_cols:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResponseParserStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; CostEstimate\n</code></pre> <p>Response parsing has no LLM cost.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; CostEstimate:\n    \"\"\"Response parsing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.responses) for b in input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage","title":"ResultWriterStage","text":"<pre><code>ResultWriterStage()\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, DataFrame, OutputSpec], DataFrame]</code></p> <p>Write results to destination with merge support.</p> <p>Responsibilities: - Merge results with original data - Write to configured destination - Support atomic writes - Return merged DataFrame</p> <p>Initialize result writer stage.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize result writer stage.\"\"\"\n    super().__init__(\"ResultWriter\")\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, DataFrame, OutputSpec], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Write results to destination and return merged DataFrame.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Write results to destination and return merged DataFrame.\"\"\"\n    original_df, results_df, output_spec = input_data\n\n    # Merge results with original data\n    merged_df = self._merge_results(\n        original_df, results_df, output_spec.merge_strategy\n    )\n\n    # Write to destination\n    if output_spec.destination_path:\n        writer = create_data_writer(output_spec.destination_type)\n\n        if output_spec.atomic_write:\n            confirmation = writer.atomic_write(\n                merged_df, output_spec.destination_path\n            )\n        else:\n            confirmation = writer.write(merged_df, output_spec.destination_path)\n\n        self.logger.info(\n            f\"Wrote {confirmation.rows_written} rows to {confirmation.path}\"\n        )\n\n    # Always return the merged DataFrame (needed for quality validation)\n    return merged_df\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate input data and output specification.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def validate_input(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; ValidationResult:\n    \"\"\"Validate input data and output specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    original_df, results_df, output_spec = input_data\n\n    if original_df.empty:\n        result.add_warning(\"Original DataFrame is empty\")\n\n    if results_df.empty:\n        result.add_error(\"Results DataFrame is empty\")\n\n    # Check destination path if specified\n    if output_spec.destination_path:\n        dest_dir = output_spec.destination_path.parent\n        if not dest_dir.exists():\n            result.add_warning(f\"Destination directory does not exist: {dest_dir}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/#ondine.stages.ResultWriterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; CostEstimate\n</code></pre> <p>Result writing has no LLM cost.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def estimate_cost(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; CostEstimate:\n    \"\"\"Result writing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[1]),\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry","title":"StageRegistry","text":"<p>Global registry for custom pipeline stages.</p> <p>Enables registration and discovery of custom stages that can be injected into pipelines at specific positions.</p> Example"},{"location":"api/stages/#ondine.stages.StageRegistry--register-custom-stage","title":"Register custom stage","text":"<p>@StageRegistry.register(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     def execute(self, context):         # Custom retrieval logic         ...</p>"},{"location":"api/stages/#ondine.stages.StageRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline = (     PipelineBuilder.create()     .with_stage(\"rag_retrieval\", position=\"before_prompt\", index=\"my-docs\")     .build() )</p>"},{"location":"api/stages/#ondine.stages.StageRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(stage_name: str, stage_class: type) -&gt; type\n</code></pre> <p>Register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")</p> required <code>stage_class</code> <code>type</code> <p>Stage class implementing PipelineStage interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered stage class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name already registered</p> Example <p>@StageRegistry.register(\"fact_checker\") class FactCheckerStage(PipelineStage):     def execute(self, context):         # Verify LLM output against sources         ...</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef register(cls, stage_name: str, stage_class: type) -&gt; type:\n    \"\"\"\n    Register a custom pipeline stage.\n\n    Args:\n        stage_name: Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")\n        stage_class: Stage class implementing PipelineStage interface\n\n    Returns:\n        The registered stage class (enables use as decorator)\n\n    Raises:\n        ValueError: If stage_name already registered\n\n    Example:\n        @StageRegistry.register(\"fact_checker\")\n        class FactCheckerStage(PipelineStage):\n            def execute(self, context):\n                # Verify LLM output against sources\n                ...\n    \"\"\"\n    if stage_name in cls._stages:\n        raise ValueError(\n            f\"Stage '{stage_name}' already registered. \"\n            f\"Use a different stage_name or unregister first.\"\n        )\n\n    cls._stages[stage_name] = stage_class\n    return stage_class\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(stage_name: str) -&gt; type\n</code></pre> <p>Get stage class by name.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>Pipeline stage class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Example <p>stage_class = StageRegistry.get(\"rag_retrieval\") stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef get(cls, stage_name: str) -&gt; type:\n    \"\"\"\n    Get stage class by name.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        Pipeline stage class\n\n    Raises:\n        ValueError: If stage not found\n\n    Example:\n        stage_class = StageRegistry.get(\"rag_retrieval\")\n        stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")\n    \"\"\"\n    if stage_name not in cls._stages:\n        available = \", \".join(sorted(cls._stages.keys()))\n        raise ValueError(\n            f\"Unknown stage: '{stage_name}'. \"\n            f\"Available stages: {available if available else 'none'}\"\n        )\n\n    return cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.list_stages","title":"list_stages  <code>classmethod</code>","text":"<pre><code>list_stages() -&gt; dict[str, type]\n</code></pre> <p>List all registered stages.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping stage names to stage classes</p> Example <p>stages = StageRegistry.list_stages() print(f\"Available custom stages: {list(stages.keys())}\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef list_stages(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered stages.\n\n    Returns:\n        Dictionary mapping stage names to stage classes\n\n    Example:\n        stages = StageRegistry.list_stages()\n        print(f\"Available custom stages: {list(stages.keys())}\")\n    \"\"\"\n    return cls._stages.copy()\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(stage_name: str) -&gt; bool\n</code></pre> <p>Check if stage is registered.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, stage_name: str) -&gt; bool:\n    \"\"\"\n    Check if stage is registered.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    return stage_name in cls._stages\n</code></pre>"},{"location":"api/stages/#ondine.stages.StageRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(stage_name: str) -&gt; None\n</code></pre> <p>Unregister a stage (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, stage_name: str) -&gt; None:\n    \"\"\"\n    Unregister a stage (mainly for testing).\n\n    Args:\n        stage_name: Stage identifier\n\n    Raises:\n        ValueError: If stage not found\n    \"\"\"\n    if stage_name not in cls._stages:\n        raise ValueError(f\"Stage '{stage_name}' not registered\")\n\n    del cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/#ondine.stages.create_response_parser","title":"create_response_parser","text":"<pre><code>create_response_parser(prompt_spec: PromptSpec, output_columns: list[str]) -&gt; ResponseParser\n</code></pre> <p>Create appropriate response parser based on prompt specification.</p> <p>This factory enables configuration-driven parser selection, supporting: - Raw text output (default, backward compatible) - JSON structured output (for multiple fields) - Regex pattern extraction (for formatted text)</p> <p>Parameters:</p> Name Type Description Default <code>prompt_spec</code> <code>PromptSpec</code> <p>Prompt specification with response_format</p> required <code>output_columns</code> <code>list[str]</code> <p>Expected output column names</p> required <p>Returns:</p> Type Description <code>ResponseParser</code> <p>Configured ResponseParser instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If response_format is invalid or required config missing</p> Example Source code in <code>ondine/stages/parser_factory.py</code> <pre><code>def create_response_parser(\n    prompt_spec: PromptSpec,\n    output_columns: list[str],\n) -&gt; ResponseParser:\n    r\"\"\"\n    Create appropriate response parser based on prompt specification.\n\n    This factory enables configuration-driven parser selection, supporting:\n    - Raw text output (default, backward compatible)\n    - JSON structured output (for multiple fields)\n    - Regex pattern extraction (for formatted text)\n\n    Args:\n        prompt_spec: Prompt specification with response_format\n        output_columns: Expected output column names\n\n    Returns:\n        Configured ResponseParser instance\n\n    Raises:\n        ValueError: If response_format is invalid or required config missing\n\n    Example:\n        # JSON mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"json\",\n                json_fields=[\"score\", \"explanation\"]\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n\n        # Regex mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"regex\",\n                regex_patterns={\n                    \"score\": r\"SCORE:\\s*(\\d+)\",\n                    \"explanation\": r\"EXPLANATION:\\s*(.+)\"\n                }\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n    \"\"\"\n    response_format = prompt_spec.response_format.lower()\n\n    if response_format == \"json\":\n        logger.info(\"Creating JSONParser for multi-field extraction\")\n\n        # Validate JSON fields match output columns\n        if prompt_spec.json_fields:\n            json_set = set(prompt_spec.json_fields)\n            output_set = set(output_columns)\n            if json_set != output_set:\n                logger.warning(\n                    f\"JSON fields {json_set} don't match output columns {output_set}. \"\n                    f\"Using output_columns as authoritative.\"\n                )\n\n        return JSONParser(strict=False)\n\n    if response_format == \"regex\":\n        logger.info(\"Creating RegexParser for pattern-based extraction\")\n\n        if not prompt_spec.regex_patterns:\n            raise ValueError(\n                \"response_format='regex' requires regex_patterns to be specified\"\n            )\n\n        # Validate regex patterns cover all output columns\n        pattern_cols = set(prompt_spec.regex_patterns.keys())\n        output_set = set(output_columns)\n        missing = output_set - pattern_cols\n        if missing:\n            raise ValueError(f\"Missing regex patterns for output columns: {missing}\")\n\n        return RegexParser(patterns=prompt_spec.regex_patterns)\n\n    if response_format == \"raw\":\n        logger.info(\"Creating RawTextParser (default, backward compatible)\")\n\n        if len(output_columns) &gt; 1:\n            logger.warning(\n                f\"Using RawTextParser with {len(output_columns)} output columns. \"\n                f\"Only the first column will be populated. \"\n                f\"Consider using response_format='json' or 'regex' for multi-column output.\"\n            )\n\n        return RawTextParser()\n\n    # Should never reach here due to Pydantic validation\n    raise ValueError(\n        f\"Invalid response_format: '{response_format}'. \"\n        f\"Must be 'raw', 'json', or 'regex'.\"\n    )\n</code></pre>"},{"location":"api/stages/#ondine.stages.create_response_parser--json-mode","title":"JSON mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"json\",         json_fields=[\"score\", \"explanation\"]     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/#ondine.stages.create_response_parser--regex-mode","title":"Regex mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"regex\",         regex_patterns={             \"score\": r\"SCORE:\\s(\\d+)\",             \"explanation\": r\"EXPLANATION:\\s(.+)\"         }     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/#ondine.stages.stage","title":"stage","text":"<pre><code>stage(name: str)\n</code></pre> <p>Decorator to register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique stage identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@stage(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     '''Retrieve context from vector store and enrich data.'''</p> <pre><code>def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n    super().__init__(name=\"rag_retrieval\")\n    self.vector_store = vector_store\n    self.index_name = index_name\n    self.top_k = top_k\n\ndef execute(self, context: ExecutionContext) -&gt; StageResult:\n    # Retrieve context for each row\n    enriched_rows = []\n    for _, row in context.data.iterrows():\n        query = row['text']\n        results = self._retrieve(query)\n        row['retrieved_context'] = self._format_context(results)\n        enriched_rows.append(row)\n\n    context.data = pd.DataFrame(enriched_rows)\n    return StageResult(success=True, data=context.data)\n\ndef _retrieve(self, query: str):\n    # Integration with vector store\n    ...\n\ndef _format_context(self, results):\n    # Format retrieved docs\n    ...\n</code></pre> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>def stage(name: str):\n    \"\"\"\n    Decorator to register a custom pipeline stage.\n\n    Args:\n        name: Unique stage identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @stage(\"rag_retrieval\")\n        class RAGRetrievalStage(PipelineStage):\n            '''Retrieve context from vector store and enrich data.'''\n\n            def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n                super().__init__(name=\"rag_retrieval\")\n                self.vector_store = vector_store\n                self.index_name = index_name\n                self.top_k = top_k\n\n            def execute(self, context: ExecutionContext) -&gt; StageResult:\n                # Retrieve context for each row\n                enriched_rows = []\n                for _, row in context.data.iterrows():\n                    query = row['text']\n                    results = self._retrieve(query)\n                    row['retrieved_context'] = self._format_context(results)\n                    enriched_rows.append(row)\n\n                context.data = pd.DataFrame(enriched_rows)\n                return StageResult(success=True, data=context.data)\n\n            def _retrieve(self, query: str):\n                # Integration with vector store\n                ...\n\n            def _format_context(self, results):\n                # Format retrieved docs\n                ...\n    \"\"\"\n\n    def decorator(cls):\n        StageRegistry.register(name, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/stages/data_loader_stage/","title":"data_loader_stage","text":""},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage","title":"data_loader_stage","text":"<p>Data loading stage for reading tabular data.</p>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage","title":"DataLoaderStage","text":"<pre><code>DataLoaderStage(dataframe: DataFrame | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[DatasetSpec, DataFrame]</code></p> <p>Load data from source and validate schema.</p> <p>Responsibilities: - Read data from configured source - Validate required columns exist - Apply any filters - Update context with row count</p> <p>Initialize data loader stage.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame | None</code> <p>Optional pre-loaded dataframe (for DataFrame source)</p> <code>None</code> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def __init__(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"\n    Initialize data loader stage.\n\n    Args:\n        dataframe: Optional pre-loaded dataframe (for DataFrame source)\n    \"\"\"\n    super().__init__(\"DataLoader\")\n    self.dataframe = dataframe\n</code></pre>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage.process","title":"process","text":"<pre><code>process(spec: DatasetSpec, context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Load data from source.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def process(self, spec: DatasetSpec, context: Any) -&gt; pd.DataFrame:\n    \"\"\"Load data from source.\"\"\"\n    # Create appropriate reader\n    reader = create_data_reader(\n        source_type=spec.source_type,\n        source_path=spec.source_path,\n        dataframe=self.dataframe,\n        delimiter=spec.delimiter,\n        encoding=spec.encoding,\n        sheet_name=spec.sheet_name,\n    )\n\n    # Read data\n    df = reader.read()\n\n    # Validate columns exist\n    missing_cols = set(spec.input_columns) - set(df.columns)\n    if missing_cols:\n        raise ValueError(f\"Missing columns: {missing_cols}\")\n\n    # Apply filters if specified\n    if spec.filters:\n        for column, value in spec.filters.items():\n            if column in df.columns:\n                df = df[df[column] == value]\n\n    # Update context with total rows\n    context.total_rows = len(df)\n\n    self.logger.info(f\"Loaded {len(df)} rows from {spec.source_type}\")\n\n    return df\n</code></pre>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(spec: DatasetSpec) -&gt; ValidationResult\n</code></pre> <p>Validate dataset specification.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def validate_input(self, spec: DatasetSpec) -&gt; ValidationResult:\n    \"\"\"Validate dataset specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Check file exists for file sources\n    if spec.source_path and not spec.source_path.exists():\n        result.add_error(f\"Source file not found: {spec.source_path}\")\n\n    # Check input columns specified\n    if not spec.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    # Check output columns specified\n    if not spec.output_columns:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/data_loader_stage/#ondine.stages.data_loader_stage.DataLoaderStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(spec: DatasetSpec) -&gt; CostEstimate\n</code></pre> <p>Data loading has no LLM cost.</p> Source code in <code>ondine/stages/data_loader_stage.py</code> <pre><code>def estimate_cost(self, spec: DatasetSpec) -&gt; CostEstimate:\n    \"\"\"Data loading has no LLM cost.\"\"\"\n    # Try to determine row count if dataframe is available\n    row_count = 0\n    if self.dataframe is not None:\n        row_count = len(self.dataframe)\n\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=row_count,\n    )\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/","title":"llm_invocation_stage","text":""},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage","title":"llm_invocation_stage","text":"<p>LLM invocation stage with concurrency and retry logic.</p>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage","title":"LLMInvocationStage","text":"<pre><code>LLMInvocationStage(llm_client: LLMClient, concurrency: int = 5, rate_limiter: RateLimiter | None = None, retry_handler: RetryHandler | None = None, error_policy: ErrorPolicy = ErrorPolicy.SKIP, max_retries: int = 3)\n</code></pre> <p>               Bases: <code>PipelineStage[list[PromptBatch], list[ResponseBatch]]</code></p> <p>Invoke LLM with prompts using concurrency and retries.</p> <p>Responsibilities: - Execute LLM calls with rate limiting - Handle retries for transient failures - Track tokens and costs - Support concurrent processing</p> <p>Initialize LLM invocation stage.</p> <p>Parameters:</p> Name Type Description Default <code>llm_client</code> <code>LLMClient</code> <p>LLM client instance</p> required <code>concurrency</code> <code>int</code> <p>Max concurrent requests</p> <code>5</code> <code>rate_limiter</code> <code>RateLimiter | None</code> <p>Optional rate limiter</p> <code>None</code> <code>retry_handler</code> <code>RetryHandler | None</code> <p>Optional retry handler</p> <code>None</code> <code>error_policy</code> <code>ErrorPolicy</code> <p>Policy for handling errors</p> <code>SKIP</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def __init__(\n    self,\n    llm_client: LLMClient,\n    concurrency: int = 5,\n    rate_limiter: RateLimiter | None = None,\n    retry_handler: RetryHandler | None = None,\n    error_policy: ErrorPolicy = ErrorPolicy.SKIP,\n    max_retries: int = 3,\n):\n    \"\"\"\n    Initialize LLM invocation stage.\n\n    Args:\n        llm_client: LLM client instance\n        concurrency: Max concurrent requests\n        rate_limiter: Optional rate limiter\n        retry_handler: Optional retry handler\n        error_policy: Policy for handling errors\n        max_retries: Maximum retry attempts\n    \"\"\"\n    super().__init__(\"LLMInvocation\")\n    self.llm_client = llm_client\n    self.concurrency = concurrency\n    self.rate_limiter = rate_limiter\n    self.retry_handler = retry_handler or RetryHandler()\n    self.error_handler = ErrorHandler(\n        policy=error_policy,\n        max_retries=max_retries,\n        default_value_factory=lambda: LLMResponse(\n            text=\"\",\n            tokens_in=0,\n            tokens_out=0,\n            model=llm_client.model,\n            cost=Decimal(\"0.0\"),\n            latency_ms=0.0,\n        ),\n    )\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage.process","title":"process","text":"<pre><code>process(batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]\n</code></pre> <p>Execute LLM calls for all prompt batches.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def process(self, batches: list[PromptBatch], context: Any) -&gt; list[ResponseBatch]:\n    \"\"\"Execute LLM calls for all prompt batches.\"\"\"\n    response_batches: list[ResponseBatch] = []\n\n    for _batch_idx, batch in enumerate(batches):\n        self.logger.info(\n            f\"Processing batch {batch.batch_id} ({len(batch.prompts)} prompts)\"\n        )\n\n        # Process batch with concurrency\n        responses = self._process_batch_concurrent(batch.prompts, context)\n\n        # Notify progress after each batch\n        if hasattr(context, \"notify_progress\"):\n            context.notify_progress()\n\n        # Calculate batch metrics\n        total_tokens = sum(r.tokens_in + r.tokens_out for r in responses)\n        total_cost = sum(r.cost for r in responses)\n        latencies = [r.latency_ms for r in responses]\n\n        # Create response batch\n        response_batch = ResponseBatch(\n            responses=[r.text for r in responses],\n            metadata=batch.metadata,\n            tokens_used=total_tokens,\n            cost=total_cost,\n            batch_id=batch.batch_id,\n            latencies_ms=latencies,\n        )\n        response_batches.append(response_batch)\n\n        # Update context\n        context.add_cost(total_cost, total_tokens)\n        context.update_row(batch.metadata[-1].row_index if batch.metadata else 0)\n\n    return response_batches\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(batches: list[PromptBatch]) -&gt; ValidationResult\n</code></pre> <p>Validate prompt batches.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def validate_input(self, batches: list[PromptBatch]) -&gt; ValidationResult:\n    \"\"\"Validate prompt batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    if not batches:\n        result.add_error(\"No prompt batches provided\")\n\n    for batch in batches:\n        if not batch.prompts:\n            result.add_error(f\"Batch {batch.batch_id} has no prompts\")\n\n        if len(batch.prompts) != len(batch.metadata):\n            result.add_error(f\"Batch {batch.batch_id} prompt/metadata mismatch\")\n\n    return result\n</code></pre>"},{"location":"api/stages/llm_invocation_stage/#ondine.stages.llm_invocation_stage.LLMInvocationStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(batches: list[PromptBatch]) -&gt; CostEstimate\n</code></pre> <p>Estimate LLM invocation cost.</p> Source code in <code>ondine/stages/llm_invocation_stage.py</code> <pre><code>def estimate_cost(self, batches: list[PromptBatch]) -&gt; CostEstimate:\n    \"\"\"Estimate LLM invocation cost.\"\"\"\n    total_input_tokens = 0\n    total_output_tokens = 0\n\n    # Estimate tokens for all prompts\n    for batch in batches:\n        for prompt in batch.prompts:\n            input_tokens = self.llm_client.estimate_tokens(prompt)\n            total_input_tokens += input_tokens\n\n            # Assume average output length (can be made configurable)\n            estimated_output = int(input_tokens * 0.5)\n            total_output_tokens += estimated_output\n\n    total_cost = self.llm_client.calculate_cost(\n        total_input_tokens, total_output_tokens\n    )\n\n    return CostEstimate(\n        total_cost=total_cost,\n        total_tokens=total_input_tokens + total_output_tokens,\n        input_tokens=total_input_tokens,\n        output_tokens=total_output_tokens,\n        rows=sum(len(b.prompts) for b in batches),\n        confidence=\"estimate\",\n    )\n</code></pre>"},{"location":"api/stages/multi_run_stage/","title":"multi_run_stage","text":""},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage","title":"multi_run_stage","text":"<p>Multi-run stage for executing stages multiple times with aggregation.</p> <p>Implements Decorator pattern to wrap any stage and run it multiple times.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AggregationStrategy","title":"AggregationStrategy","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Abstract base for aggregation strategies.</p> <p>Follows Strategy pattern for different ways to aggregate results.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AggregationStrategy.aggregate","title":"aggregate  <code>abstractmethod</code>","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Aggregate multiple results into one.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[T]</code> <p>List of results from multiple runs</p> required <p>Returns:</p> Type Description <code>T</code> <p>Aggregated result</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>@abstractmethod\ndef aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"\n    Aggregate multiple results into one.\n\n    Args:\n        results: List of results from multiple runs\n\n    Returns:\n        Aggregated result\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.ConsensusStrategy","title":"ConsensusStrategy","text":"<p>               Bases: <code>AggregationStrategy[str]</code></p> <p>Returns most common result (consensus voting).</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.ConsensusStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[str]) -&gt; str\n</code></pre> <p>Return most frequent result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[str]) -&gt; str:\n    \"\"\"Return most frequent result.\"\"\"\n    if not results:\n        return \"\"\n\n    # Count occurrences\n    counter = Counter(results)\n    return counter.most_common(1)[0][0]\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.FirstSuccessStrategy","title":"FirstSuccessStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns first successful (non-None) result.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.FirstSuccessStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; T\n</code></pre> <p>Return first non-None result.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; T:\n    \"\"\"Return first non-None result.\"\"\"\n    for result in results:\n        if result is not None:\n            return result\n    return results[0] if results else None\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AllStrategy","title":"AllStrategy","text":"<p>               Bases: <code>AggregationStrategy[T]</code></p> <p>Returns all results as list (no aggregation).</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AllStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[T]) -&gt; list[T]\n</code></pre> <p>Return all results.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[T]) -&gt; list[T]:\n    \"\"\"Return all results.\"\"\"\n    return results\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AverageStrategy","title":"AverageStrategy","text":"<p>               Bases: <code>AggregationStrategy[float]</code></p> <p>Returns average of numeric results.</p>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.AverageStrategy.aggregate","title":"aggregate","text":"<pre><code>aggregate(results: list[float]) -&gt; float\n</code></pre> <p>Return average.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def aggregate(self, results: list[float]) -&gt; float:\n    \"\"\"Return average.\"\"\"\n    if not results:\n        return 0.0\n    return sum(results) / len(results)\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage","title":"MultiRunStage","text":"<pre><code>MultiRunStage(wrapped_stage: PipelineStage[TInput, TOutput], num_runs: int = 3, aggregation_strategy: AggregationStrategy | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[TInput, TOutput]</code></p> <p>Decorator stage that runs wrapped stage multiple times.</p> <p>Use cases: - Run LLM 3 times, take consensus (reduce hallucinations) - Retry until success - Collect multiple responses for analysis</p> Example <p>multi_llm = MultiRunStage(     wrapped=LLMInvocationStage(...),     num_runs=3,     aggregation=ConsensusStrategy() )</p> <p>Initialize multi-run stage.</p> <p>Parameters:</p> Name Type Description Default <code>wrapped_stage</code> <code>PipelineStage[TInput, TOutput]</code> <p>Stage to execute multiple times</p> required <code>num_runs</code> <code>int</code> <p>Number of times to run</p> <code>3</code> <code>aggregation_strategy</code> <code>AggregationStrategy | None</code> <p>Strategy for aggregating results</p> <code>None</code> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def __init__(\n    self,\n    wrapped_stage: PipelineStage[TInput, TOutput],\n    num_runs: int = 3,\n    aggregation_strategy: AggregationStrategy | None = None,\n):\n    \"\"\"\n    Initialize multi-run stage.\n\n    Args:\n        wrapped_stage: Stage to execute multiple times\n        num_runs: Number of times to run\n        aggregation_strategy: Strategy for aggregating results\n    \"\"\"\n    super().__init__(f\"MultiRun({wrapped_stage.name})\")\n    self.wrapped_stage = wrapped_stage\n    self.num_runs = num_runs\n    self.aggregation_strategy = aggregation_strategy or ConsensusStrategy()\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage.process","title":"process","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute wrapped stage multiple times and aggregate.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"Execute wrapped stage multiple times and aggregate.\"\"\"\n    results: list[TOutput] = []\n\n    self.logger.info(f\"Running {self.wrapped_stage.name} {self.num_runs} times\")\n\n    for run_num in range(self.num_runs):\n        try:\n            result = self.wrapped_stage.process(input_data, context)\n            results.append(result)\n        except Exception as e:\n            self.logger.error(f\"Run {run_num + 1}/{self.num_runs} failed: {e}\")\n            # Continue with other runs\n            continue\n\n    if not results:\n        raise RuntimeError(\n            f\"All {self.num_runs} runs failed for {self.wrapped_stage.name}\"\n        )\n\n    # Aggregate results\n    aggregated = self.aggregation_strategy.aggregate(results)\n\n    self.logger.info(\n        f\"Aggregated {len(results)} results using \"\n        f\"{self.aggregation_strategy.__class__.__name__}\"\n    )\n\n    return aggregated\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Delegate validation to wrapped stage.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"Delegate validation to wrapped stage.\"\"\"\n    return self.wrapped_stage.validate_input(input_data)\n</code></pre>"},{"location":"api/stages/multi_run_stage/#ondine.stages.multi_run_stage.MultiRunStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate cost as num_runs \u00d7 wrapped stage cost.</p> Source code in <code>ondine/stages/multi_run_stage.py</code> <pre><code>def estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"Estimate cost as num_runs \u00d7 wrapped stage cost.\"\"\"\n    single_run_cost = self.wrapped_stage.estimate_cost(input_data)\n\n    return CostEstimate(\n        total_cost=single_run_cost.total_cost * self.num_runs,\n        total_tokens=single_run_cost.total_tokens * self.num_runs,\n        input_tokens=single_run_cost.input_tokens * self.num_runs,\n        output_tokens=single_run_cost.output_tokens * self.num_runs,\n        rows=single_run_cost.rows,\n        confidence=f\"{single_run_cost.confidence} \u00d7 {self.num_runs} runs\",\n    )\n</code></pre>"},{"location":"api/stages/parser_factory/","title":"parser_factory","text":""},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory","title":"parser_factory","text":"<p>Factory for creating response parsers based on configuration.</p>"},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory.create_response_parser","title":"create_response_parser","text":"<pre><code>create_response_parser(prompt_spec: PromptSpec, output_columns: list[str]) -&gt; ResponseParser\n</code></pre> <p>Create appropriate response parser based on prompt specification.</p> <p>This factory enables configuration-driven parser selection, supporting: - Raw text output (default, backward compatible) - JSON structured output (for multiple fields) - Regex pattern extraction (for formatted text)</p> <p>Parameters:</p> Name Type Description Default <code>prompt_spec</code> <code>PromptSpec</code> <p>Prompt specification with response_format</p> required <code>output_columns</code> <code>list[str]</code> <p>Expected output column names</p> required <p>Returns:</p> Type Description <code>ResponseParser</code> <p>Configured ResponseParser instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If response_format is invalid or required config missing</p> Example Source code in <code>ondine/stages/parser_factory.py</code> <pre><code>def create_response_parser(\n    prompt_spec: PromptSpec,\n    output_columns: list[str],\n) -&gt; ResponseParser:\n    r\"\"\"\n    Create appropriate response parser based on prompt specification.\n\n    This factory enables configuration-driven parser selection, supporting:\n    - Raw text output (default, backward compatible)\n    - JSON structured output (for multiple fields)\n    - Regex pattern extraction (for formatted text)\n\n    Args:\n        prompt_spec: Prompt specification with response_format\n        output_columns: Expected output column names\n\n    Returns:\n        Configured ResponseParser instance\n\n    Raises:\n        ValueError: If response_format is invalid or required config missing\n\n    Example:\n        # JSON mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"json\",\n                json_fields=[\"score\", \"explanation\"]\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n\n        # Regex mode\n        parser = create_response_parser(\n            prompt_spec=PromptSpec(\n                template=\"...\",\n                response_format=\"regex\",\n                regex_patterns={\n                    \"score\": r\"SCORE:\\s*(\\d+)\",\n                    \"explanation\": r\"EXPLANATION:\\s*(.+)\"\n                }\n            ),\n            output_columns=[\"score\", \"explanation\"]\n        )\n    \"\"\"\n    response_format = prompt_spec.response_format.lower()\n\n    if response_format == \"json\":\n        logger.info(\"Creating JSONParser for multi-field extraction\")\n\n        # Validate JSON fields match output columns\n        if prompt_spec.json_fields:\n            json_set = set(prompt_spec.json_fields)\n            output_set = set(output_columns)\n            if json_set != output_set:\n                logger.warning(\n                    f\"JSON fields {json_set} don't match output columns {output_set}. \"\n                    f\"Using output_columns as authoritative.\"\n                )\n\n        return JSONParser(strict=False)\n\n    if response_format == \"regex\":\n        logger.info(\"Creating RegexParser for pattern-based extraction\")\n\n        if not prompt_spec.regex_patterns:\n            raise ValueError(\n                \"response_format='regex' requires regex_patterns to be specified\"\n            )\n\n        # Validate regex patterns cover all output columns\n        pattern_cols = set(prompt_spec.regex_patterns.keys())\n        output_set = set(output_columns)\n        missing = output_set - pattern_cols\n        if missing:\n            raise ValueError(f\"Missing regex patterns for output columns: {missing}\")\n\n        return RegexParser(patterns=prompt_spec.regex_patterns)\n\n    if response_format == \"raw\":\n        logger.info(\"Creating RawTextParser (default, backward compatible)\")\n\n        if len(output_columns) &gt; 1:\n            logger.warning(\n                f\"Using RawTextParser with {len(output_columns)} output columns. \"\n                f\"Only the first column will be populated. \"\n                f\"Consider using response_format='json' or 'regex' for multi-column output.\"\n            )\n\n        return RawTextParser()\n\n    # Should never reach here due to Pydantic validation\n    raise ValueError(\n        f\"Invalid response_format: '{response_format}'. \"\n        f\"Must be 'raw', 'json', or 'regex'.\"\n    )\n</code></pre>"},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory.create_response_parser--json-mode","title":"JSON mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"json\",         json_fields=[\"score\", \"explanation\"]     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/parser_factory/#ondine.stages.parser_factory.create_response_parser--regex-mode","title":"Regex mode","text":"<p>parser = create_response_parser(     prompt_spec=PromptSpec(         template=\"...\",         response_format=\"regex\",         regex_patterns={             \"score\": r\"SCORE:\\s(\\d+)\",             \"explanation\": r\"EXPLANATION:\\s(.+)\"         }     ),     output_columns=[\"score\", \"explanation\"] )</p>"},{"location":"api/stages/pipeline_stage/","title":"pipeline_stage","text":""},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage","title":"pipeline_stage","text":"<p>Base pipeline stage abstraction.</p> <p>Defines the contract for all processing stages using Template Method pattern for execution flow.</p>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage","title":"PipelineStage","text":"<pre><code>PipelineStage(name: str)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[TInput, TOutput]</code></p> <p>Abstract base class for all pipeline stages.</p> <p>Implements Template Method pattern with hooks for extensibility. All stages follow Single Responsibility and are composable.</p> <p>Initialize pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Human-readable stage name</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize pipeline stage.\n\n    Args:\n        name: Human-readable stage name\n    \"\"\"\n    self.name = name\n    self.logger = get_logger(f\"{__name__}.{name}\")\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.process","title":"process  <code>abstractmethod</code>","text":"<pre><code>process(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Core processing logic (must be implemented by subclasses).</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data for this stage</p> required <code>context</code> <code>Any</code> <p>Execution context with shared state</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output data</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef process(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Core processing logic (must be implemented by subclasses).\n\n    Args:\n        input_data: Input data for this stage\n        context: Execution context with shared state\n\n    Returns:\n        Processed output data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.validate_input","title":"validate_input  <code>abstractmethod</code>","text":"<pre><code>validate_input(input_data: TInput) -&gt; ValidationResult\n</code></pre> <p>Validate input before processing.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input to validate</p> required <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with errors/warnings</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef validate_input(self, input_data: TInput) -&gt; ValidationResult:\n    \"\"\"\n    Validate input before processing.\n\n    Args:\n        input_data: Input to validate\n\n    Returns:\n        ValidationResult with errors/warnings\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.execute","title":"execute","text":"<pre><code>execute(input_data: TInput, context: Any) -&gt; TOutput\n</code></pre> <p>Execute stage with pre/post hooks (Template Method).</p> <p>This method orchestrates the execution flow and should not be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>TOutput</code> <p>Processed output</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input validation fails</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def execute(self, input_data: TInput, context: Any) -&gt; TOutput:\n    \"\"\"\n    Execute stage with pre/post hooks (Template Method).\n\n    This method orchestrates the execution flow and should not\n    be overridden.\n\n    Args:\n        input_data: Input data\n        context: Execution context\n\n    Returns:\n        Processed output\n\n    Raises:\n        ValueError: If input validation fails\n    \"\"\"\n    self.logger.info(f\"Starting stage: {self.name}\")\n\n    # Pre-processing hook\n    self.before_process(context)\n\n    # Validate input\n    validation = self.validate_input(input_data)\n    if not validation.is_valid:\n        error_msg = f\"Input validation failed: {validation.errors}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n\n    if validation.warnings:\n        for warning in validation.warnings:\n            self.logger.warning(warning)\n\n    # Core processing\n    try:\n        result = self.process(input_data, context)\n        self.logger.info(f\"Completed stage: {self.name}\")\n\n        # Post-processing hook\n        self.after_process(result, context)\n\n        return result\n    except Exception as e:\n        self.logger.error(f\"Stage {self.name} failed: {e}\")\n        error_decision = self.on_error(e, context)\n        raise error_decision\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.before_process","title":"before_process","text":"<pre><code>before_process(context: Any) -&gt; None\n</code></pre> <p>Hook called before processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def before_process(self, context: Any) -&gt; None:\n    \"\"\"\n    Hook called before processing (default: no-op).\n\n    Args:\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.after_process","title":"after_process","text":"<pre><code>after_process(result: TOutput, context: Any) -&gt; None\n</code></pre> <p>Hook called after successful processing (default: no-op).</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TOutput</code> <p>Processing result</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def after_process(self, result: TOutput, context: Any) -&gt; None:\n    \"\"\"\n    Hook called after successful processing (default: no-op).\n\n    Args:\n        result: Processing result\n        context: Execution context\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.on_error","title":"on_error","text":"<pre><code>on_error(error: Exception, context: Any) -&gt; Exception\n</code></pre> <p>Hook called on processing error (default: re-raise).</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>The exception that occurred</p> required <code>context</code> <code>Any</code> <p>Execution context</p> required <p>Returns:</p> Type Description <code>Exception</code> <p>Exception to raise (can transform error)</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>def on_error(self, error: Exception, context: Any) -&gt; Exception:\n    \"\"\"\n    Hook called on processing error (default: re-raise).\n\n    Args:\n        error: The exception that occurred\n        context: Execution context\n\n    Returns:\n        Exception to raise (can transform error)\n    \"\"\"\n    return error\n</code></pre>"},{"location":"api/stages/pipeline_stage/#ondine.stages.pipeline_stage.PipelineStage.estimate_cost","title":"estimate_cost  <code>abstractmethod</code>","text":"<pre><code>estimate_cost(input_data: TInput) -&gt; CostEstimate\n</code></pre> <p>Estimate processing cost for this stage.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>TInput</code> <p>Input data to estimate for</p> required <p>Returns:</p> Type Description <code>CostEstimate</code> <p>Cost estimate</p> Source code in <code>ondine/stages/pipeline_stage.py</code> <pre><code>@abstractmethod\ndef estimate_cost(self, input_data: TInput) -&gt; CostEstimate:\n    \"\"\"\n    Estimate processing cost for this stage.\n\n    Args:\n        input_data: Input data to estimate for\n\n    Returns:\n        Cost estimate\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/","title":"prompt_formatter_stage","text":""},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage","title":"prompt_formatter_stage","text":"<p>Prompt formatting stage for template-based prompt generation.</p>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage","title":"PromptFormatterStage","text":"<pre><code>PromptFormatterStage(batch_size: int = 100, use_jinja2: bool = False)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, PromptSpec], list[PromptBatch]]</code></p> <p>Format prompts using template and row data.</p> <p>Responsibilities: - Extract input columns from rows - Format prompts using template - Batch prompts for efficient processing - Attach metadata for tracking</p> <p>Initialize prompt formatter stage.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of prompts per batch</p> <code>100</code> <code>use_jinja2</code> <code>bool</code> <p>Use Jinja2 for template rendering</p> <code>False</code> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def __init__(self, batch_size: int = 100, use_jinja2: bool = False):\n    \"\"\"\n    Initialize prompt formatter stage.\n\n    Args:\n        batch_size: Number of prompts per batch\n        use_jinja2: Use Jinja2 for template rendering\n    \"\"\"\n    super().__init__(\"PromptFormatter\")\n    self.batch_size = batch_size\n    self.use_jinja2 = use_jinja2\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, PromptSpec], context: Any) -&gt; list[PromptBatch]\n</code></pre> <p>Format prompts from DataFrame rows.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def process(\n    self, input_data: tuple[pd.DataFrame, PromptSpec], context: Any\n) -&gt; list[PromptBatch]:\n    \"\"\"Format prompts from DataFrame rows.\"\"\"\n    df, prompt_spec = input_data\n\n    prompts: list[str] = []\n    metadata_list: list[RowMetadata] = []\n\n    # Extract template variables\n    template_str = prompt_spec.template\n\n    # Create template renderer\n    if self.use_jinja2:\n        # Note: autoescape=False is intentional for LLM prompts (not HTML)\n        # We're generating text prompts, not web content, so HTML escaping\n        # would corrupt the prompt data sent to the LLM\n        template = Jinja2Template(template_str, autoescape=False)  # noqa: S701\n\n    # Format prompt for each row\n    for idx, row in df.iterrows():\n        try:\n            # Extract input columns\n            row_data = {col: row[col] for col in df.columns if col in template_str}\n\n            # Format prompt (Jinja2 or f-string)\n            if self.use_jinja2:\n                prompt = template.render(**row_data)\n            else:\n                prompt = template_str.format(**row_data)\n\n            # Add few-shot examples if specified\n            if prompt_spec.few_shot_examples:\n                examples_text = self._format_few_shot_examples(\n                    prompt_spec.few_shot_examples\n                )\n                prompt = f\"{examples_text}\\n\\n{prompt}\"\n\n            # Add system message if specified\n            if prompt_spec.system_message:\n                prompt = f\"{prompt_spec.system_message}\\n\\n{prompt}\"\n\n            prompts.append(prompt)\n\n            # Create metadata\n            metadata = RowMetadata(\n                row_index=idx,\n                row_id=row.get(\"id\", None),\n            )\n            metadata_list.append(metadata)\n\n        except KeyError as e:\n            self.logger.warning(f\"Missing template variable at row {idx}: {e}\")\n            continue\n        except Exception as e:\n            self.logger.error(f\"Error formatting prompt at row {idx}: {e}\")\n            continue\n\n    # Create batches\n    batches: list[PromptBatch] = []\n    for i in range(0, len(prompts), self.batch_size):\n        batch_prompts = prompts[i : i + self.batch_size]\n        batch_metadata = metadata_list[i : i + self.batch_size]\n\n        batch = PromptBatch(\n            prompts=batch_prompts,\n            metadata=batch_metadata,\n            batch_id=i // self.batch_size,\n        )\n        batches.append(batch)\n\n    self.logger.info(\n        f\"Formatted {len(prompts)} prompts into {len(batches)} batches\"\n    )\n\n    return batches\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, PromptSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate DataFrame and prompt specification.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; ValidationResult:\n    \"\"\"Validate DataFrame and prompt specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    df, prompt_spec = input_data\n\n    # Check DataFrame not empty\n    if df.empty:\n        result.add_error(\"DataFrame is empty\")\n\n    # Check template variables exist in DataFrame\n    template = prompt_spec.template\n    import re\n\n    variables = re.findall(r\"\\{(\\w+)\\}\", template)\n    missing_vars = set(variables) - set(df.columns)\n\n    if missing_vars:\n        result.add_error(f\"Template variables not in DataFrame: {missing_vars}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/prompt_formatter_stage/#ondine.stages.prompt_formatter_stage.PromptFormatterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, PromptSpec]) -&gt; CostEstimate\n</code></pre> <p>Prompt formatting has no LLM cost.</p> Source code in <code>ondine/stages/prompt_formatter_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[pd.DataFrame, PromptSpec]\n) -&gt; CostEstimate:\n    \"\"\"Prompt formatting has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/response_parser_stage/","title":"response_parser_stage","text":""},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage","title":"response_parser_stage","text":"<p>Response parsing stage for structured output extraction.</p>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParser","title":"ResponseParser","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for response parsers (Strategy pattern).</p>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParser.parse","title":"parse  <code>abstractmethod</code>","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse response into structured data.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>@abstractmethod\ndef parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse response into structured data.\"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RawTextParser","title":"RawTextParser","text":"<p>               Bases: <code>ResponseParser</code></p> <p>Parser that returns raw text.</p>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RawTextParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Return response as-is, after cleaning chat format artifacts.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Return response as-is, after cleaning chat format artifacts.\"\"\"\n    cleaned = response.strip()\n\n    # Strip common chat format prefixes (assistant:, user:, system:)\n    for prefix in [\"assistant:\", \"user:\", \"system:\"]:\n        if cleaned.lower().startswith(prefix):\n            cleaned = cleaned[len(prefix) :].strip()\n            break\n\n    return {\"output\": cleaned}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.JSONParser","title":"JSONParser","text":"<pre><code>JSONParser(strict: bool = False)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts JSON from response.</p> <p>Initialize JSON parser.</p> <p>Parameters:</p> Name Type Description Default <code>strict</code> <code>bool</code> <p>If True, fail on invalid JSON; if False, try to extract</p> <code>False</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, strict: bool = False):\n    \"\"\"\n    Initialize JSON parser.\n\n    Args:\n        strict: If True, fail on invalid JSON; if False, try to extract\n    \"\"\"\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.JSONParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Parse JSON from response.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Parse JSON from response.\"\"\"\n    try:\n        return json.loads(response.strip())\n    except json.JSONDecodeError:\n        if self.strict:\n            raise\n\n        # Try to extract JSON from markdown code blocks\n        if \"```json\" in response:\n            start = response.find(\"```json\") + 7\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        if \"```\" in response:\n            start = response.find(\"```\") + 3\n            end = response.find(\"```\", start)\n            json_str = response[start:end].strip()\n            return json.loads(json_str)\n        # Return as raw text if can't parse\n        return {\"output\": response.strip()}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.PydanticParser","title":"PydanticParser","text":"<pre><code>PydanticParser(model: type[BaseModel], strict: bool = True)\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that validates responses against Pydantic models.</p> <p>Provides type-safe extraction with automatic validation.</p> <p>Initialize Pydantic parser.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>type[BaseModel]</code> <p>Pydantic model class for validation</p> required <code>strict</code> <code>bool</code> <p>If True, fail on validation errors</p> <code>True</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, model: type[BaseModel], strict: bool = True):\n    \"\"\"\n    Initialize Pydantic parser.\n\n    Args:\n        model: Pydantic model class for validation\n        strict: If True, fail on validation errors\n    \"\"\"\n    self.model = model\n    self.strict = strict\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.PydanticParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; BaseModel\n</code></pre> <p>Parse and validate response with Pydantic model.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; BaseModel:\n    \"\"\"Parse and validate response with Pydantic model.\"\"\"\n    try:\n        # Try to parse as JSON first\n        json_parser = JSONParser(strict=False)\n        data = json_parser.parse(response)\n\n        # Validate with Pydantic and return the model instance\n        return self.model(**data)\n\n    except ValidationError as e:\n        if self.strict:\n            raise ValueError(f\"Pydantic validation failed: {e}\")\n        # Return raw data if validation fails\n        return {\"output\": response.strip(), \"validation_error\": str(e)}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RegexParser","title":"RegexParser","text":"<pre><code>RegexParser(patterns: dict[str, str])\n</code></pre> <p>               Bases: <code>ResponseParser</code></p> <p>Parser that extracts data using regex patterns.</p> <p>Useful for extracting specific fields from structured text.</p> <p>Initialize regex parser.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>dict[str, str]</code> <p>Dict mapping field names to regex patterns</p> required Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(self, patterns: dict[str, str]):\n    \"\"\"\n    Initialize regex parser.\n\n    Args:\n        patterns: Dict mapping field names to regex patterns\n    \"\"\"\n    self.patterns = {key: re.compile(pattern) for key, pattern in patterns.items()}\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.RegexParser.parse","title":"parse","text":"<pre><code>parse(response: str) -&gt; dict[str, Any]\n</code></pre> <p>Extract fields using regex patterns.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def parse(self, response: str) -&gt; dict[str, Any]:\n    \"\"\"Extract fields using regex patterns.\"\"\"\n    result = {}\n\n    for field_name, pattern in self.patterns.items():\n        match = pattern.search(response)\n        if match:\n            # Use first group if groups exist, else full match\n            if match.groups():\n                result[field_name] = match.group(1)\n            else:\n                result[field_name] = match.group(0)\n        else:\n            result[field_name] = None\n\n    return result\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage","title":"ResponseParserStage","text":"<pre><code>ResponseParserStage(parser: ResponseParser | None = None, output_columns: list[str] | None = None)\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[list[ResponseBatch], list[str]], DataFrame]</code></p> <p>Parse LLM responses into structured DataFrame.</p> <p>Responsibilities: - Parse responses using configured parser - Map parsed data to output columns - Handle parse errors gracefully - Return DataFrame with results</p> <p>Initialize response parser stage.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ResponseParser | None</code> <p>Response parser (default: RawTextParser)</p> <code>None</code> <code>output_columns</code> <code>list[str] | None</code> <p>Output column names</p> <code>None</code> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def __init__(\n    self,\n    parser: ResponseParser | None = None,\n    output_columns: list[str] | None = None,\n):\n    \"\"\"\n    Initialize response parser stage.\n\n    Args:\n        parser: Response parser (default: RawTextParser)\n        output_columns: Output column names\n    \"\"\"\n    super().__init__(\"ResponseParser\")\n    self.parser = parser or RawTextParser()\n    self.output_columns = output_columns or [\"output\"]\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage.process","title":"process","text":"<pre><code>process(input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Parse responses into DataFrame.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[list[ResponseBatch], list[str]] | list[ResponseBatch],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Parse responses into DataFrame.\"\"\"\n    # Handle both tuple (batches, output_cols) and list [batches] for backward compatibility\n    if isinstance(input_data, tuple):\n        batches, output_cols = input_data\n        # Use output_cols from input_data (overrides self.output_columns if provided)\n        if not output_cols:\n            output_cols = self.output_columns\n    else:\n        # Backward compatibility: input_data is just the list of batches\n        batches = input_data\n        output_cols = self.output_columns\n\n    # Initialize result storage\n    results: dict[int, dict[str, Any]] = {}\n\n    # Parse all responses\n    for batch in batches:\n        for response, metadata in zip(\n            batch.responses, batch.metadata, strict=False\n        ):\n            try:\n                # Parse response text\n                response_text = (\n                    response.text if hasattr(response, \"text\") else str(response)\n                )\n                parsed = self.parser.parse(response_text)\n\n                # Map to output columns\n                row_data = {}\n                if len(output_cols) == 1:\n                    # Single output column\n                    if isinstance(parsed, dict) and \"output\" in parsed:\n                        row_data[output_cols[0]] = parsed[\"output\"]\n                    elif isinstance(parsed, dict):\n                        # Use first value\n                        row_data[output_cols[0]] = next(iter(parsed.values()))\n                    else:\n                        row_data[output_cols[0]] = parsed\n                else:\n                    # Multiple output columns\n                    for col in output_cols:\n                        row_data[col] = parsed.get(col, None)\n\n                results[metadata.row_index] = row_data\n\n            except Exception as e:\n                self.logger.error(\n                    f\"Failed to parse response at row {metadata.row_index}: {e}\"\n                )\n                # Store None for failed parses\n                results[metadata.row_index] = {col: None for col in output_cols}\n\n    # Create DataFrame\n    df = pd.DataFrame.from_dict(results, orient=\"index\")\n    df.index.name = \"row_index\"\n\n    self.logger.info(f\"Parsed {len(results)} responses\")\n\n    return df\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; ValidationResult\n</code></pre> <p>Validate response batches.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def validate_input(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; ValidationResult:\n    \"\"\"Validate response batches.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    batches, output_cols = input_data\n\n    if not batches:\n        result.add_error(\"No response batches provided\")\n\n    if not output_cols:\n        result.add_error(\"No output columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/response_parser_stage/#ondine.stages.response_parser_stage.ResponseParserStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[list[ResponseBatch], list[str]]) -&gt; CostEstimate\n</code></pre> <p>Response parsing has no LLM cost.</p> Source code in <code>ondine/stages/response_parser_stage.py</code> <pre><code>def estimate_cost(\n    self, input_data: tuple[list[ResponseBatch], list[str]]\n) -&gt; CostEstimate:\n    \"\"\"Response parsing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=sum(len(b.responses) for b in input_data[0]),\n    )\n</code></pre>"},{"location":"api/stages/result_writer_stage/","title":"result_writer_stage","text":""},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage","title":"result_writer_stage","text":"<p>Result writing stage for persisting output.</p>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage","title":"ResultWriterStage","text":"<pre><code>ResultWriterStage()\n</code></pre> <p>               Bases: <code>PipelineStage[tuple[DataFrame, DataFrame, OutputSpec], DataFrame]</code></p> <p>Write results to destination with merge support.</p> <p>Responsibilities: - Merge results with original data - Write to configured destination - Support atomic writes - Return merged DataFrame</p> <p>Initialize result writer stage.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize result writer stage.\"\"\"\n    super().__init__(\"ResultWriter\")\n</code></pre>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage.process","title":"process","text":"<pre><code>process(input_data: tuple[DataFrame, DataFrame, OutputSpec], context: Any) -&gt; pd.DataFrame\n</code></pre> <p>Write results to destination and return merged DataFrame.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def process(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n    context: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Write results to destination and return merged DataFrame.\"\"\"\n    original_df, results_df, output_spec = input_data\n\n    # Merge results with original data\n    merged_df = self._merge_results(\n        original_df, results_df, output_spec.merge_strategy\n    )\n\n    # Write to destination\n    if output_spec.destination_path:\n        writer = create_data_writer(output_spec.destination_type)\n\n        if output_spec.atomic_write:\n            confirmation = writer.atomic_write(\n                merged_df, output_spec.destination_path\n            )\n        else:\n            confirmation = writer.write(merged_df, output_spec.destination_path)\n\n        self.logger.info(\n            f\"Wrote {confirmation.rows_written} rows to {confirmation.path}\"\n        )\n\n    # Always return the merged DataFrame (needed for quality validation)\n    return merged_df\n</code></pre>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; ValidationResult\n</code></pre> <p>Validate input data and output specification.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def validate_input(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; ValidationResult:\n    \"\"\"Validate input data and output specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    original_df, results_df, output_spec = input_data\n\n    if original_df.empty:\n        result.add_warning(\"Original DataFrame is empty\")\n\n    if results_df.empty:\n        result.add_error(\"Results DataFrame is empty\")\n\n    # Check destination path if specified\n    if output_spec.destination_path:\n        dest_dir = output_spec.destination_path.parent\n        if not dest_dir.exists():\n            result.add_warning(f\"Destination directory does not exist: {dest_dir}\")\n\n    return result\n</code></pre>"},{"location":"api/stages/result_writer_stage/#ondine.stages.result_writer_stage.ResultWriterStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(input_data: tuple[DataFrame, DataFrame, OutputSpec]) -&gt; CostEstimate\n</code></pre> <p>Result writing has no LLM cost.</p> Source code in <code>ondine/stages/result_writer_stage.py</code> <pre><code>def estimate_cost(\n    self,\n    input_data: tuple[pd.DataFrame, pd.DataFrame, OutputSpec],\n) -&gt; CostEstimate:\n    \"\"\"Result writing has no LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=len(input_data[1]),\n    )\n</code></pre>"},{"location":"api/stages/stage_registry/","title":"stage_registry","text":""},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry","title":"stage_registry","text":"<p>Stage registry for extensible pipeline stage plugins.</p> <p>Enables custom pipeline stages to be registered and injected into processing pipelines without modifying core code.</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.PipelineStage","title":"PipelineStage","text":"<p>Protocol for pipeline stage implementations (imported to avoid circular dependency).</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry","title":"StageRegistry","text":"<p>Global registry for custom pipeline stages.</p> <p>Enables registration and discovery of custom stages that can be injected into pipelines at specific positions.</p> Example"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry--register-custom-stage","title":"Register custom stage","text":"<p>@StageRegistry.register(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     def execute(self, context):         # Custom retrieval logic         ...</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry--use-in-pipeline","title":"Use in pipeline","text":"<p>pipeline = (     PipelineBuilder.create()     .with_stage(\"rag_retrieval\", position=\"before_prompt\", index=\"my-docs\")     .build() )</p>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(stage_name: str, stage_class: type) -&gt; type\n</code></pre> <p>Register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")</p> required <code>stage_class</code> <code>type</code> <p>Stage class implementing PipelineStage interface</p> required <p>Returns:</p> Type Description <code>type</code> <p>The registered stage class (enables use as decorator)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage_name already registered</p> Example <p>@StageRegistry.register(\"fact_checker\") class FactCheckerStage(PipelineStage):     def execute(self, context):         # Verify LLM output against sources         ...</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef register(cls, stage_name: str, stage_class: type) -&gt; type:\n    \"\"\"\n    Register a custom pipeline stage.\n\n    Args:\n        stage_name: Unique stage identifier (e.g., \"rag_retrieval\", \"content_moderation\")\n        stage_class: Stage class implementing PipelineStage interface\n\n    Returns:\n        The registered stage class (enables use as decorator)\n\n    Raises:\n        ValueError: If stage_name already registered\n\n    Example:\n        @StageRegistry.register(\"fact_checker\")\n        class FactCheckerStage(PipelineStage):\n            def execute(self, context):\n                # Verify LLM output against sources\n                ...\n    \"\"\"\n    if stage_name in cls._stages:\n        raise ValueError(\n            f\"Stage '{stage_name}' already registered. \"\n            f\"Use a different stage_name or unregister first.\"\n        )\n\n    cls._stages[stage_name] = stage_class\n    return stage_class\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(stage_name: str) -&gt; type\n</code></pre> <p>Get stage class by name.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>type</code> <p>Pipeline stage class</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Example <p>stage_class = StageRegistry.get(\"rag_retrieval\") stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef get(cls, stage_name: str) -&gt; type:\n    \"\"\"\n    Get stage class by name.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        Pipeline stage class\n\n    Raises:\n        ValueError: If stage not found\n\n    Example:\n        stage_class = StageRegistry.get(\"rag_retrieval\")\n        stage = stage_class(vector_store=\"pinecone\", index=\"my-docs\")\n    \"\"\"\n    if stage_name not in cls._stages:\n        available = \", \".join(sorted(cls._stages.keys()))\n        raise ValueError(\n            f\"Unknown stage: '{stage_name}'. \"\n            f\"Available stages: {available if available else 'none'}\"\n        )\n\n    return cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.list_stages","title":"list_stages  <code>classmethod</code>","text":"<pre><code>list_stages() -&gt; dict[str, type]\n</code></pre> <p>List all registered stages.</p> <p>Returns:</p> Type Description <code>dict[str, type]</code> <p>Dictionary mapping stage names to stage classes</p> Example <p>stages = StageRegistry.list_stages() print(f\"Available custom stages: {list(stages.keys())}\")</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef list_stages(cls) -&gt; dict[str, type]:\n    \"\"\"\n    List all registered stages.\n\n    Returns:\n        Dictionary mapping stage names to stage classes\n\n    Example:\n        stages = StageRegistry.list_stages()\n        print(f\"Available custom stages: {list(stages.keys())}\")\n    \"\"\"\n    return cls._stages.copy()\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(stage_name: str) -&gt; bool\n</code></pre> <p>Check if stage is registered.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if registered, False otherwise</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef is_registered(cls, stage_name: str) -&gt; bool:\n    \"\"\"\n    Check if stage is registered.\n\n    Args:\n        stage_name: Stage identifier\n\n    Returns:\n        True if registered, False otherwise\n    \"\"\"\n    return stage_name in cls._stages\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.StageRegistry.unregister","title":"unregister  <code>classmethod</code>","text":"<pre><code>unregister(stage_name: str) -&gt; None\n</code></pre> <p>Unregister a stage (mainly for testing).</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Stage identifier</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If stage not found</p> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>@classmethod\ndef unregister(cls, stage_name: str) -&gt; None:\n    \"\"\"\n    Unregister a stage (mainly for testing).\n\n    Args:\n        stage_name: Stage identifier\n\n    Raises:\n        ValueError: If stage not found\n    \"\"\"\n    if stage_name not in cls._stages:\n        raise ValueError(f\"Stage '{stage_name}' not registered\")\n\n    del cls._stages[stage_name]\n</code></pre>"},{"location":"api/stages/stage_registry/#ondine.stages.stage_registry.stage","title":"stage","text":"<pre><code>stage(name: str)\n</code></pre> <p>Decorator to register a custom pipeline stage.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique stage identifier</p> required <p>Returns:</p> Type Description <p>Decorator function</p> Example <p>@stage(\"rag_retrieval\") class RAGRetrievalStage(PipelineStage):     '''Retrieve context from vector store and enrich data.'''</p> <pre><code>def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n    super().__init__(name=\"rag_retrieval\")\n    self.vector_store = vector_store\n    self.index_name = index_name\n    self.top_k = top_k\n\ndef execute(self, context: ExecutionContext) -&gt; StageResult:\n    # Retrieve context for each row\n    enriched_rows = []\n    for _, row in context.data.iterrows():\n        query = row['text']\n        results = self._retrieve(query)\n        row['retrieved_context'] = self._format_context(results)\n        enriched_rows.append(row)\n\n    context.data = pd.DataFrame(enriched_rows)\n    return StageResult(success=True, data=context.data)\n\ndef _retrieve(self, query: str):\n    # Integration with vector store\n    ...\n\ndef _format_context(self, results):\n    # Format retrieved docs\n    ...\n</code></pre> Source code in <code>ondine/stages/stage_registry.py</code> <pre><code>def stage(name: str):\n    \"\"\"\n    Decorator to register a custom pipeline stage.\n\n    Args:\n        name: Unique stage identifier\n\n    Returns:\n        Decorator function\n\n    Example:\n        @stage(\"rag_retrieval\")\n        class RAGRetrievalStage(PipelineStage):\n            '''Retrieve context from vector store and enrich data.'''\n\n            def __init__(self, vector_store: str, index_name: str, top_k: int = 3):\n                super().__init__(name=\"rag_retrieval\")\n                self.vector_store = vector_store\n                self.index_name = index_name\n                self.top_k = top_k\n\n            def execute(self, context: ExecutionContext) -&gt; StageResult:\n                # Retrieve context for each row\n                enriched_rows = []\n                for _, row in context.data.iterrows():\n                    query = row['text']\n                    results = self._retrieve(query)\n                    row['retrieved_context'] = self._format_context(results)\n                    enriched_rows.append(row)\n\n                context.data = pd.DataFrame(enriched_rows)\n                return StageResult(success=True, data=context.data)\n\n            def _retrieve(self, query: str):\n                # Integration with vector store\n                ...\n\n            def _format_context(self, results):\n                # Format retrieved docs\n                ...\n    \"\"\"\n\n    def decorator(cls):\n        StageRegistry.register(name, cls)\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/","title":"streaming_loader_stage","text":""},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage","title":"streaming_loader_stage","text":"<p>Streaming data loader for memory-efficient processing of large files.</p> <p>Implements streaming pattern for datasets that don't fit in memory.</p>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage","title":"StreamingDataLoaderStage","text":"<pre><code>StreamingDataLoaderStage(chunk_size: int = 1000)\n</code></pre> <p>               Bases: <code>PipelineStage[DatasetSpec, Iterator[DataFrame]]</code></p> <p>Load data in chunks for memory-efficient processing.</p> <p>Use this for very large datasets (100K+ rows) that don't fit in memory.</p> <p>Initialize streaming data loader.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Number of rows per chunk</p> <code>1000</code> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def __init__(self, chunk_size: int = 1000):\n    \"\"\"\n    Initialize streaming data loader.\n\n    Args:\n        chunk_size: Number of rows per chunk\n    \"\"\"\n    super().__init__(\"StreamingDataLoader\")\n    self.chunk_size = chunk_size\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage.process","title":"process","text":"<pre><code>process(spec: DatasetSpec, context: Any) -&gt; Iterator[pd.DataFrame]\n</code></pre> <p>Load data as iterator of chunks.</p> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def process(self, spec: DatasetSpec, context: Any) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Load data as iterator of chunks.\"\"\"\n    # Create appropriate reader\n    reader = create_data_reader(\n        source_type=spec.source_type,\n        source_path=spec.source_path,\n        delimiter=spec.delimiter,\n        encoding=spec.encoding,\n        sheet_name=spec.sheet_name,\n    )\n\n    self.logger.info(f\"Streaming data in chunks of {self.chunk_size} rows\")\n\n    # Return chunked iterator\n    return reader.read_chunked(self.chunk_size)\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage.validate_input","title":"validate_input","text":"<pre><code>validate_input(spec: DatasetSpec) -&gt; ValidationResult\n</code></pre> <p>Validate dataset specification.</p> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def validate_input(self, spec: DatasetSpec) -&gt; ValidationResult:\n    \"\"\"Validate dataset specification.\"\"\"\n    result = ValidationResult(is_valid=True)\n\n    # Check file exists for file sources\n    if spec.source_path and not spec.source_path.exists():\n        result.add_error(f\"Source file not found: {spec.source_path}\")\n\n    # Check columns specified\n    if not spec.input_columns:\n        result.add_error(\"No input columns specified\")\n\n    return result\n</code></pre>"},{"location":"api/stages/streaming_loader_stage/#ondine.stages.streaming_loader_stage.StreamingDataLoaderStage.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(spec: DatasetSpec) -&gt; CostEstimate\n</code></pre> <p>Streaming has no direct LLM cost.</p> Source code in <code>ondine/stages/streaming_loader_stage.py</code> <pre><code>def estimate_cost(self, spec: DatasetSpec) -&gt; CostEstimate:\n    \"\"\"Streaming has no direct LLM cost.\"\"\"\n    return CostEstimate(\n        total_cost=Decimal(\"0.0\"),\n        total_tokens=0,\n        input_tokens=0,\n        output_tokens=0,\n        rows=0,  # Unknown until streaming starts\n    )\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#ondine.utils","title":"utils","text":"<p>Utility modules for cross-cutting concerns.</p>"},{"location":"api/utils/#ondine.utils.BudgetController","title":"BudgetController","text":"<pre><code>BudgetController(max_budget: Decimal | None = None, warn_at_75: bool = True, warn_at_90: bool = True, fail_on_exceed: bool = True)\n</code></pre> <p>Controls and enforces budget limits during execution.</p> <p>Follows Single Responsibility: only handles budget management.</p> <p>Initialize budget controller.</p> <p>Parameters:</p> Name Type Description Default <code>max_budget</code> <code>Decimal | None</code> <p>Maximum allowed budget in USD</p> <code>None</code> <code>warn_at_75</code> <code>bool</code> <p>Warn at 75% of budget</p> <code>True</code> <code>warn_at_90</code> <code>bool</code> <p>Warn at 90% of budget</p> <code>True</code> <code>fail_on_exceed</code> <code>bool</code> <p>Raise error if budget exceeded</p> <code>True</code> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def __init__(\n    self,\n    max_budget: Decimal | None = None,\n    warn_at_75: bool = True,\n    warn_at_90: bool = True,\n    fail_on_exceed: bool = True,\n):\n    \"\"\"\n    Initialize budget controller.\n\n    Args:\n        max_budget: Maximum allowed budget in USD\n        warn_at_75: Warn at 75% of budget\n        warn_at_90: Warn at 90% of budget\n        fail_on_exceed: Raise error if budget exceeded\n    \"\"\"\n    self.max_budget = max_budget\n    self.warn_at_75 = warn_at_75\n    self.warn_at_90 = warn_at_90\n    self.fail_on_exceed = fail_on_exceed\n\n    self._warned_75 = False\n    self._warned_90 = False\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.check_budget","title":"check_budget","text":"<pre><code>check_budget(current_cost: Decimal) -&gt; None\n</code></pre> <p>Check if budget is within limits.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Raises:</p> Type Description <code>BudgetExceededError</code> <p>If budget exceeded and fail_on_exceed=True</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def check_budget(self, current_cost: Decimal) -&gt; None:\n    \"\"\"\n    Check if budget is within limits.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Raises:\n        BudgetExceededError: If budget exceeded and fail_on_exceed=True\n    \"\"\"\n    if self.max_budget is None:\n        return\n\n    usage_ratio = float(current_cost / self.max_budget)\n\n    # 75% warning\n    if self.warn_at_75 and not self._warned_75 and usage_ratio &gt;= 0.75:\n        logger.warning(\n            f\"Budget warning: 75% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_75 = True\n\n    # 90% warning\n    if self.warn_at_90 and not self._warned_90 and usage_ratio &gt;= 0.90:\n        logger.warning(\n            f\"Budget warning: 90% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_90 = True\n\n    # Budget exceeded\n    if current_cost &gt; self.max_budget:\n        error_msg = f\"Budget exceeded: ${current_cost:.4f} &gt; ${self.max_budget:.2f}\"\n        logger.error(error_msg)\n\n        if self.fail_on_exceed:\n            raise BudgetExceededError(error_msg)\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.get_remaining","title":"get_remaining","text":"<pre><code>get_remaining(current_cost: Decimal) -&gt; Decimal | None\n</code></pre> <p>Get remaining budget.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>Decimal | None</code> <p>Remaining budget or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_remaining(self, current_cost: Decimal) -&gt; Decimal | None:\n    \"\"\"\n    Get remaining budget.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Remaining budget or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return self.max_budget - current_cost\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.get_usage_percentage","title":"get_usage_percentage","text":"<pre><code>get_usage_percentage(current_cost: Decimal) -&gt; float | None\n</code></pre> <p>Get budget usage as percentage.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>Usage percentage or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_usage_percentage(self, current_cost: Decimal) -&gt; float | None:\n    \"\"\"\n    Get budget usage as percentage.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Usage percentage or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return float(current_cost / self.max_budget) * 100\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetController.can_afford","title":"can_afford","text":"<pre><code>can_afford(estimated_cost: Decimal, current_cost: Decimal) -&gt; bool\n</code></pre> <p>Check if estimated additional cost is within budget.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_cost</code> <code>Decimal</code> <p>Estimated cost for next operation</p> required <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if within budget</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def can_afford(self, estimated_cost: Decimal, current_cost: Decimal) -&gt; bool:\n    \"\"\"\n    Check if estimated additional cost is within budget.\n\n    Args:\n        estimated_cost: Estimated cost for next operation\n        current_cost: Current accumulated cost\n\n    Returns:\n        True if within budget\n    \"\"\"\n    if self.max_budget is None:\n        return True\n    return (current_cost + estimated_cost) &lt;= self.max_budget\n</code></pre>"},{"location":"api/utils/#ondine.utils.BudgetExceededError","title":"BudgetExceededError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when budget limit is exceeded.</p>"},{"location":"api/utils/#ondine.utils.CostCalculator","title":"CostCalculator","text":"<p>Centralized cost calculation for LLM API usage.</p> <p>Single Responsibility: Calculate cost from token counts and pricing. Used by: LLMClient, CostTracker, and any component needing cost calculation.</p> <p>Design Decision: Centralize the cost formula in one place to ensure consistency and make future changes (e.g., tiered pricing) easier.</p>"},{"location":"api/utils/#ondine.utils.CostCalculator.calculate","title":"calculate  <code>staticmethod</code>","text":"<pre><code>calculate(tokens_in: int, tokens_out: int, input_cost_per_1k: Decimal, output_cost_per_1k: Decimal) -&gt; Decimal\n</code></pre> <p>Calculate cost from token counts and pricing.</p> Formula <p>cost = (tokens_in / 1000) * input_cost_per_1k +        (tokens_out / 1000) * output_cost_per_1k</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Number of input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Number of output tokens</p> required <code>input_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 input tokens</p> required <code>output_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost as Decimal (exact precision for financial calculations)</p> Example <p>from decimal import Decimal cost = CostCalculator.calculate( ...     tokens_in=1000, ...     tokens_out=500, ...     input_cost_per_1k=Decimal(\"0.00005\"), ...     output_cost_per_1k=Decimal(\"0.00008\") ... ) cost Decimal('0.00009')</p> Source code in <code>ondine/utils/cost_calculator.py</code> <pre><code>@staticmethod\ndef calculate(\n    tokens_in: int,\n    tokens_out: int,\n    input_cost_per_1k: Decimal,\n    output_cost_per_1k: Decimal,\n) -&gt; Decimal:\n    \"\"\"\n    Calculate cost from token counts and pricing.\n\n    Formula:\n        cost = (tokens_in / 1000) * input_cost_per_1k +\n               (tokens_out / 1000) * output_cost_per_1k\n\n    Args:\n        tokens_in: Number of input tokens\n        tokens_out: Number of output tokens\n        input_cost_per_1k: Cost per 1000 input tokens\n        output_cost_per_1k: Cost per 1000 output tokens\n\n    Returns:\n        Total cost as Decimal (exact precision for financial calculations)\n\n    Example:\n        &gt;&gt;&gt; from decimal import Decimal\n        &gt;&gt;&gt; cost = CostCalculator.calculate(\n        ...     tokens_in=1000,\n        ...     tokens_out=500,\n        ...     input_cost_per_1k=Decimal(\"0.00005\"),\n        ...     output_cost_per_1k=Decimal(\"0.00008\")\n        ... )\n        &gt;&gt;&gt; cost\n        Decimal('0.00009')\n    \"\"\"\n    input_cost = (Decimal(tokens_in) / 1000) * input_cost_per_1k\n    output_cost = (Decimal(tokens_out) / 1000) * output_cost_per_1k\n    return input_cost + output_cost\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker","title":"CostTracker","text":"<pre><code>CostTracker(input_cost_per_1k: Decimal | None = None, output_cost_per_1k: Decimal | None = None)\n</code></pre> <p>Detailed cost accounting with thread-safety and per-stage breakdowns.</p> <p>Scope: Detailed financial tracking and reporting Pattern: Accumulator with thread-safe operations</p> <p>Use CostTracker for: - Stage-by-stage cost breakdowns - Detailed entry logging (timestamp, model, tokens) - Thread-safe accumulation in concurrent execution - Cost reporting and analytics - Budget enforcement (via BudgetController)</p> <p>NOT for: - Simple orchestration state (use ExecutionContext for that)</p> <p>Why separate from ExecutionContext? - CostTracker = detailed accounting system (entries, breakdowns, thread-safety) - ExecutionContext = orchestration state (progress, session, timing) - Different concerns: accounting vs execution control</p> <p>Thread Safety: - All methods protected by threading.Lock - Safe for concurrent LLM invocations</p> Example <p>tracker = CostTracker(     input_cost_per_1k=Decimal(\"0.00015\"),     output_cost_per_1k=Decimal(\"0.0006\") ) cost = tracker.add(tokens_in=1000, tokens_out=500, model=\"gpt-4o-mini\") breakdown = tracker.get_stage_costs()  # {\"llm_invocation\": Decimal(\"0.00045\")}</p> <p>See Also: - ExecutionContext: For orchestration-level state - BudgetController: For cost limit enforcement - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Initialize cost tracker.</p> <p>Parameters:</p> Name Type Description Default <code>input_cost_per_1k</code> <code>Decimal | None</code> <p>Input token cost per 1K tokens</p> <code>None</code> <code>output_cost_per_1k</code> <code>Decimal | None</code> <p>Output token cost per 1K tokens</p> <code>None</code> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def __init__(\n    self,\n    input_cost_per_1k: Decimal | None = None,\n    output_cost_per_1k: Decimal | None = None,\n):\n    \"\"\"\n    Initialize cost tracker.\n\n    Args:\n        input_cost_per_1k: Input token cost per 1K tokens\n        output_cost_per_1k: Output token cost per 1K tokens\n    \"\"\"\n    self.input_cost_per_1k = input_cost_per_1k or Decimal(\"0.0\")\n    self.output_cost_per_1k = output_cost_per_1k or Decimal(\"0.0\")\n\n    self._total_input_tokens = 0\n    self._total_output_tokens = 0\n    self._total_cost = Decimal(\"0.0\")\n    self._entries: list[CostEntry] = []\n    self._stage_costs: dict[str, Decimal] = {}\n    self._lock = threading.Lock()\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.total_cost","title":"total_cost  <code>property</code>","text":"<pre><code>total_cost: Decimal\n</code></pre> <p>Get total accumulated cost.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.total_tokens","title":"total_tokens  <code>property</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Get total token count.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Get total input tokens.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Get total output tokens.</p>"},{"location":"api/utils/#ondine.utils.CostTracker.add","title":"add","text":"<pre><code>add(tokens_in: int, tokens_out: int, model: str, timestamp: float, stage: str | None = None) -&gt; Decimal\n</code></pre> <p>Add cost entry.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens used</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens used</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>timestamp</code> <code>float</code> <p>Timestamp of request</p> required <code>stage</code> <code>str | None</code> <p>Optional stage name</p> <code>None</code> <p>Returns:</p> Type Description <code>Decimal</code> <p>Cost for this entry</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def add(\n    self,\n    tokens_in: int,\n    tokens_out: int,\n    model: str,\n    timestamp: float,\n    stage: str | None = None,\n) -&gt; Decimal:\n    \"\"\"\n    Add cost entry.\n\n    Args:\n        tokens_in: Input tokens used\n        tokens_out: Output tokens used\n        model: Model identifier\n        timestamp: Timestamp of request\n        stage: Optional stage name\n\n    Returns:\n        Cost for this entry\n    \"\"\"\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    with self._lock:\n        entry = CostEntry(\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            cost=cost,\n            model=model,\n            timestamp=timestamp,\n        )\n        self._entries.append(entry)\n\n        self._total_input_tokens += tokens_in\n        self._total_output_tokens += tokens_out\n        self._total_cost += cost\n\n        if stage:\n            self._stage_costs[stage] = (\n                self._stage_costs.get(stage, Decimal(\"0.0\")) + cost\n            )\n\n    return cost\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for given token counts.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for given token counts.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.input_cost_per_1k,\n        output_cost_per_1k=self.output_cost_per_1k,\n    )\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.get_estimate","title":"get_estimate","text":"<pre><code>get_estimate(rows: int = 0) -&gt; CostEstimate\n</code></pre> <p>Get cost estimate.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Number of rows processed</p> <code>0</code> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>CostEstimate object</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_estimate(self, rows: int = 0) -&gt; CostEstimate:\n    \"\"\"\n    Get cost estimate.\n\n    Args:\n        rows: Number of rows processed\n\n    Returns:\n        CostEstimate object\n    \"\"\"\n    with self._lock:\n        total_tokens = self._total_input_tokens + self._total_output_tokens\n        return CostEstimate(\n            total_cost=self._total_cost,\n            total_tokens=total_tokens,\n            input_tokens=self._total_input_tokens,\n            output_tokens=self._total_output_tokens,\n            rows=rows,\n            breakdown_by_stage=dict(self._stage_costs),\n            confidence=\"actual\",\n        )\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset all tracking.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all tracking.\"\"\"\n    with self._lock:\n        self._total_input_tokens = 0\n        self._total_output_tokens = 0\n        self._total_cost = Decimal(\"0.0\")\n        self._entries.clear()\n        self._stage_costs.clear()\n</code></pre>"},{"location":"api/utils/#ondine.utils.CostTracker.get_stage_costs","title":"get_stage_costs","text":"<pre><code>get_stage_costs() -&gt; dict[str, Decimal]\n</code></pre> <p>Get costs breakdown by stage.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_stage_costs(self) -&gt; dict[str, Decimal]:\n    \"\"\"Get costs breakdown by stage.\"\"\"\n    with self._lock:\n        return dict(self._stage_costs)\n</code></pre>"},{"location":"api/utils/#ondine.utils.PreprocessingStats","title":"PreprocessingStats  <code>dataclass</code>","text":"<pre><code>PreprocessingStats(rows_processed: int, chars_before: int, chars_after: int, truncated_count: int, null_count: int)\n</code></pre> <p>Statistics from preprocessing operation.</p>"},{"location":"api/utils/#ondine.utils.PreprocessingStats.reduction_pct","title":"reduction_pct  <code>property</code>","text":"<pre><code>reduction_pct: float\n</code></pre> <p>Calculate character reduction percentage.</p>"},{"location":"api/utils/#ondine.utils.TextPreprocessor","title":"TextPreprocessor","text":"<pre><code>TextPreprocessor(max_length: int = 500)\n</code></pre> <p>Composable text preprocessor following Chain of Responsibility.</p> <p>Single Responsibility: Orchestrate cleaning steps. Open/Closed: Extensible via cleaners list. Dependency Inversion: Depends on Protocol, not concrete classes.</p> <p>Initialize with default cleaning pipeline.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, max_length: int = 500):\n    \"\"\"Initialize with default cleaning pipeline.\"\"\"\n    self.cleaners: list[TextCleaner] = [\n        UnicodeNormalizer(),\n        ControlCharRemover(),\n        SpecialCharCleaner(),\n        WhitespaceNormalizer(),\n        TextTruncator(max_length),\n    ]\n</code></pre>"},{"location":"api/utils/#ondine.utils.TextPreprocessor.process","title":"process","text":"<pre><code>process(text: str) -&gt; str\n</code></pre> <p>Apply all cleaners in sequence.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"Apply all cleaners in sequence.\"\"\"\n    if pd.isna(text) or not isinstance(text, str):\n        return \"\"\n\n    for cleaner in self.cleaners:\n        text = cleaner.clean(text)\n\n    return text\n</code></pre>"},{"location":"api/utils/#ondine.utils.TextPreprocessor.add_cleaner","title":"add_cleaner","text":"<pre><code>add_cleaner(cleaner: TextCleaner) -&gt; None\n</code></pre> <p>Extend pipeline with custom cleaner.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def add_cleaner(self, cleaner: TextCleaner) -&gt; None:\n    \"\"\"Extend pipeline with custom cleaner.\"\"\"\n    self.cleaners.append(cleaner)\n</code></pre>"},{"location":"api/utils/#ondine.utils.RateLimiter","title":"RateLimiter","text":"<pre><code>RateLimiter(requests_per_minute: int, burst_size: int | None = None)\n</code></pre> <p>Token bucket rate limiter for controlling API request rates.</p> <p>Thread-safe implementation.</p> <p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_minute</code> <code>int</code> <p>Maximum requests per minute</p> required <code>burst_size</code> <code>int | None</code> <p>Maximum burst size (default: requests_per_minute)</p> <code>None</code> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def __init__(self, requests_per_minute: int, burst_size: int | None = None):\n    \"\"\"\n    Initialize rate limiter.\n\n    Args:\n        requests_per_minute: Maximum requests per minute\n        burst_size: Maximum burst size (default: requests_per_minute)\n    \"\"\"\n    self.rpm = requests_per_minute\n    self.capacity = burst_size or requests_per_minute\n    self.tokens = float(self.capacity)\n    self.last_update = time.time()\n    self.lock = threading.Lock()\n\n    # Calculate refill rate (tokens per second)\n    self.refill_rate = requests_per_minute / 60.0\n</code></pre>"},{"location":"api/utils/#ondine.utils.RateLimiter.available_tokens","title":"available_tokens  <code>property</code>","text":"<pre><code>available_tokens: float\n</code></pre> <p>Get current available tokens.</p>"},{"location":"api/utils/#ondine.utils.RateLimiter.acquire","title":"acquire","text":"<pre><code>acquire(tokens: int = 1, timeout: float | None = None) -&gt; bool\n</code></pre> <p>Acquire tokens for making requests.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>int</code> <p>Number of tokens to acquire</p> <code>1</code> <code>timeout</code> <code>float | None</code> <p>Maximum wait time in seconds (None = wait forever)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if tokens acquired, False if timeout</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokens &gt; capacity</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def acquire(self, tokens: int = 1, timeout: float | None = None) -&gt; bool:\n    \"\"\"\n    Acquire tokens for making requests.\n\n    Args:\n        tokens: Number of tokens to acquire\n        timeout: Maximum wait time in seconds (None = wait forever)\n\n    Returns:\n        True if tokens acquired, False if timeout\n\n    Raises:\n        ValueError: If tokens &gt; capacity\n    \"\"\"\n    if tokens &gt; self.capacity:\n        raise ValueError(\n            f\"Requested {tokens} tokens exceeds capacity {self.capacity}\"\n        )\n\n    deadline = None if timeout is None else time.time() + timeout\n\n    while True:\n        with self.lock:\n            self._refill()\n\n            if self.tokens &gt;= tokens:\n                self.tokens -= tokens\n                return True\n\n        # Check timeout\n        if deadline is not None and time.time() &gt;= deadline:\n            return False\n\n        # Sleep before retry\n        time.sleep(0.1)\n</code></pre>"},{"location":"api/utils/#ondine.utils.RateLimiter.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset rate limiter to full capacity.</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset rate limiter to full capacity.\"\"\"\n    with self.lock:\n        self.tokens = float(self.capacity)\n        self.last_update = time.time()\n</code></pre>"},{"location":"api/utils/#ondine.utils.NetworkError","title":"NetworkError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Network-related error.</p>"},{"location":"api/utils/#ondine.utils.RateLimitError","title":"RateLimitError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Rate limit exceeded error.</p>"},{"location":"api/utils/#ondine.utils.RetryableError","title":"RetryableError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors that should be retried.</p>"},{"location":"api/utils/#ondine.utils.RetryHandler","title":"RetryHandler","text":"<pre><code>RetryHandler(max_attempts: int = 3, initial_delay: float = 1.0, max_delay: float = 60.0, exponential_base: int = 2, retryable_exceptions: tuple[type[Exception], ...] | None = None)\n</code></pre> <p>Request-level retry with exponential backoff (for transient errors).</p> <p>Scope: Single LLM API call or operation Use when: Transient errors (rate limits, network timeouts, API hiccups) NOT for: Row-level quality issues (use Pipeline.auto_retry_failed for that)</p> <p>Retry Strategy: - Exponential backoff (1s, 2s, 4s, 8s, ...) - Configurable max attempts (default: 3) - Only retries specific exception types</p> Example <p>handler = RetryHandler(max_attempts=3, initial_delay=1.0) result = handler.execute(lambda: call_llm_api())</p> <p>See Also: - ErrorHandler: Orchestrates retry decisions based on policy - Pipeline._auto_retry_failed_rows(): Row-level quality retry - docs/architecture/decisions/ADR-006-retry-levels.md</p> <p>Initialize retry handler.</p> <p>Parameters:</p> Name Type Description Default <code>max_attempts</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay in seconds</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds</p> <code>60.0</code> <code>exponential_base</code> <code>int</code> <p>Base for exponential backoff</p> <code>2</code> <code>retryable_exceptions</code> <code>tuple[type[Exception], ...] | None</code> <p>Exception types to retry</p> <code>None</code> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def __init__(\n    self,\n    max_attempts: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: int = 2,\n    retryable_exceptions: tuple[type[Exception], ...] | None = None,\n):\n    \"\"\"\n    Initialize retry handler.\n\n    Args:\n        max_attempts: Maximum retry attempts\n        initial_delay: Initial delay in seconds\n        max_delay: Maximum delay in seconds\n        exponential_base: Base for exponential backoff\n        retryable_exceptions: Exception types to retry\n    \"\"\"\n    self.max_attempts = max_attempts\n    self.initial_delay = initial_delay\n    self.max_delay = max_delay\n    self.exponential_base = exponential_base\n\n    if retryable_exceptions is None:\n        self.retryable_exceptions = (\n            RetryableError,\n            RateLimitError,\n            NetworkError,\n        )\n    else:\n        self.retryable_exceptions = retryable_exceptions\n</code></pre>"},{"location":"api/utils/#ondine.utils.RetryHandler.execute","title":"execute","text":"<pre><code>execute(func: Callable[[], T]) -&gt; T\n</code></pre> <p>Execute function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[], T]</code> <p>Function to execute</p> required <p>Returns:</p> Type Description <code>T</code> <p>Result from function</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If all retries exhausted</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def execute(self, func: Callable[[], T]) -&gt; T:\n    \"\"\"\n    Execute function with retry logic.\n\n    Args:\n        func: Function to execute\n\n    Returns:\n        Result from function\n\n    Raises:\n        Exception: If all retries exhausted\n    \"\"\"\n    retryer = Retrying(\n        stop=stop_after_attempt(self.max_attempts),\n        wait=wait_exponential(\n            multiplier=self.initial_delay,\n            max=self.max_delay,\n            exp_base=self.exponential_base,\n        ),\n        retry=retry_if_exception_type(self.retryable_exceptions),\n        reraise=True,\n    )\n\n    return retryer(func)\n</code></pre>"},{"location":"api/utils/#ondine.utils.RetryHandler.calculate_delay","title":"calculate_delay","text":"<pre><code>calculate_delay(attempt: int) -&gt; float\n</code></pre> <p>Calculate delay for given attempt number.</p> <p>Parameters:</p> Name Type Description Default <code>attempt</code> <code>int</code> <p>Attempt number (1-based)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def calculate_delay(self, attempt: int) -&gt; float:\n    \"\"\"\n    Calculate delay for given attempt number.\n\n    Args:\n        attempt: Attempt number (1-based)\n\n    Returns:\n        Delay in seconds\n    \"\"\"\n    delay = self.initial_delay * (self.exponential_base ** (attempt - 1))\n    return min(delay, self.max_delay)\n</code></pre>"},{"location":"api/utils/#ondine.utils.preprocess_dataframe","title":"preprocess_dataframe","text":"<pre><code>preprocess_dataframe(df: DataFrame, input_columns: list[str], max_length: int = 500) -&gt; tuple[pd.DataFrame, PreprocessingStats]\n</code></pre> <p>Preprocess input columns in dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe</p> required <code>input_columns</code> <code>list[str]</code> <p>Columns to clean</p> required <code>max_length</code> <code>int</code> <p>Max chars per field</p> <code>500</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, PreprocessingStats]</code> <p>(cleaned_df, stats)</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def preprocess_dataframe(\n    df: pd.DataFrame,\n    input_columns: list[str],\n    max_length: int = 500,\n) -&gt; tuple[pd.DataFrame, PreprocessingStats]:\n    \"\"\"\n    Preprocess input columns in dataframe.\n\n    Args:\n        df: Input dataframe\n        input_columns: Columns to clean\n        max_length: Max chars per field\n\n    Returns:\n        (cleaned_df, stats)\n    \"\"\"\n    result = df.copy()\n    preprocessor = TextPreprocessor(max_length)\n\n    chars_before = 0\n    chars_after = 0\n    truncated = 0\n    nulls = 0\n\n    for col in input_columns:\n        if col not in result.columns:\n            continue\n\n        for idx in result.index:\n            original = result.at[idx, col]\n\n            if pd.isna(original):\n                nulls += 1\n                continue\n\n            original_str = str(original)\n            chars_before += len(original_str)\n\n            cleaned = preprocessor.process(original_str)\n            chars_after += len(cleaned)\n\n            if len(original_str) &gt; max_length:\n                truncated += 1\n\n            result.at[idx, col] = cleaned\n\n    stats = PreprocessingStats(\n        rows_processed=len(result),\n        chars_before=chars_before,\n        chars_after=chars_after,\n        truncated_count=truncated,\n        null_count=nulls,\n    )\n\n    return result, stats\n</code></pre>"},{"location":"api/utils/#ondine.utils.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(level: str = 'INFO', json_format: bool = False, include_timestamp: bool = True) -&gt; None\n</code></pre> <p>Configure structured logging for the SDK.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> <code>json_format</code> <code>bool</code> <p>Use JSON output format</p> <code>False</code> <code>include_timestamp</code> <code>bool</code> <p>Include timestamps in logs</p> <code>True</code> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def configure_logging(\n    level: str = \"INFO\",\n    json_format: bool = False,\n    include_timestamp: bool = True,\n) -&gt; None:\n    \"\"\"\n    Configure structured logging for the SDK.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        json_format: Use JSON output format\n        include_timestamp: Include timestamps in logs\n    \"\"\"\n    global _logging_configured\n\n    # Set stdlib logging level\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=getattr(logging, level.upper()),\n    )\n\n    # Configure structlog processors\n    processors = [\n        structlog.contextvars.merge_contextvars,\n        structlog.processors.add_log_level,\n        structlog.processors.StackInfoRenderer(),\n    ]\n\n    if include_timestamp:\n        processors.append(structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"))\n\n    if json_format:\n        processors.append(structlog.processors.JSONRenderer())\n    else:\n        # Use custom compact console renderer (no padding)\n        processors.append(_compact_console_renderer)\n\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.make_filtering_bound_logger(\n            getattr(logging, level.upper())\n        ),\n        context_class=dict,\n        logger_factory=structlog.PrintLoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    _logging_configured = True\n</code></pre>"},{"location":"api/utils/#ondine.utils.get_logger","title":"get_logger","text":"<pre><code>get_logger(name: str) -&gt; structlog.BoundLogger\n</code></pre> <p>Get a structured logger instance.</p> <p>Auto-configures logging on first use if not already configured.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically name)</p> required <p>Returns:</p> Type Description <code>BoundLogger</code> <p>Configured structlog logger</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def get_logger(name: str) -&gt; structlog.BoundLogger:\n    \"\"\"\n    Get a structured logger instance.\n\n    Auto-configures logging on first use if not already configured.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured structlog logger\n    \"\"\"\n    global _logging_configured\n\n    # Auto-configure logging on first use\n    if not _logging_configured:\n        configure_logging()\n\n    return structlog.get_logger(name)\n</code></pre>"},{"location":"api/utils/#ondine.utils.sanitize_for_logging","title":"sanitize_for_logging","text":"<pre><code>sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Sanitize sensitive data for logging.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary potentially containing sensitive data</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Sanitized dictionary</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Sanitize sensitive data for logging.\n\n    Args:\n        data: Dictionary potentially containing sensitive data\n\n    Returns:\n        Sanitized dictionary\n    \"\"\"\n    sensitive_keys = {\n        \"api_key\",\n        \"password\",\n        \"secret\",\n        \"token\",\n        \"authorization\",\n        \"credential\",\n    }\n\n    sanitized = {}\n    for key, value in data.items():\n        key_lower = key.lower()\n        if any(sensitive in key_lower for sensitive in sensitive_keys):\n            sanitized[key] = \"***REDACTED***\"\n        elif isinstance(value, dict):\n            sanitized[key] = sanitize_for_logging(value)\n        else:\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"api/utils/budget_controller/","title":"budget_controller","text":""},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller","title":"budget_controller","text":"<p>Budget control and enforcement for LLM costs.</p> <p>Implements cost monitoring with threshold warnings and hard limits.</p>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetExceededError","title":"BudgetExceededError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when budget limit is exceeded.</p>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController","title":"BudgetController","text":"<pre><code>BudgetController(max_budget: Decimal | None = None, warn_at_75: bool = True, warn_at_90: bool = True, fail_on_exceed: bool = True)\n</code></pre> <p>Controls and enforces budget limits during execution.</p> <p>Follows Single Responsibility: only handles budget management.</p> <p>Initialize budget controller.</p> <p>Parameters:</p> Name Type Description Default <code>max_budget</code> <code>Decimal | None</code> <p>Maximum allowed budget in USD</p> <code>None</code> <code>warn_at_75</code> <code>bool</code> <p>Warn at 75% of budget</p> <code>True</code> <code>warn_at_90</code> <code>bool</code> <p>Warn at 90% of budget</p> <code>True</code> <code>fail_on_exceed</code> <code>bool</code> <p>Raise error if budget exceeded</p> <code>True</code> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def __init__(\n    self,\n    max_budget: Decimal | None = None,\n    warn_at_75: bool = True,\n    warn_at_90: bool = True,\n    fail_on_exceed: bool = True,\n):\n    \"\"\"\n    Initialize budget controller.\n\n    Args:\n        max_budget: Maximum allowed budget in USD\n        warn_at_75: Warn at 75% of budget\n        warn_at_90: Warn at 90% of budget\n        fail_on_exceed: Raise error if budget exceeded\n    \"\"\"\n    self.max_budget = max_budget\n    self.warn_at_75 = warn_at_75\n    self.warn_at_90 = warn_at_90\n    self.fail_on_exceed = fail_on_exceed\n\n    self._warned_75 = False\n    self._warned_90 = False\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.check_budget","title":"check_budget","text":"<pre><code>check_budget(current_cost: Decimal) -&gt; None\n</code></pre> <p>Check if budget is within limits.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Raises:</p> Type Description <code>BudgetExceededError</code> <p>If budget exceeded and fail_on_exceed=True</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def check_budget(self, current_cost: Decimal) -&gt; None:\n    \"\"\"\n    Check if budget is within limits.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Raises:\n        BudgetExceededError: If budget exceeded and fail_on_exceed=True\n    \"\"\"\n    if self.max_budget is None:\n        return\n\n    usage_ratio = float(current_cost / self.max_budget)\n\n    # 75% warning\n    if self.warn_at_75 and not self._warned_75 and usage_ratio &gt;= 0.75:\n        logger.warning(\n            f\"Budget warning: 75% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_75 = True\n\n    # 90% warning\n    if self.warn_at_90 and not self._warned_90 and usage_ratio &gt;= 0.90:\n        logger.warning(\n            f\"Budget warning: 90% used \"\n            f\"(${current_cost:.4f} / ${self.max_budget:.2f})\"\n        )\n        self._warned_90 = True\n\n    # Budget exceeded\n    if current_cost &gt; self.max_budget:\n        error_msg = f\"Budget exceeded: ${current_cost:.4f} &gt; ${self.max_budget:.2f}\"\n        logger.error(error_msg)\n\n        if self.fail_on_exceed:\n            raise BudgetExceededError(error_msg)\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.get_remaining","title":"get_remaining","text":"<pre><code>get_remaining(current_cost: Decimal) -&gt; Decimal | None\n</code></pre> <p>Get remaining budget.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>Decimal | None</code> <p>Remaining budget or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_remaining(self, current_cost: Decimal) -&gt; Decimal | None:\n    \"\"\"\n    Get remaining budget.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Remaining budget or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return self.max_budget - current_cost\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.get_usage_percentage","title":"get_usage_percentage","text":"<pre><code>get_usage_percentage(current_cost: Decimal) -&gt; float | None\n</code></pre> <p>Get budget usage as percentage.</p> <p>Parameters:</p> Name Type Description Default <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>float | None</code> <p>Usage percentage or None if no limit</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def get_usage_percentage(self, current_cost: Decimal) -&gt; float | None:\n    \"\"\"\n    Get budget usage as percentage.\n\n    Args:\n        current_cost: Current accumulated cost\n\n    Returns:\n        Usage percentage or None if no limit\n    \"\"\"\n    if self.max_budget is None:\n        return None\n    return float(current_cost / self.max_budget) * 100\n</code></pre>"},{"location":"api/utils/budget_controller/#ondine.utils.budget_controller.BudgetController.can_afford","title":"can_afford","text":"<pre><code>can_afford(estimated_cost: Decimal, current_cost: Decimal) -&gt; bool\n</code></pre> <p>Check if estimated additional cost is within budget.</p> <p>Parameters:</p> Name Type Description Default <code>estimated_cost</code> <code>Decimal</code> <p>Estimated cost for next operation</p> required <code>current_cost</code> <code>Decimal</code> <p>Current accumulated cost</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if within budget</p> Source code in <code>ondine/utils/budget_controller.py</code> <pre><code>def can_afford(self, estimated_cost: Decimal, current_cost: Decimal) -&gt; bool:\n    \"\"\"\n    Check if estimated additional cost is within budget.\n\n    Args:\n        estimated_cost: Estimated cost for next operation\n        current_cost: Current accumulated cost\n\n    Returns:\n        True if within budget\n    \"\"\"\n    if self.max_budget is None:\n        return True\n    return (current_cost + estimated_cost) &lt;= self.max_budget\n</code></pre>"},{"location":"api/utils/cost_calculator/","title":"cost_calculator","text":""},{"location":"api/utils/cost_calculator/#ondine.utils.cost_calculator","title":"cost_calculator","text":"<p>Centralized cost calculation utilities.</p> <p>Provides single source of truth for LLM cost calculation formula, eliminating duplication across LLMClient and CostTracker.</p>"},{"location":"api/utils/cost_calculator/#ondine.utils.cost_calculator.CostCalculator","title":"CostCalculator","text":"<p>Centralized cost calculation for LLM API usage.</p> <p>Single Responsibility: Calculate cost from token counts and pricing. Used by: LLMClient, CostTracker, and any component needing cost calculation.</p> <p>Design Decision: Centralize the cost formula in one place to ensure consistency and make future changes (e.g., tiered pricing) easier.</p>"},{"location":"api/utils/cost_calculator/#ondine.utils.cost_calculator.CostCalculator.calculate","title":"calculate  <code>staticmethod</code>","text":"<pre><code>calculate(tokens_in: int, tokens_out: int, input_cost_per_1k: Decimal, output_cost_per_1k: Decimal) -&gt; Decimal\n</code></pre> <p>Calculate cost from token counts and pricing.</p> Formula <p>cost = (tokens_in / 1000) * input_cost_per_1k +        (tokens_out / 1000) * output_cost_per_1k</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Number of input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Number of output tokens</p> required <code>input_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 input tokens</p> required <code>output_cost_per_1k</code> <code>Decimal</code> <p>Cost per 1000 output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost as Decimal (exact precision for financial calculations)</p> Example <p>from decimal import Decimal cost = CostCalculator.calculate( ...     tokens_in=1000, ...     tokens_out=500, ...     input_cost_per_1k=Decimal(\"0.00005\"), ...     output_cost_per_1k=Decimal(\"0.00008\") ... ) cost Decimal('0.00009')</p> Source code in <code>ondine/utils/cost_calculator.py</code> <pre><code>@staticmethod\ndef calculate(\n    tokens_in: int,\n    tokens_out: int,\n    input_cost_per_1k: Decimal,\n    output_cost_per_1k: Decimal,\n) -&gt; Decimal:\n    \"\"\"\n    Calculate cost from token counts and pricing.\n\n    Formula:\n        cost = (tokens_in / 1000) * input_cost_per_1k +\n               (tokens_out / 1000) * output_cost_per_1k\n\n    Args:\n        tokens_in: Number of input tokens\n        tokens_out: Number of output tokens\n        input_cost_per_1k: Cost per 1000 input tokens\n        output_cost_per_1k: Cost per 1000 output tokens\n\n    Returns:\n        Total cost as Decimal (exact precision for financial calculations)\n\n    Example:\n        &gt;&gt;&gt; from decimal import Decimal\n        &gt;&gt;&gt; cost = CostCalculator.calculate(\n        ...     tokens_in=1000,\n        ...     tokens_out=500,\n        ...     input_cost_per_1k=Decimal(\"0.00005\"),\n        ...     output_cost_per_1k=Decimal(\"0.00008\")\n        ... )\n        &gt;&gt;&gt; cost\n        Decimal('0.00009')\n    \"\"\"\n    input_cost = (Decimal(tokens_in) / 1000) * input_cost_per_1k\n    output_cost = (Decimal(tokens_out) / 1000) * output_cost_per_1k\n    return input_cost + output_cost\n</code></pre>"},{"location":"api/utils/cost_tracker/","title":"cost_tracker","text":""},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker","title":"cost_tracker","text":"<p>Cost tracking for LLM API calls.</p> <p>Provides accurate cost tracking with thread safety and detailed breakdowns.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostEntry","title":"CostEntry  <code>dataclass</code>","text":"<pre><code>CostEntry(tokens_in: int, tokens_out: int, cost: Decimal, model: str, timestamp: float)\n</code></pre> <p>Single cost tracking entry.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker","title":"CostTracker","text":"<pre><code>CostTracker(input_cost_per_1k: Decimal | None = None, output_cost_per_1k: Decimal | None = None)\n</code></pre> <p>Detailed cost accounting with thread-safety and per-stage breakdowns.</p> <p>Scope: Detailed financial tracking and reporting Pattern: Accumulator with thread-safe operations</p> <p>Use CostTracker for: - Stage-by-stage cost breakdowns - Detailed entry logging (timestamp, model, tokens) - Thread-safe accumulation in concurrent execution - Cost reporting and analytics - Budget enforcement (via BudgetController)</p> <p>NOT for: - Simple orchestration state (use ExecutionContext for that)</p> <p>Why separate from ExecutionContext? - CostTracker = detailed accounting system (entries, breakdowns, thread-safety) - ExecutionContext = orchestration state (progress, session, timing) - Different concerns: accounting vs execution control</p> <p>Thread Safety: - All methods protected by threading.Lock - Safe for concurrent LLM invocations</p> Example <p>tracker = CostTracker(     input_cost_per_1k=Decimal(\"0.00015\"),     output_cost_per_1k=Decimal(\"0.0006\") ) cost = tracker.add(tokens_in=1000, tokens_out=500, model=\"gpt-4o-mini\") breakdown = tracker.get_stage_costs()  # {\"llm_invocation\": Decimal(\"0.00045\")}</p> <p>See Also: - ExecutionContext: For orchestration-level state - BudgetController: For cost limit enforcement - docs/TECHNICAL_REFERENCE.md: Cost tracking architecture</p> <p>Initialize cost tracker.</p> <p>Parameters:</p> Name Type Description Default <code>input_cost_per_1k</code> <code>Decimal | None</code> <p>Input token cost per 1K tokens</p> <code>None</code> <code>output_cost_per_1k</code> <code>Decimal | None</code> <p>Output token cost per 1K tokens</p> <code>None</code> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def __init__(\n    self,\n    input_cost_per_1k: Decimal | None = None,\n    output_cost_per_1k: Decimal | None = None,\n):\n    \"\"\"\n    Initialize cost tracker.\n\n    Args:\n        input_cost_per_1k: Input token cost per 1K tokens\n        output_cost_per_1k: Output token cost per 1K tokens\n    \"\"\"\n    self.input_cost_per_1k = input_cost_per_1k or Decimal(\"0.0\")\n    self.output_cost_per_1k = output_cost_per_1k or Decimal(\"0.0\")\n\n    self._total_input_tokens = 0\n    self._total_output_tokens = 0\n    self._total_cost = Decimal(\"0.0\")\n    self._entries: list[CostEntry] = []\n    self._stage_costs: dict[str, Decimal] = {}\n    self._lock = threading.Lock()\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.total_cost","title":"total_cost  <code>property</code>","text":"<pre><code>total_cost: Decimal\n</code></pre> <p>Get total accumulated cost.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.total_tokens","title":"total_tokens  <code>property</code>","text":"<pre><code>total_tokens: int\n</code></pre> <p>Get total token count.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Get total input tokens.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Get total output tokens.</p>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.add","title":"add","text":"<pre><code>add(tokens_in: int, tokens_out: int, model: str, timestamp: float, stage: str | None = None) -&gt; Decimal\n</code></pre> <p>Add cost entry.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens used</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens used</p> required <code>model</code> <code>str</code> <p>Model identifier</p> required <code>timestamp</code> <code>float</code> <p>Timestamp of request</p> required <code>stage</code> <code>str | None</code> <p>Optional stage name</p> <code>None</code> <p>Returns:</p> Type Description <code>Decimal</code> <p>Cost for this entry</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def add(\n    self,\n    tokens_in: int,\n    tokens_out: int,\n    model: str,\n    timestamp: float,\n    stage: str | None = None,\n) -&gt; Decimal:\n    \"\"\"\n    Add cost entry.\n\n    Args:\n        tokens_in: Input tokens used\n        tokens_out: Output tokens used\n        model: Model identifier\n        timestamp: Timestamp of request\n        stage: Optional stage name\n\n    Returns:\n        Cost for this entry\n    \"\"\"\n    cost = self.calculate_cost(tokens_in, tokens_out)\n\n    with self._lock:\n        entry = CostEntry(\n            tokens_in=tokens_in,\n            tokens_out=tokens_out,\n            cost=cost,\n            model=model,\n            timestamp=timestamp,\n        )\n        self._entries.append(entry)\n\n        self._total_input_tokens += tokens_in\n        self._total_output_tokens += tokens_out\n        self._total_cost += cost\n\n        if stage:\n            self._stage_costs[stage] = (\n                self._stage_costs.get(stage, Decimal(\"0.0\")) + cost\n            )\n\n    return cost\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(tokens_in: int, tokens_out: int) -&gt; Decimal\n</code></pre> <p>Calculate cost for given token counts.</p> <p>Parameters:</p> Name Type Description Default <code>tokens_in</code> <code>int</code> <p>Input tokens</p> required <code>tokens_out</code> <code>int</code> <p>Output tokens</p> required <p>Returns:</p> Type Description <code>Decimal</code> <p>Total cost</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def calculate_cost(self, tokens_in: int, tokens_out: int) -&gt; Decimal:\n    \"\"\"\n    Calculate cost for given token counts.\n\n    Args:\n        tokens_in: Input tokens\n        tokens_out: Output tokens\n\n    Returns:\n        Total cost\n    \"\"\"\n    from ondine.utils.cost_calculator import CostCalculator\n\n    return CostCalculator.calculate(\n        tokens_in=tokens_in,\n        tokens_out=tokens_out,\n        input_cost_per_1k=self.input_cost_per_1k,\n        output_cost_per_1k=self.output_cost_per_1k,\n    )\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.get_estimate","title":"get_estimate","text":"<pre><code>get_estimate(rows: int = 0) -&gt; CostEstimate\n</code></pre> <p>Get cost estimate.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>int</code> <p>Number of rows processed</p> <code>0</code> <p>Returns:</p> Type Description <code>CostEstimate</code> <p>CostEstimate object</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_estimate(self, rows: int = 0) -&gt; CostEstimate:\n    \"\"\"\n    Get cost estimate.\n\n    Args:\n        rows: Number of rows processed\n\n    Returns:\n        CostEstimate object\n    \"\"\"\n    with self._lock:\n        total_tokens = self._total_input_tokens + self._total_output_tokens\n        return CostEstimate(\n            total_cost=self._total_cost,\n            total_tokens=total_tokens,\n            input_tokens=self._total_input_tokens,\n            output_tokens=self._total_output_tokens,\n            rows=rows,\n            breakdown_by_stage=dict(self._stage_costs),\n            confidence=\"actual\",\n        )\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset all tracking.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all tracking.\"\"\"\n    with self._lock:\n        self._total_input_tokens = 0\n        self._total_output_tokens = 0\n        self._total_cost = Decimal(\"0.0\")\n        self._entries.clear()\n        self._stage_costs.clear()\n</code></pre>"},{"location":"api/utils/cost_tracker/#ondine.utils.cost_tracker.CostTracker.get_stage_costs","title":"get_stage_costs","text":"<pre><code>get_stage_costs() -&gt; dict[str, Decimal]\n</code></pre> <p>Get costs breakdown by stage.</p> Source code in <code>ondine/utils/cost_tracker.py</code> <pre><code>def get_stage_costs(self) -&gt; dict[str, Decimal]:\n    \"\"\"Get costs breakdown by stage.\"\"\"\n    with self._lock:\n        return dict(self._stage_costs)\n</code></pre>"},{"location":"api/utils/input_preprocessing/","title":"input_preprocessing","text":""},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing","title":"input_preprocessing","text":"<p>Input preprocessing for LLM prompts.</p> <p>Best practices: Remove noise, normalize whitespace, control length.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.PreprocessingStats","title":"PreprocessingStats  <code>dataclass</code>","text":"<pre><code>PreprocessingStats(rows_processed: int, chars_before: int, chars_after: int, truncated_count: int, null_count: int)\n</code></pre> <p>Statistics from preprocessing operation.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.PreprocessingStats.reduction_pct","title":"reduction_pct  <code>property</code>","text":"<pre><code>reduction_pct: float\n</code></pre> <p>Calculate character reduction percentage.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextCleaner","title":"TextCleaner","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for text cleaning strategies.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextCleaner.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Clean text according to strategy.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Clean text according to strategy.\"\"\"\n    ...\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.UnicodeNormalizer","title":"UnicodeNormalizer","text":"<p>Normalize Unicode to canonical form (NFC).</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.UnicodeNormalizer.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Normalize Unicode: \u00e9 vs e + \u00b4 \u2192 consistent form.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Normalize Unicode: \u00e9 vs e + \u00b4 \u2192 consistent form.\"\"\"\n    return unicodedata.normalize(\"NFC\", text)\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.ControlCharRemover","title":"ControlCharRemover","text":"<p>Remove control characters that confuse tokenizers.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.ControlCharRemover.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Replace control chars with space (preserves word boundaries).</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Replace control chars with space (preserves word boundaries).\"\"\"\n    return \"\".join(\n        char if unicodedata.category(char)[0] != \"C\" else \" \" for char in text\n    )\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.SpecialCharCleaner","title":"SpecialCharCleaner","text":"<pre><code>SpecialCharCleaner(preserve: str = ',\\\\-/\\\\.\\\\(\\\\)&amp;')\n</code></pre> <p>Remove noise characters while preserving semantic punctuation.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, preserve: str = r\",\\-/\\.\\(\\)&amp;\"):\n    self.preserve = preserve\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.SpecialCharCleaner.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Remove \u00ae\u2122\u00a9 and excessive special chars.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Remove \u00ae\u2122\u00a9 and excessive special chars.\"\"\"\n    # Remove trademark symbols\n    text = re.sub(r\"[\u00ae\u2122\u00a9\u2117\u2120]\", \"\", text)\n\n    # Normalize quotes\n    text = text.replace('\"', '\"').replace('\"', '\"')\n    text = text.replace(\"\"\", \"'\").replace(\"\"\", \"'\")\n\n    # Remove zero-width characters\n    text = re.sub(r\"[\\u200b-\\u200f\\ufeff]\", \"\", text)\n\n    # Keep only: alphanumeric, whitespace, specified punctuation\n    pattern = f\"[^\\\\w\\\\s{self.preserve}]\"\n    return re.sub(pattern, \" \", text)\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.WhitespaceNormalizer","title":"WhitespaceNormalizer","text":"<p>Collapse multiple spaces/tabs/newlines.</p>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.WhitespaceNormalizer.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Replace tabs/newlines with spaces, collapse multiples.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Replace tabs/newlines with spaces, collapse multiples.\"\"\"\n    text = text.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    return re.sub(r\"\\s+\", \" \", text).strip()\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextTruncator","title":"TextTruncator","text":"<pre><code>TextTruncator(max_length: int = 500)\n</code></pre> <p>Intelligently truncate at word boundaries.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, max_length: int = 500):\n    self.max_length = max_length\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextTruncator.clean","title":"clean","text":"<pre><code>clean(text: str) -&gt; str\n</code></pre> <p>Truncate respecting word boundaries.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def clean(self, text: str) -&gt; str:\n    \"\"\"Truncate respecting word boundaries.\"\"\"\n    if len(text) &lt;= self.max_length:\n        return text\n\n    # Reserve space for ellipsis\n    limit = self.max_length - 3\n    min_keep = int(self.max_length * 0.7)\n\n    # Try delimiters first: |, ;, ' - '\n    for delim in [\"|\", \";\", \" - \", \"  \"]:\n        pos = text.rfind(delim, 0, limit)\n        if pos &gt; min_keep:\n            return text[:pos].strip() + \"...\"\n\n    # Fall back to last space\n    pos = text.rfind(\" \", 0, limit)\n    if pos &gt; min_keep:\n        return text[:pos].strip() + \"...\"\n\n    # Hard truncate if no boundary found\n    return text[:limit].strip() + \"...\"\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextPreprocessor","title":"TextPreprocessor","text":"<pre><code>TextPreprocessor(max_length: int = 500)\n</code></pre> <p>Composable text preprocessor following Chain of Responsibility.</p> <p>Single Responsibility: Orchestrate cleaning steps. Open/Closed: Extensible via cleaners list. Dependency Inversion: Depends on Protocol, not concrete classes.</p> <p>Initialize with default cleaning pipeline.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def __init__(self, max_length: int = 500):\n    \"\"\"Initialize with default cleaning pipeline.\"\"\"\n    self.cleaners: list[TextCleaner] = [\n        UnicodeNormalizer(),\n        ControlCharRemover(),\n        SpecialCharCleaner(),\n        WhitespaceNormalizer(),\n        TextTruncator(max_length),\n    ]\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextPreprocessor.process","title":"process","text":"<pre><code>process(text: str) -&gt; str\n</code></pre> <p>Apply all cleaners in sequence.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def process(self, text: str) -&gt; str:\n    \"\"\"Apply all cleaners in sequence.\"\"\"\n    if pd.isna(text) or not isinstance(text, str):\n        return \"\"\n\n    for cleaner in self.cleaners:\n        text = cleaner.clean(text)\n\n    return text\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.TextPreprocessor.add_cleaner","title":"add_cleaner","text":"<pre><code>add_cleaner(cleaner: TextCleaner) -&gt; None\n</code></pre> <p>Extend pipeline with custom cleaner.</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def add_cleaner(self, cleaner: TextCleaner) -&gt; None:\n    \"\"\"Extend pipeline with custom cleaner.\"\"\"\n    self.cleaners.append(cleaner)\n</code></pre>"},{"location":"api/utils/input_preprocessing/#ondine.utils.input_preprocessing.preprocess_dataframe","title":"preprocess_dataframe","text":"<pre><code>preprocess_dataframe(df: DataFrame, input_columns: list[str], max_length: int = 500) -&gt; tuple[pd.DataFrame, PreprocessingStats]\n</code></pre> <p>Preprocess input columns in dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe</p> required <code>input_columns</code> <code>list[str]</code> <p>Columns to clean</p> required <code>max_length</code> <code>int</code> <p>Max chars per field</p> <code>500</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, PreprocessingStats]</code> <p>(cleaned_df, stats)</p> Source code in <code>ondine/utils/input_preprocessing.py</code> <pre><code>def preprocess_dataframe(\n    df: pd.DataFrame,\n    input_columns: list[str],\n    max_length: int = 500,\n) -&gt; tuple[pd.DataFrame, PreprocessingStats]:\n    \"\"\"\n    Preprocess input columns in dataframe.\n\n    Args:\n        df: Input dataframe\n        input_columns: Columns to clean\n        max_length: Max chars per field\n\n    Returns:\n        (cleaned_df, stats)\n    \"\"\"\n    result = df.copy()\n    preprocessor = TextPreprocessor(max_length)\n\n    chars_before = 0\n    chars_after = 0\n    truncated = 0\n    nulls = 0\n\n    for col in input_columns:\n        if col not in result.columns:\n            continue\n\n        for idx in result.index:\n            original = result.at[idx, col]\n\n            if pd.isna(original):\n                nulls += 1\n                continue\n\n            original_str = str(original)\n            chars_before += len(original_str)\n\n            cleaned = preprocessor.process(original_str)\n            chars_after += len(cleaned)\n\n            if len(original_str) &gt; max_length:\n                truncated += 1\n\n            result.at[idx, col] = cleaned\n\n    stats = PreprocessingStats(\n        rows_processed=len(result),\n        chars_before=chars_before,\n        chars_after=chars_after,\n        truncated_count=truncated,\n        null_count=nulls,\n    )\n\n    return result, stats\n</code></pre>"},{"location":"api/utils/logging_utils/","title":"logging_utils","text":""},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils","title":"logging_utils","text":"<p>Structured logging utilities.</p> <p>Provides consistent logging configuration across the SDK using structlog.</p>"},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(level: str = 'INFO', json_format: bool = False, include_timestamp: bool = True) -&gt; None\n</code></pre> <p>Configure structured logging for the SDK.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> <code>json_format</code> <code>bool</code> <p>Use JSON output format</p> <code>False</code> <code>include_timestamp</code> <code>bool</code> <p>Include timestamps in logs</p> <code>True</code> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def configure_logging(\n    level: str = \"INFO\",\n    json_format: bool = False,\n    include_timestamp: bool = True,\n) -&gt; None:\n    \"\"\"\n    Configure structured logging for the SDK.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        json_format: Use JSON output format\n        include_timestamp: Include timestamps in logs\n    \"\"\"\n    global _logging_configured\n\n    # Set stdlib logging level\n    logging.basicConfig(\n        format=\"%(message)s\",\n        stream=sys.stdout,\n        level=getattr(logging, level.upper()),\n    )\n\n    # Configure structlog processors\n    processors = [\n        structlog.contextvars.merge_contextvars,\n        structlog.processors.add_log_level,\n        structlog.processors.StackInfoRenderer(),\n    ]\n\n    if include_timestamp:\n        processors.append(structlog.processors.TimeStamper(fmt=\"%Y-%m-%d %H:%M:%S\"))\n\n    if json_format:\n        processors.append(structlog.processors.JSONRenderer())\n    else:\n        # Use custom compact console renderer (no padding)\n        processors.append(_compact_console_renderer)\n\n    structlog.configure(\n        processors=processors,\n        wrapper_class=structlog.make_filtering_bound_logger(\n            getattr(logging, level.upper())\n        ),\n        context_class=dict,\n        logger_factory=structlog.PrintLoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    _logging_configured = True\n</code></pre>"},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils.get_logger","title":"get_logger","text":"<pre><code>get_logger(name: str) -&gt; structlog.BoundLogger\n</code></pre> <p>Get a structured logger instance.</p> <p>Auto-configures logging on first use if not already configured.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically name)</p> required <p>Returns:</p> Type Description <code>BoundLogger</code> <p>Configured structlog logger</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def get_logger(name: str) -&gt; structlog.BoundLogger:\n    \"\"\"\n    Get a structured logger instance.\n\n    Auto-configures logging on first use if not already configured.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured structlog logger\n    \"\"\"\n    global _logging_configured\n\n    # Auto-configure logging on first use\n    if not _logging_configured:\n        configure_logging()\n\n    return structlog.get_logger(name)\n</code></pre>"},{"location":"api/utils/logging_utils/#ondine.utils.logging_utils.sanitize_for_logging","title":"sanitize_for_logging","text":"<pre><code>sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Sanitize sensitive data for logging.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary potentially containing sensitive data</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Sanitized dictionary</p> Source code in <code>ondine/utils/logging_utils.py</code> <pre><code>def sanitize_for_logging(data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Sanitize sensitive data for logging.\n\n    Args:\n        data: Dictionary potentially containing sensitive data\n\n    Returns:\n        Sanitized dictionary\n    \"\"\"\n    sensitive_keys = {\n        \"api_key\",\n        \"password\",\n        \"secret\",\n        \"token\",\n        \"authorization\",\n        \"credential\",\n    }\n\n    sanitized = {}\n    for key, value in data.items():\n        key_lower = key.lower()\n        if any(sensitive in key_lower for sensitive in sensitive_keys):\n            sanitized[key] = \"***REDACTED***\"\n        elif isinstance(value, dict):\n            sanitized[key] = sanitize_for_logging(value)\n        else:\n            sanitized[key] = value\n\n    return sanitized\n</code></pre>"},{"location":"api/utils/metrics_exporter/","title":"metrics_exporter","text":""},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter","title":"metrics_exporter","text":"<p>Prometheus metrics export for monitoring.</p> <p>Provides instrumentation for external monitoring systems.</p>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics","title":"PrometheusMetrics","text":"<pre><code>PrometheusMetrics(port: int = 9090)\n</code></pre> <p>Prometheus metrics exporter.</p> <p>Follows Single Responsibility: only handles metrics export.</p> <p>Initialize Prometheus metrics.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Port for metrics HTTP server</p> <code>9090</code> Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def __init__(self, port: int = 9090):\n    \"\"\"\n    Initialize Prometheus metrics.\n\n    Args:\n        port: Port for metrics HTTP server\n    \"\"\"\n    self.port = port\n    self._server_started = False\n\n    # Define metrics\n    self.requests_total = Counter(\n        \"llm_requests_total\",\n        \"Total LLM requests\",\n        [\"provider\", \"model\", \"stage\"],\n    )\n\n    self.request_duration = Histogram(\n        \"llm_request_duration_seconds\",\n        \"LLM request duration in seconds\",\n        [\"provider\", \"stage\"],\n    )\n\n    self.cost_total = Gauge(\n        \"llm_cost_total_usd\",\n        \"Total cost in USD\",\n        [\"provider\"],\n    )\n\n    self.errors_total = Counter(\n        \"llm_errors_total\",\n        \"Total errors\",\n        [\"stage\", \"error_type\"],\n    )\n\n    self.rows_processed = Gauge(\n        \"llm_rows_processed_total\",\n        \"Total rows processed\",\n        [\"stage\"],\n    )\n\n    self.rows_per_second = Gauge(\n        \"llm_rows_per_second\",\n        \"Processing throughput\",\n    )\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.start_server","title":"start_server","text":"<pre><code>start_server() -&gt; None\n</code></pre> <p>Start HTTP server for metrics endpoint.</p> Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def start_server(self) -&gt; None:\n    \"\"\"Start HTTP server for metrics endpoint.\"\"\"\n    if not self._server_started:\n        try:\n            start_http_server(self.port)\n            self._server_started = True\n            logger.info(f\"Prometheus metrics server started on port {self.port}\")\n        except Exception as e:\n            logger.error(f\"Failed to start metrics server: {e}\")\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_request","title":"record_request","text":"<pre><code>record_request(provider: str, model: str, stage: str, duration: float) -&gt; None\n</code></pre> <p>Record LLM request metrics.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>model</code> <code>str</code> <p>Model name</p> required <code>stage</code> <code>str</code> <p>Stage name</p> required <code>duration</code> <code>float</code> <p>Request duration in seconds</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_request(\n    self, provider: str, model: str, stage: str, duration: float\n) -&gt; None:\n    \"\"\"\n    Record LLM request metrics.\n\n    Args:\n        provider: Provider name\n        model: Model name\n        stage: Stage name\n        duration: Request duration in seconds\n    \"\"\"\n    self.requests_total.labels(provider=provider, model=model, stage=stage).inc()\n\n    self.request_duration.labels(provider=provider, stage=stage).observe(duration)\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_cost","title":"record_cost","text":"<pre><code>record_cost(provider: str, cost: float) -&gt; None\n</code></pre> <p>Record cost metric.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>cost</code> <code>float</code> <p>Cost in USD</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_cost(self, provider: str, cost: float) -&gt; None:\n    \"\"\"\n    Record cost metric.\n\n    Args:\n        provider: Provider name\n        cost: Cost in USD\n    \"\"\"\n    self.cost_total.labels(provider=provider).set(cost)\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_error","title":"record_error","text":"<pre><code>record_error(stage: str, error_type: str) -&gt; None\n</code></pre> <p>Record error metric.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Stage name</p> required <code>error_type</code> <code>str</code> <p>Error type</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_error(self, stage: str, error_type: str) -&gt; None:\n    \"\"\"\n    Record error metric.\n\n    Args:\n        stage: Stage name\n        error_type: Error type\n    \"\"\"\n    self.errors_total.labels(stage=stage, error_type=error_type).inc()\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_rows_processed","title":"record_rows_processed","text":"<pre><code>record_rows_processed(stage: str, count: int) -&gt; None\n</code></pre> <p>Record rows processed.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Stage name</p> required <code>count</code> <code>int</code> <p>Number of rows</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_rows_processed(self, stage: str, count: int) -&gt; None:\n    \"\"\"\n    Record rows processed.\n\n    Args:\n        stage: Stage name\n        count: Number of rows\n    \"\"\"\n    self.rows_processed.labels(stage=stage).set(count)\n</code></pre>"},{"location":"api/utils/metrics_exporter/#ondine.utils.metrics_exporter.PrometheusMetrics.record_throughput","title":"record_throughput","text":"<pre><code>record_throughput(rows_per_second: float) -&gt; None\n</code></pre> <p>Record processing throughput.</p> <p>Parameters:</p> Name Type Description Default <code>rows_per_second</code> <code>float</code> <p>Throughput metric</p> required Source code in <code>ondine/utils/metrics_exporter.py</code> <pre><code>def record_throughput(self, rows_per_second: float) -&gt; None:\n    \"\"\"\n    Record processing throughput.\n\n    Args:\n        rows_per_second: Throughput metric\n    \"\"\"\n    self.rows_per_second.set(rows_per_second)\n</code></pre>"},{"location":"api/utils/rate_limiter/","title":"rate_limiter","text":""},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter","title":"rate_limiter","text":"<p>Token bucket rate limiter for API calls.</p> <p>Implements token bucket algorithm for rate limiting.</p>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter","title":"RateLimiter","text":"<pre><code>RateLimiter(requests_per_minute: int, burst_size: int | None = None)\n</code></pre> <p>Token bucket rate limiter for controlling API request rates.</p> <p>Thread-safe implementation.</p> <p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_minute</code> <code>int</code> <p>Maximum requests per minute</p> required <code>burst_size</code> <code>int | None</code> <p>Maximum burst size (default: requests_per_minute)</p> <code>None</code> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def __init__(self, requests_per_minute: int, burst_size: int | None = None):\n    \"\"\"\n    Initialize rate limiter.\n\n    Args:\n        requests_per_minute: Maximum requests per minute\n        burst_size: Maximum burst size (default: requests_per_minute)\n    \"\"\"\n    self.rpm = requests_per_minute\n    self.capacity = burst_size or requests_per_minute\n    self.tokens = float(self.capacity)\n    self.last_update = time.time()\n    self.lock = threading.Lock()\n\n    # Calculate refill rate (tokens per second)\n    self.refill_rate = requests_per_minute / 60.0\n</code></pre>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter.available_tokens","title":"available_tokens  <code>property</code>","text":"<pre><code>available_tokens: float\n</code></pre> <p>Get current available tokens.</p>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter.acquire","title":"acquire","text":"<pre><code>acquire(tokens: int = 1, timeout: float | None = None) -&gt; bool\n</code></pre> <p>Acquire tokens for making requests.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>int</code> <p>Number of tokens to acquire</p> <code>1</code> <code>timeout</code> <code>float | None</code> <p>Maximum wait time in seconds (None = wait forever)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if tokens acquired, False if timeout</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokens &gt; capacity</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def acquire(self, tokens: int = 1, timeout: float | None = None) -&gt; bool:\n    \"\"\"\n    Acquire tokens for making requests.\n\n    Args:\n        tokens: Number of tokens to acquire\n        timeout: Maximum wait time in seconds (None = wait forever)\n\n    Returns:\n        True if tokens acquired, False if timeout\n\n    Raises:\n        ValueError: If tokens &gt; capacity\n    \"\"\"\n    if tokens &gt; self.capacity:\n        raise ValueError(\n            f\"Requested {tokens} tokens exceeds capacity {self.capacity}\"\n        )\n\n    deadline = None if timeout is None else time.time() + timeout\n\n    while True:\n        with self.lock:\n            self._refill()\n\n            if self.tokens &gt;= tokens:\n                self.tokens -= tokens\n                return True\n\n        # Check timeout\n        if deadline is not None and time.time() &gt;= deadline:\n            return False\n\n        # Sleep before retry\n        time.sleep(0.1)\n</code></pre>"},{"location":"api/utils/rate_limiter/#ondine.utils.rate_limiter.RateLimiter.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset rate limiter to full capacity.</p> Source code in <code>ondine/utils/rate_limiter.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset rate limiter to full capacity.\"\"\"\n    with self.lock:\n        self.tokens = float(self.capacity)\n        self.last_update = time.time()\n</code></pre>"},{"location":"api/utils/retry_handler/","title":"retry_handler","text":""},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler","title":"retry_handler","text":"<p>Retry handling with exponential backoff.</p> <p>Provides robust retry logic for transient failures.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryableError","title":"RetryableError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors that should be retried.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RateLimitError","title":"RateLimitError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Rate limit exceeded error.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.NetworkError","title":"NetworkError","text":"<p>               Bases: <code>RetryableError</code></p> <p>Network-related error.</p>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryHandler","title":"RetryHandler","text":"<pre><code>RetryHandler(max_attempts: int = 3, initial_delay: float = 1.0, max_delay: float = 60.0, exponential_base: int = 2, retryable_exceptions: tuple[type[Exception], ...] | None = None)\n</code></pre> <p>Request-level retry with exponential backoff (for transient errors).</p> <p>Scope: Single LLM API call or operation Use when: Transient errors (rate limits, network timeouts, API hiccups) NOT for: Row-level quality issues (use Pipeline.auto_retry_failed for that)</p> <p>Retry Strategy: - Exponential backoff (1s, 2s, 4s, 8s, ...) - Configurable max attempts (default: 3) - Only retries specific exception types</p> Example <p>handler = RetryHandler(max_attempts=3, initial_delay=1.0) result = handler.execute(lambda: call_llm_api())</p> <p>See Also: - ErrorHandler: Orchestrates retry decisions based on policy - Pipeline._auto_retry_failed_rows(): Row-level quality retry - docs/architecture/decisions/ADR-006-retry-levels.md</p> <p>Initialize retry handler.</p> <p>Parameters:</p> Name Type Description Default <code>max_attempts</code> <code>int</code> <p>Maximum retry attempts</p> <code>3</code> <code>initial_delay</code> <code>float</code> <p>Initial delay in seconds</p> <code>1.0</code> <code>max_delay</code> <code>float</code> <p>Maximum delay in seconds</p> <code>60.0</code> <code>exponential_base</code> <code>int</code> <p>Base for exponential backoff</p> <code>2</code> <code>retryable_exceptions</code> <code>tuple[type[Exception], ...] | None</code> <p>Exception types to retry</p> <code>None</code> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def __init__(\n    self,\n    max_attempts: int = 3,\n    initial_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: int = 2,\n    retryable_exceptions: tuple[type[Exception], ...] | None = None,\n):\n    \"\"\"\n    Initialize retry handler.\n\n    Args:\n        max_attempts: Maximum retry attempts\n        initial_delay: Initial delay in seconds\n        max_delay: Maximum delay in seconds\n        exponential_base: Base for exponential backoff\n        retryable_exceptions: Exception types to retry\n    \"\"\"\n    self.max_attempts = max_attempts\n    self.initial_delay = initial_delay\n    self.max_delay = max_delay\n    self.exponential_base = exponential_base\n\n    if retryable_exceptions is None:\n        self.retryable_exceptions = (\n            RetryableError,\n            RateLimitError,\n            NetworkError,\n        )\n    else:\n        self.retryable_exceptions = retryable_exceptions\n</code></pre>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryHandler.execute","title":"execute","text":"<pre><code>execute(func: Callable[[], T]) -&gt; T\n</code></pre> <p>Execute function with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[], T]</code> <p>Function to execute</p> required <p>Returns:</p> Type Description <code>T</code> <p>Result from function</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If all retries exhausted</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def execute(self, func: Callable[[], T]) -&gt; T:\n    \"\"\"\n    Execute function with retry logic.\n\n    Args:\n        func: Function to execute\n\n    Returns:\n        Result from function\n\n    Raises:\n        Exception: If all retries exhausted\n    \"\"\"\n    retryer = Retrying(\n        stop=stop_after_attempt(self.max_attempts),\n        wait=wait_exponential(\n            multiplier=self.initial_delay,\n            max=self.max_delay,\n            exp_base=self.exponential_base,\n        ),\n        retry=retry_if_exception_type(self.retryable_exceptions),\n        reraise=True,\n    )\n\n    return retryer(func)\n</code></pre>"},{"location":"api/utils/retry_handler/#ondine.utils.retry_handler.RetryHandler.calculate_delay","title":"calculate_delay","text":"<pre><code>calculate_delay(attempt: int) -&gt; float\n</code></pre> <p>Calculate delay for given attempt number.</p> <p>Parameters:</p> Name Type Description Default <code>attempt</code> <code>int</code> <p>Attempt number (1-based)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Delay in seconds</p> Source code in <code>ondine/utils/retry_handler.py</code> <pre><code>def calculate_delay(self, attempt: int) -&gt; float:\n    \"\"\"\n    Calculate delay for given attempt number.\n\n    Args:\n        attempt: Attempt number (1-based)\n\n    Returns:\n        Delay in seconds\n    \"\"\"\n    delay = self.initial_delay * (self.exponential_base ** (attempt - 1))\n    return min(delay, self.max_delay)\n</code></pre>"},{"location":"architecture/technical-reference/","title":"Ondine - Complete Technical Reference","text":"<p>Version: 1.0.0 Last Updated: October 18, 2025 Purpose: Comprehensive technical documentation of every component, class, design decision, and relationship in the Ondine LLM Dataset Engine.</p> <p>Quick Navigation: - Architecture Overview &amp; Diagrams: See <code>ARCHITECTURE.md</code> (auto-generated from <code>architecture/model.yaml</code>) - Design Decisions: See <code>architecture/decisions/</code> (ADRs) - Implementation Details: This document (TECHNICAL_REFERENCE.md)</p> <p>Note: This document provides detailed implementation information. For structural relationships and visual diagrams, see <code>ARCHITECTURE.md</code>. For design rationale and trade-offs, see the ADRs.</p>"},{"location":"architecture/technical-reference/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Part 1: Architecture Overview</li> <li>Part 2: External Dependencies</li> <li>Part 3: Layer 0 - Core Utilities</li> <li>Part 4: Core Models &amp; Specifications</li> <li>Part 5: Layer 1 - Infrastructure Adapters</li> <li>Part 6: Layer 2 - Processing Stages</li> <li>Part 7: Layer 3 - Orchestration Engine</li> <li>Part 8: Layer 4 - High-Level API</li> <li>Part 9: Configuration System</li> <li>Part 10: CLI Interface</li> <li>Part 11: Framework Integrations</li> <li>Part 12: Execution Flows</li> <li>Part 13: Data Flows</li> <li>Part 14: Extension Points</li> </ul>"},{"location":"architecture/technical-reference/#part-1-architecture-overview","title":"Part 1: Architecture Overview","text":""},{"location":"architecture/technical-reference/#11-system-architecture","title":"1.1 System Architecture","text":"<p>Ondine follows a 5-layer architecture:</p> <pre><code>graph TB\n    subgraph \"Layer 4: High-Level API\"\n        API[Pipeline]\n        Builder[PipelineBuilder]\n        Composer[PipelineComposer]\n        Processor[DatasetProcessor]\n    end\n\n    subgraph \"Layer 3: Orchestration\"\n        Executor[PipelineExecutor]\n        Strategy[ExecutionStrategy]\n        SyncExec[SyncExecutor]\n        AsyncExec[AsyncExecutor]\n        StreamExec[StreamingExecutor]\n        Context[ExecutionContext]\n        StateManager[StateManager]\n        Observers[Observers]\n    end\n\n    subgraph \"Layer 2: Processing Stages\"\n        DataLoader[DataLoaderStage]\n        PromptFormatter[PromptFormatterStage]\n        LLMInvocation[LLMInvocationStage]\n        ResponseParser[ResponseParserStage]\n        ResultWriter[ResultWriterStage]\n    end\n\n    subgraph \"Layer 1: Infrastructure Adapters\"\n        LLMClient[LLM Client]\n        DataIO[Data I/O]\n        Checkpoint[Checkpoint Storage]\n    end\n\n    subgraph \"Layer 0: Utilities\"\n        Retry[RetryHandler]\n        RateLimit[RateLimiter]\n        Cost[CostTracker]\n        Budget[BudgetController]\n        Logging[Logging Utils]\n    end\n\n    API --&gt; Executor\n    Builder --&gt; API\n    Executor --&gt; Strategy\n    Strategy --&gt; SyncExec\n    Strategy --&gt; AsyncExec\n    Strategy --&gt; StreamExec\n    Executor --&gt; Context\n    Executor --&gt; StateManager\n    Executor --&gt; Observers\n    Executor --&gt; DataLoader\n    DataLoader --&gt; PromptFormatter\n    PromptFormatter --&gt; LLMInvocation\n    LLMInvocation --&gt; ResponseParser\n    ResponseParser --&gt; ResultWriter\n    LLMInvocation --&gt; LLMClient\n    DataLoader --&gt; DataIO\n    ResultWriter --&gt; DataIO\n    StateManager --&gt; Checkpoint\n    LLMInvocation --&gt; Retry\n    LLMInvocation --&gt; RateLimit\n    LLMInvocation --&gt; Cost\n    Executor --&gt; Budget\n</code></pre>"},{"location":"architecture/technical-reference/#layer-responsibilities","title":"Layer Responsibilities","text":"Layer Directory Purpose Dependencies Layer 0 <code>utils/</code> Cross-cutting concerns (retry, rate limiting, cost tracking) External libraries only Layer 1 <code>adapters/</code> External system integrations (LLM providers, file I/O) Layer 0 + external APIs Layer 2 <code>stages/</code> Data transformation logic (load, format, invoke, parse, write) Layers 0-1 Layer 3 <code>orchestration/</code> Execution control and state management Layers 0-2 Layer 4 <code>api/</code> User-facing interfaces (Pipeline, Builder) All layers"},{"location":"architecture/technical-reference/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Dependency Rule: Dependencies only point inward (higher layers depend on lower layers, never the reverse)</li> <li>Focused Components: Each component has one clear purpose</li> <li>Extensible: Open for extension, closed for modification</li> <li>Abstraction: Depend on abstractions, not concretions</li> <li>Simple: Keep it simple</li> </ol>"},{"location":"architecture/technical-reference/#12-design-patterns-catalog","title":"1.2 Design Patterns Catalog","text":"Pattern Where Used Purpose Implementation Facade <code>Pipeline</code> Simplify complex subsystem Hides orchestration complexity Builder <code>PipelineBuilder</code> Fluent construction API Chainable method calls Strategy <code>ExecutionStrategy</code> Pluggable execution modes <code>SyncExecutor</code>, <code>AsyncExecutor</code>, <code>StreamingExecutor</code> Template Method <code>PipelineStage.execute()</code> Standardized stage flow Base class with hooks Observer <code>ExecutionObserver</code> Monitoring hooks <code>ProgressObserver</code>, <code>CostObserver</code>, <code>LoggingObserver</code> Adapter <code>LLMClient</code>, <code>DataReader</code> Interface translation Wrap external libraries Factory <code>ParserFactory</code> Object creation Create parsers by type Composite <code>PipelineComposer</code> Tree structure Compose pipelines Singleton Config instances Single instance Via module-level state Chain of Responsibility Stage pipeline Sequential processing Each stage processes then passes Protocol <code>TextCleaner</code> Structural typing Duck typing with validation Memento <code>ExecutionContext</code> State capture Serializable state"},{"location":"architecture/technical-reference/#13-thread-safety-strategy","title":"1.3 Thread Safety Strategy","text":""},{"location":"architecture/technical-reference/#thread-safe-components","title":"Thread-Safe Components","text":"Component Mechanism Reason <code>CostTracker</code> <code>threading.Lock</code> Shared cost accumulation <code>RateLimiter</code> <code>threading.Lock</code> Token bucket state <code>CheckpointStorage</code> File-based locks Concurrent writes <code>LLMInvocationStage</code> <code>ThreadPoolExecutor</code> Concurrent LLM calls"},{"location":"architecture/technical-reference/#thread-unsafe-components-by-design","title":"Thread-Unsafe Components (By Design)","text":"Component Why Not Thread-Safe <code>PipelineBuilder</code> Construction phase only, not shared <code>Pipeline</code> (single exec) Each execution is sequential <code>DataLoaderStage</code> Reads once, no shared state"},{"location":"architecture/technical-reference/#part-2-external-dependencies","title":"Part 2: External Dependencies","text":""},{"location":"architecture/technical-reference/#21-dependency-catalog","title":"2.1 Dependency Catalog","text":""},{"location":"architecture/technical-reference/#production-dependencies-20-libraries","title":"Production Dependencies (20 libraries)","text":"Library Version Category License Why Chosen Alternatives Considered llama-index &gt;=0.12.0 LLM MIT LLM provider clients (OpenAI, Anthropic, Groq). Ondine adds batch orchestration, cost tracking, checkpointing, YAML config. LangChain (more complex), direct APIs (no abstraction) llama-index-llms-openai &gt;=0.3.0 LLM MIT Official OpenAI integration <code>openai</code> package (less abstraction) llama-index-llms-azure-openai &gt;=0.3.0 LLM MIT Enterprise Azure support Custom Azure client llama-index-llms-anthropic &gt;=0.3.0 LLM MIT Claude integration <code>anthropic</code> package (less abstraction) llama-index-llms-groq &gt;=0.3.0 LLM MIT Fast, affordable inference Direct Groq API pandas &gt;=2.0.0 Data BSD-3 Industry standard, rich API Polars (less mature ecosystem) polars &gt;=0.20.0 Data MIT Fast Parquet reading Pandas (slower for large files) pydantic &gt;=2.0.0 Validation MIT Validation + serialization + type hints dataclasses (no validation), marshmallow (complex) python-dotenv &gt;=1.0.0 Config BSD-3 Simple .env loading os.environ (manual) tqdm &gt;=4.66.0 UI MPL/MIT Simple, widely used progress bars rich.progress (overkill), progressbar2 tenacity &gt;=8.2.0 Reliability Apache-2.0 Flexible retry logic <code>backoff</code> (simpler but less flexible) openpyxl &gt;=3.1.0 Data MIT Excel file support xlrd (deprecated), pyexcel pyarrow &gt;=15.0.0 Data Apache-2.0 Fast Parquet I/O fastparquet (slower) tiktoken &gt;=0.5.0 LLM MIT Fast token counting, OpenAI-native transformers (slower), estimate (inaccurate) structlog &gt;=24.0.0 Logging MIT/Apache Structured logging, JSON output standard logging (unstructured) jinja2 &gt;=3.1.0 Templating BSD-3 Powerful prompt templating string.Template (too simple), mako prometheus-client &gt;=0.20.0 Monitoring Apache-2.0 Industry standard metrics Custom metrics (reinvent wheel) click &gt;=8.1.0 CLI BSD-3 Decorator-based, simple argparse (verbose), typer (overkill) rich &gt;=13.0.0 CLI MIT Beautiful tables and formatting colorama (basic), termcolor"},{"location":"architecture/technical-reference/#dev-dependencies-8-libraries","title":"Dev Dependencies (8 libraries)","text":"Library Purpose Why Chosen pytest Testing framework Industry standard, rich plugins pytest-cov Coverage reporting Integrates with pytest pytest-asyncio Async test support Test async executors black Code formatting Opinionated, consistent ruff Fast linting Faster than flake8/pylint mypy Type checking Catch type errors ipython Interactive shell Better REPL jupyter Notebooks Interactive exploration"},{"location":"architecture/technical-reference/#optional-dependencies-1-group","title":"Optional Dependencies (1 group)","text":"Group Libraries Purpose Platform mlx mlx&gt;=0.29.0, mlx-lm&gt;=0.28.0 Apple Silicon local inference macOS only (M1/M2/M3/M4) <p>Installation: <code>pip install ondine[mlx]</code></p>"},{"location":"architecture/technical-reference/#22-dependency-graph","title":"2.2 Dependency Graph","text":"<pre><code>graph TD\n    Ondine[Ondine Core]\n\n    Ondine --&gt; LI[llama-index]\n    Ondine --&gt; Pandas\n    Ondine --&gt; Pydantic\n    Ondine --&gt; Structlog\n    Ondine --&gt; Click\n    Ondine --&gt; Rich\n\n    LI --&gt; LIOAI[llama-index-llms-openai]\n    LI --&gt; LIAZ[llama-index-llms-azure]\n    LI --&gt; LIANT[llama-index-llms-anthropic]\n    LI --&gt; LIGR[llama-index-llms-groq]\n\n    Pandas --&gt; Openpyxl[openpyxl]\n    Pandas --&gt; Polars\n\n    Ondine --&gt; Tiktoken[tiktoken]\n    Ondine --&gt; Tenacity[tenacity]\n    Ondine --&gt; Tqdm[tqdm]\n    Ondine --&gt; Jinja2[jinja2]\n    Ondine --&gt; Prometheus[prometheus-client]\n    Ondine --&gt; Dotenv[python-dotenv]\n\n    Ondine -.-&gt; MLX[mlx + mlx-lm]\n    MLX -.-&gt; MLXMetal[mlx-metal]\n\n    Polars --&gt; PyArrow[pyarrow]\n\n    style MLX stroke-dasharray: 5 5\n    style MLXMetal stroke-dasharray: 5 5\n</code></pre> <p>Note: Dashed lines indicate optional dependencies (<code>pip install ondine[mlx]</code>)</p>"},{"location":"architecture/technical-reference/#critical-dependencies-cannot-be-removed","title":"Critical Dependencies (Cannot Be Removed)","text":"<ol> <li>llama-index - LLM provider clients (OpenAI, Anthropic, Groq, Azure). Ondine wraps these with LLMSpec/LLMClient for batch processing, cost tracking, unified config.</li> <li>pandas - Data manipulation backbone, used throughout</li> <li>pydantic - Configuration validation, type safety</li> <li>structlog - Structured logging, observability</li> </ol>"},{"location":"architecture/technical-reference/#optional-dependencies-can-be-removed","title":"Optional Dependencies (Can Be Removed)","text":"<ol> <li>polars - Only for fast Parquet reading, pandas can handle it (slower)</li> <li>prometheus-client - Only for metrics export, can be disabled</li> <li>rich - Only for CLI pretty printing, can fall back to basic output</li> <li>jinja2 - Only for advanced templating, can use string.format()</li> <li>mlx + mlx-lm - Only for Apple Silicon local inference, use cloud providers instead</li> </ol>"},{"location":"architecture/technical-reference/#part-5-layer-1-infrastructure-adapters-llm-providers","title":"Part 5: Layer 1 - Infrastructure Adapters (LLM Providers)","text":""},{"location":"architecture/technical-reference/#51-llm-provider-overview","title":"5.1 LLM Provider Overview","text":"<p>Ondine supports multiple LLM providers through the Adapter pattern, allowing easy switching between providers without changing core logic.</p>"},{"location":"architecture/technical-reference/#supported-providers","title":"Supported Providers","text":"Provider Category Platform Cost Use Case OpenAI Cloud API All $$ Production, high quality Azure OpenAI Cloud API All $$ Enterprise, compliance Anthropic Cloud API All $$$ Claude models, long context Groq Cloud API All Free tier Fast inference, development OpenAI-Compatible Custom/Local/Cloud All Varies Ollama, vLLM, Together.AI, custom APIs"},{"location":"architecture/technical-reference/#provider-selection-guide","title":"Provider Selection Guide","text":"<p>Choose OpenAI if: Production quality, mature ecosystem, GPT-4 Choose Azure if: Enterprise compliance, private deployments Choose Anthropic if: Claude models, 100K+ context Choose Groq if: Fast inference, free tier, development Choose OpenAI-Compatible if: Custom endpoints, Ollama, vLLM, Together.AI, self-hosted</p>"},{"location":"architecture/technical-reference/#52-openai-compatible-provider-custom-apis","title":"5.2 OpenAI-Compatible Provider (Custom APIs)","text":""},{"location":"architecture/technical-reference/#purpose","title":"Purpose","text":"<p>Enable integration with any LLM API that implements the OpenAI chat completions format.</p>"},{"location":"architecture/technical-reference/#class-openaicompatibleclient","title":"Class: <code>OpenAICompatibleClient</code>","text":"<p>Inheritance: <code>LLMClient</code> (Adapter pattern)</p> <p>Platform: All (platform-agnostic)</p> <p>Responsibility: Connect to custom OpenAI-compatible API endpoints</p> <p>Supports: - Ollama (local LLM server) - vLLM (self-hosted inference) - Together.AI (cloud API) - Anyscale (cloud API) - LocalAI (self-hosted) - Any custom OpenAI-compatible API</p>"},{"location":"architecture/technical-reference/#configuration","title":"Configuration","text":"<p>Required Fields: - <code>provider: openai_compatible</code> - <code>base_url</code>: Custom API endpoint URL - <code>model</code>: Model identifier</p> <p>Optional Fields: - <code>provider_name</code>: Custom name for logging/metrics - <code>api_key</code>: Authentication (or use env var, or \"dummy\" for local) - <code>input_cost_per_1k_tokens</code>: Custom pricing - <code>output_cost_per_1k_tokens</code>: Custom pricing</p>"},{"location":"architecture/technical-reference/#architecture","title":"Architecture","text":"<pre><code>class OpenAICompatibleClient(LLMClient):\n    def __init__(self, spec: LLMSpec):\n        # Validate base_url is provided\n        # Initialize OpenAI client with custom base_url\n        # Use provider_name for metrics\n\n    def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n        # Call custom API using OpenAI format\n        # Return standardized LLMResponse\n</code></pre> <p>Design Decision: Reuse OpenAI Client</p> <p>Instead of reimplementing HTTP calls, leverage llama-index's OpenAI client with custom <code>api_base</code>:</p> <pre><code>self.client = OpenAI(\n    model=spec.model,\n    api_key=api_key or \"dummy\",\n    api_base=spec.base_url,  # Custom URL\n)\n</code></pre> <p>Rationale: - DRY: Reuse existing, well-tested code - Reliability: llama-index handles edge cases - Compatibility: Ensures OpenAI format compliance - Maintainability: Updates to OpenAI client benefit us</p>"},{"location":"architecture/technical-reference/#example-configurations","title":"Example Configurations","text":""},{"location":"architecture/technical-reference/#local-ollama-free","title":"Local Ollama (Free)","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"Ollama-Local\"\n  model: llama3.1:70b\n  base_url: http://localhost:11434/v1\n  # No API key needed\n  input_cost_per_1k_tokens: 0.0\n  output_cost_per_1k_tokens: 0.0\n</code></pre>"},{"location":"architecture/technical-reference/#togetherai-cloud","title":"Together.AI (Cloud)","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"Together.AI\"\n  model: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n  base_url: https://api.together.xyz/v1\n  api_key: \\${TOGETHER_API_KEY}\n  input_cost_per_1k_tokens: 0.0006\n  output_cost_per_1k_tokens: 0.0006\n</code></pre>"},{"location":"architecture/technical-reference/#self-hosted-vllm","title":"Self-Hosted vLLM","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"vLLM-Custom\"\n  model: meta-llama/Llama-3.1-70B-Instruct\n  base_url: http://your-vllm-server:8000/v1\n  input_cost_per_1k_tokens: 0.0  # Self-hosted = free\n  output_cost_per_1k_tokens: 0.0\n</code></pre>"},{"location":"architecture/technical-reference/#validation","title":"Validation","text":"<p>Pydantic Model Validator: <pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self) -&gt; \"LLMSpec\":\n    if self.provider == LLMProvider.OPENAI_COMPATIBLE and self.base_url is None:\n        raise ValueError(\"base_url required for openai_compatible provider\")\n    return self\n</code></pre></p> <p>Design: Fail fast with clear error message</p>"},{"location":"architecture/technical-reference/#token-estimation","title":"Token Estimation","text":"<p>Uses tiktoken with cl100k_base encoding (approximation): <pre><code>self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\ntokens = len(self.tokenizer.encode(text))\n</code></pre></p> <p>Tradeoff: Not model-specific, but consistent and fast</p>"},{"location":"architecture/technical-reference/#performance-characteristics","title":"Performance Characteristics","text":"<p>Depends on backend: - Ollama local: ~10-20 tokens/sec (70B model, single GPU) - vLLM multi-GPU: ~50-80 tokens/sec (70B model, tensor parallel) - Together.AI cloud: ~30-50 tokens/sec (network latency)</p>"},{"location":"architecture/technical-reference/#authentication-handling","title":"Authentication Handling","text":"<p>Priority order: 1. <code>spec.api_key</code> (explicit in config) 2. <code>OPENAI_COMPATIBLE_API_KEY</code> env var 3. <code>\"dummy\"</code> fallback (for local APIs without auth)</p> <p>Design: Flexible authentication for different scenarios</p>"},{"location":"architecture/technical-reference/#provider-naming","title":"Provider Naming","text":"<p>The <code>provider_name</code> field appears in metrics/logging: <pre><code>model=f\"{self.provider_name}/{self.model}\"\n# Example: \"Together.AI/meta-llama/Llama-3.1-70B\"\n</code></pre></p> <p>Benefit: Clear identification in logs and cost tracking</p>"},{"location":"architecture/technical-reference/#dependencies","title":"Dependencies","text":"<pre><code>from llama_index.llms.openai import OpenAI  # Reuse OpenAI client\nimport tiktoken  # Token estimation\n</code></pre>"},{"location":"architecture/technical-reference/#used-by","title":"Used By","text":"<ul> <li><code>create_llm_client()</code> factory</li> <li>Any pipeline with <code>provider: openai_compatible</code></li> <li>Examples: Ollama, Together.AI, vLLM configs</li> </ul>"},{"location":"architecture/technical-reference/#testing-strategy","title":"Testing Strategy","text":"<p>Unit Tests (14 tests): - Validation (base_url requirement) - Provider naming - Dummy API keys for local APIs - Cost calculation with custom pricing - Factory integration - Backward compatibility</p>"},{"location":"architecture/technical-reference/#known-limitations","title":"Known Limitations","text":"<ul> <li>Assumes OpenAI-compatible format (doesn't support exotic APIs)</li> <li>Token counting is approximate (cl100k_base encoding)</li> <li>Can't use provider-specific features (only OpenAI format)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements","title":"Future Improvements","text":"<ul> <li>[ ] Support custom headers (for exotic auth)</li> <li>[ ] Add provider-specific tokenizers</li> <li>[ ] Support streaming responses</li> <li>[ ] Add connection pooling for performance</li> </ul>"},{"location":"architecture/technical-reference/#part-3-layer-0-core-utilities","title":"Part 3: Layer 0 - Core Utilities","text":""},{"location":"architecture/technical-reference/#31-overview","title":"3.1 Overview","text":"<p>Layer 0 provides cross-cutting concerns that are used throughout the system: - Retry logic with exponential backoff - Rate limiting (token bucket algorithm) - Cost tracking and accumulation - Budget enforcement - Structured logging - Metrics export (Prometheus) - Input text preprocessing</p> <p>Design Principle: These utilities have no dependencies on higher layers, only on external libraries.</p>"},{"location":"architecture/technical-reference/#32-utilsretry_handlerpy","title":"3.2 <code>utils/retry_handler.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_1","title":"Purpose","text":"<p>Implements robust retry logic with exponential backoff for transient failures.</p>"},{"location":"architecture/technical-reference/#classes","title":"Classes","text":""},{"location":"architecture/technical-reference/#retryableerror-exception","title":"<code>RetryableError</code> (Exception)","text":"<p>Inheritance: <code>Exception</code></p> <p>Purpose: Base class for errors that should be retried</p> <p>Design Decision: Create custom exception hierarchy to distinguish retryable vs. non-retryable errors</p> <pre><code>class RetryableError(Exception):\n    \"\"\"Base class for errors that should be retried.\"\"\"\n    pass\n</code></pre>"},{"location":"architecture/technical-reference/#ratelimiterror-exception","title":"<code>RateLimitError</code> (Exception)","text":"<p>Inheritance: <code>RetryableError</code></p> <p>Purpose: Specific error for rate limit scenarios</p> <p>Usage: Raised by LLM clients when rate limits are hit</p>"},{"location":"architecture/technical-reference/#networkerror-exception","title":"<code>NetworkError</code> (Exception)","text":"<p>Inheritance: <code>RetryableError</code></p> <p>Purpose: Specific error for network-related failures</p> <p>Usage: Raised when network calls fail transiently</p>"},{"location":"architecture/technical-reference/#retryhandler-class","title":"<code>RetryHandler</code> (Class)","text":"<p>Responsibility: Handle retry logic with configurable exponential backoff</p> <p>Single Responsibility: ONLY handles retry logic, nothing else</p> <p>Attributes: <pre><code>max_attempts: int           # Maximum retry attempts (default: 3)\ninitial_delay: float        # Initial delay in seconds (default: 1.0)\nmax_delay: float            # Maximum delay cap (default: 60.0)\nexponential_base: int       # Base for exponential calculation (default: 2)\nretryable_exceptions: tuple # Which exceptions to retry\n</code></pre></p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__max_attempts-initial_delay-max_delay-exponential_base-retryable_exceptions","title":"<code>__init__(max_attempts, initial_delay, max_delay, exponential_base, retryable_exceptions)</code>","text":"<p>Purpose: Initialize retry configuration</p> <p>Parameters: - <code>max_attempts: int = 3</code> - How many times to retry - <code>initial_delay: float = 1.0</code> - Starting delay - <code>max_delay: float = 60.0</code> - Maximum delay cap - <code>exponential_base: int = 2</code> - Exponent base (delay = initial * base^attempt) - <code>retryable_exceptions: Optional[tuple] = None</code> - Which exceptions trigger retry</p> <p>Design Decision: Make all parameters configurable for flexibility across different failure scenarios</p> <p>Default Behavior: If <code>retryable_exceptions</code> is None, defaults to <code>(RetryableError, RateLimitError, NetworkError)</code></p>"},{"location":"architecture/technical-reference/#executefunc-callable-t-t","title":"<code>execute(func: Callable[[], T]) -&gt; T</code>","text":"<p>Purpose: Execute a function with retry logic</p> <p>Algorithm: 1. Create <code>Retrying</code> instance from tenacity library 2. Configure stop condition: <code>stop_after_attempt(max_attempts)</code> 3. Configure wait strategy: exponential backoff with <code>wait_exponential()</code> 4. Configure retry condition: <code>retry_if_exception_type(retryable_exceptions)</code> 5. Execute function through retryer 6. If all retries fail, reraise last exception</p> <p>Parameters: - <code>func: Callable[[], T]</code> - Zero-argument function to execute</p> <p>Returns: <code>T</code> - Result from successful function execution</p> <p>Raises: Last exception if all retries exhausted</p> <p>Example: <pre><code>retry_handler = RetryHandler(max_attempts=3, initial_delay=1.0)\n\ndef risky_operation():\n    # Might fail with RateLimitError\n    return call_llm_api()\n\nresult = retry_handler.execute(risky_operation)\n</code></pre></p> <p>Design Decision: Use tenacity library instead of custom implementation - Why: tenacity is battle-tested, handles edge cases, provides flexible configuration - Alternative: Custom retry loop (simpler but less robust) - Trade-off: Additional dependency vs. reliability</p>"},{"location":"architecture/technical-reference/#calculate_delayattempt-int-float","title":"<code>calculate_delay(attempt: int) -&gt; float</code>","text":"<p>Purpose: Calculate delay for given attempt number (for informational/testing purposes)</p> <p>Algorithm: <pre><code>delay = initial_delay * (exponential_base ** (attempt - 1))\nreturn min(delay, max_delay)\n</code></pre></p> <p>Parameters: - <code>attempt: int</code> - Attempt number (1-based)</p> <p>Returns: <code>float</code> - Delay in seconds</p> <p>Example: <pre><code>handler = RetryHandler(initial_delay=1.0, exponential_base=2, max_delay=60.0)\nhandler.calculate_delay(1)  # 1.0 second\nhandler.calculate_delay(2)  # 2.0 seconds\nhandler.calculate_delay(3)  # 4.0 seconds\nhandler.calculate_delay(4)  # 8.0 seconds\nhandler.calculate_delay(10) # 60.0 seconds (capped at max_delay)\n</code></pre></p>"},{"location":"architecture/technical-reference/#thread-safety","title":"Thread Safety","text":"<ul> <li>Thread-safe: Yes (stateless operation, no shared mutable state)</li> <li>Each <code>execute()</code> call is independent</li> <li>tenacity handles thread safety internally</li> </ul>"},{"location":"architecture/technical-reference/#dependencies_1","title":"Dependencies","text":"<pre><code>import time                  # For delays\nfrom typing import Callable  # Type hints\nfrom tenacity import (       # Retry library\n    Retrying,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_1","title":"Used By","text":"<ul> <li><code>LLMInvocationStage</code> - Retries LLM API calls</li> <li>Any component needing retry logic</li> </ul>"},{"location":"architecture/technical-reference/#design-patterns","title":"Design Patterns","text":"<ol> <li>Strategy Pattern: Configurable retry strategy via parameters</li> <li>Dependency Inversion: Uses abstract <code>Callable</code> type, not specific implementations</li> </ol>"},{"location":"architecture/technical-reference/#time-complexity","title":"Time Complexity","text":"<ul> <li>Exponential backoff: O(2^n) time in worst case (where n = max_attempts)</li> <li>Actual delays: 1s, 2s, 4s, 8s, 16s, ...</li> </ul>"},{"location":"architecture/technical-reference/#testing-considerations","title":"Testing Considerations","text":"<ul> <li>Test with mocked functions that fail predictably</li> <li>Verify exponential backoff timing</li> <li>Test max_attempts enforcement</li> <li>Test exception filtering (retryable vs. non-retryable)</li> </ul>"},{"location":"architecture/technical-reference/#known-limitations_1","title":"Known Limitations","text":"<ul> <li>No jitter (could add randomness to prevent thundering herd)</li> <li>Fixed exponential formula (could support other backoff strategies)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_1","title":"Future Improvements","text":"<ul> <li>[ ] Add jitter support for distributed systems</li> <li>[ ] Support custom backoff strategies (linear, polynomial)</li> <li>[ ] Add metrics for retry count tracking</li> </ul>"},{"location":"architecture/technical-reference/#33-utilsrate_limiterpy","title":"3.3 <code>utils/rate_limiter.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_2","title":"Purpose","text":"<p>Implements token bucket algorithm for rate limiting API calls.</p>"},{"location":"architecture/technical-reference/#classes_1","title":"Classes","text":""},{"location":"architecture/technical-reference/#ratelimiter-class","title":"<code>RateLimiter</code> (Class)","text":"<p>Responsibility: Control request rate using token bucket algorithm</p> <p>Algorithm: Token Bucket - Bucket has maximum capacity (burst_size) - Tokens refill at constant rate (requests_per_minute / 60) - Each request consumes tokens - If no tokens available, wait until refilled</p> <p>Thread Safety: Thread-safe via <code>threading.Lock</code></p> <p>Attributes: <pre><code>rpm: int                    # Requests per minute limit\ncapacity: int               # Maximum burst size\ntokens: float               # Current available tokens\nlast_update: float          # Last refill timestamp\nlock: threading.Lock        # Thread safety\nrefill_rate: float          # Tokens per second\n</code></pre></p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__requests_per_minute-int-burst_size-optionalint-none","title":"<code>__init__(requests_per_minute: int, burst_size: Optional[int] = None)</code>","text":"<p>Purpose: Initialize rate limiter with capacity and refill rate</p> <p>Parameters: - <code>requests_per_minute: int</code> - Maximum requests allowed per minute - <code>burst_size: Optional[int] = None</code> - Maximum burst size (defaults to <code>requests_per_minute</code>)</p> <p>Design Decision: Separate burst_size from requests_per_minute - Why: Allow bursts up to capacity, then throttle to sustained rate - Example: <code>rpm=60, burst=120</code> allows 120 immediate requests, then throttles to 1/second</p> <p>Initialization: <pre><code>self.rpm = requests_per_minute\nself.capacity = burst_size or requests_per_minute\nself.tokens = float(self.capacity)  # Start with full capacity\nself.last_update = time.time()\nself.lock = threading.Lock()\nself.refill_rate = requests_per_minute / 60.0  # Tokens per second\n</code></pre></p>"},{"location":"architecture/technical-reference/#acquiretokens-int-1-timeout-optionalfloat-none-bool","title":"<code>acquire(tokens: int = 1, timeout: Optional[float] = None) -&gt; bool</code>","text":"<p>Purpose: Acquire tokens for making requests (blocks until available)</p> <p>Algorithm: 1. Check if requested tokens &lt;= capacity (raise ValueError if not) 2. Loop:    a. Acquire lock    b. Refill tokens based on elapsed time    c. If sufficient tokens available:       - Deduct tokens       - Return True    d. Release lock    e. Check timeout    f. Sleep briefly (0.1s) before retry</p> <p>Parameters: - <code>tokens: int = 1</code> - Number of tokens to acquire - <code>timeout: Optional[float] = None</code> - Maximum wait time (None = wait forever)</p> <p>Returns: <code>bool</code> - <code>True</code> if tokens acquired - <code>False</code> if timeout reached</p> <p>Raises: <code>ValueError</code> if <code>tokens &gt; capacity</code></p> <p>Thread Safety: Lock protects token bucket state during check-and-decrement</p> <p>Example: <pre><code>limiter = RateLimiter(requests_per_minute=60, burst_size=120)\n\n# Acquire 1 token (default)\nif limiter.acquire():\n    make_api_call()\n\n# Acquire with timeout\nif limiter.acquire(tokens=1, timeout=5.0):\n    make_api_call()\nelse:\n    print(\"Timeout waiting for rate limit\")\n\n# Acquire multiple tokens\nif limiter.acquire(tokens=5):\n    make_batch_api_call(batch_size=5)\n</code></pre></p>"},{"location":"architecture/technical-reference/#_refill-none","title":"<code>_refill() -&gt; None</code>","text":"<p>Purpose: Refill tokens based on elapsed time (internal method)</p> <p>Algorithm: <pre><code>now = time.time()\nelapsed = now - self.last_update\ntokens_to_add = elapsed * self.refill_rate\nself.tokens = min(self.capacity, self.tokens + tokens_to_add)\nself.last_update = now\n</code></pre></p> <p>Design Decision: Continuous refill vs. discrete intervals - Chosen: Continuous refill (calculate tokens based on elapsed time) - Why: More accurate, smoother rate limiting - Alternative: Refill in discrete chunks (simpler but less accurate)</p> <p>Thread Safety: Called only while holding lock</p>"},{"location":"architecture/technical-reference/#available_tokens-property","title":"<code>available_tokens</code> (Property)","text":"<p>Purpose: Get current available tokens (thread-safe)</p> <p>Returns: <code>float</code> - Current token count</p> <p>Thread Safety: Acquires lock, refills, returns tokens</p> <p>Example: <pre><code>limiter = RateLimiter(requests_per_minute=60)\nprint(f\"Available: {limiter.available_tokens}\")  # e.g., 45.3\n</code></pre></p>"},{"location":"architecture/technical-reference/#reset-none","title":"<code>reset() -&gt; None</code>","text":"<p>Purpose: Reset rate limiter to full capacity</p> <p>Thread Safety: Acquires lock, resets tokens and timestamp</p> <p>Use Case: Manual reset after known idle period</p>"},{"location":"architecture/technical-reference/#token-bucket-algorithm","title":"Token Bucket Algorithm","text":"<p>Visual: <pre><code>Bucket (capacity = 100)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593    \u2502  Current: 80 tokens\n\u2502             \u2502  Refill rate: 60/minute = 1/second\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAfter 5 seconds:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593 \u2502  Current: 85 tokens (80 + 5*1)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAfter acquire(10):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593    \u2502  Current: 75 tokens\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Properties: - Allows bursts up to capacity - Refills continuously at constant rate - Smooth rate limiting (no hard boundaries)</p>"},{"location":"architecture/technical-reference/#dependencies_2","title":"Dependencies","text":"<pre><code>import threading          # Lock for thread safety\nimport time              # Timestamp tracking\nfrom typing import Optional\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_2","title":"Used By","text":"<ul> <li><code>LLMInvocationStage</code> - Rate limit API calls</li> <li>Any component needing request throttling</li> </ul>"},{"location":"architecture/technical-reference/#thread-safety-details","title":"Thread Safety Details","text":"<pre><code>with self.lock:  # Critical section\n    self._refill()\n    if self.tokens &gt;= tokens:\n        self.tokens -= tokens\n        return True\n</code></pre> <p>Race Condition Prevention: - Without lock: Two threads could both see sufficient tokens, both decrement, exceed capacity - With lock: Only one thread can check-and-decrement at a time</p>"},{"location":"architecture/technical-reference/#performance","title":"Performance","text":"<ul> <li>Time Complexity: O(1) per acquire</li> <li>Space Complexity: O(1) - constant space</li> <li>Lock Contention: Low (critical section is very short)</li> </ul>"},{"location":"architecture/technical-reference/#configuration-examples","title":"Configuration Examples","text":"<pre><code># Conservative (prevent rate limits)\nlimiter = RateLimiter(requests_per_minute=30, burst_size=30)\n\n# Aggressive (maximize throughput)\nlimiter = RateLimiter(requests_per_minute=100, burst_size=200)\n\n# Bursty workload (allow initial burst, then throttle)\nlimiter = RateLimiter(requests_per_minute=60, burst_size=180)\n</code></pre>"},{"location":"architecture/technical-reference/#testing","title":"Testing","text":"<pre><code>def test_rate_limiter():\n    limiter = RateLimiter(requests_per_minute=60)\n\n    # Should acquire immediately\n    assert limiter.acquire(timeout=0.1)\n\n    # Deplete tokens\n    for _ in range(59):\n        limiter.acquire()\n\n    # Should timeout (no tokens left)\n    assert not limiter.acquire(timeout=0.1)\n\n    # Wait for refill\n    time.sleep(2.0)\n    assert limiter.acquire()  # Should have ~2 tokens refilled\n</code></pre>"},{"location":"architecture/technical-reference/#known-limitations_2","title":"Known Limitations","text":"<ul> <li>No distributed rate limiting (single-process only)</li> <li>Fixed refill rate (can't adjust dynamically)</li> <li>Polling-based waiting (could use condition variables)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_2","title":"Future Improvements","text":"<ul> <li>[ ] Add distributed rate limiting (Redis-based)</li> <li>[ ] Support dynamic rate adjustment</li> <li>[ ] Use condition variables instead of polling</li> <li>[ ] Add metrics for rate limit hits</li> </ul>"},{"location":"architecture/technical-reference/#34-utilscost_trackerpy","title":"3.4 <code>utils/cost_tracker.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_3","title":"Purpose","text":"<p>Thread-safe cost tracking for LLM API usage with detailed breakdowns.</p>"},{"location":"architecture/technical-reference/#classes_2","title":"Classes","text":""},{"location":"architecture/technical-reference/#costentry-dataclass","title":"<code>CostEntry</code> (Dataclass)","text":"<p>Purpose: Single cost tracking entry</p> <p>Attributes: <pre><code>tokens_in: int      # Input tokens consumed\ntokens_out: int     # Output tokens generated\ncost: Decimal       # Cost for this entry\nmodel: str          # Model identifier\ntimestamp: float    # When request occurred\n</code></pre></p> <p>Design Decision: Use dataclass for simplicity and immutability</p>"},{"location":"architecture/technical-reference/#costtracker-class","title":"<code>CostTracker</code> (Class)","text":"<p>Responsibility: Accumulate and track costs across LLM calls</p> <p>Single Responsibility: ONLY handles cost accounting, not enforcement (that's BudgetController's job)</p> <p>Thread Safety: Thread-safe via <code>threading.Lock</code></p> <p>Attributes: <pre><code>input_cost_per_1k: Decimal       # Input token price per 1K tokens\noutput_cost_per_1k: Decimal      # Output token price per 1K tokens\n_total_input_tokens: int         # Cumulative input tokens\n_total_output_tokens: int        # Cumulative output tokens\n_total_cost: Decimal             # Cumulative cost (Decimal for precision)\n_entries: list[CostEntry]        # Detailed entry log\n_stage_costs: Dict[str, Decimal] # Cost breakdown by stage\n_lock: threading.Lock            # Thread safety\n</code></pre></p> <p>Design Decision: Use <code>Decimal</code> for cost (not <code>float</code>) - Why: Avoid floating-point precision errors (<code>0.1 + 0.2 != 0.3</code>) - Critical: Financial calculations must be exact - Trade-off: Slightly slower than float, but necessary for accuracy</p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__input_cost_per_1k-output_cost_per_1k","title":"<code>__init__(input_cost_per_1k, output_cost_per_1k)</code>","text":"<p>Purpose: Initialize cost tracker with pricing</p> <p>Parameters: - <code>input_cost_per_1k: Optional[Decimal] = None</code> - Price per 1K input tokens - <code>output_cost_per_1k: Optional[Decimal] = None</code> - Price per 1K output tokens</p> <p>Default: Both default to <code>Decimal(\"0.0\")</code> if not provided</p> <p>Initialization: <pre><code>self.input_cost_per_1k = input_cost_per_1k or Decimal(\"0.0\")\nself.output_cost_per_1k = output_cost_per_1k or Decimal(\"0.0\")\nself._total_input_tokens = 0\nself._total_output_tokens = 0\nself._total_cost = Decimal(\"0.0\")\nself._entries = []\nself._stage_costs = {}\nself._lock = threading.Lock()\n</code></pre></p>"},{"location":"architecture/technical-reference/#addtokens_in-tokens_out-model-timestamp-stage-decimal","title":"<code>add(tokens_in, tokens_out, model, timestamp, stage) -&gt; Decimal</code>","text":"<p>Purpose: Add cost entry and return cost for this operation</p> <p>Algorithm: 1. Calculate cost: <code>(tokens_in/1000) * input_price + (tokens_out/1000) * output_price</code> 2. Acquire lock 3. Create <code>CostEntry</code> and append to <code>_entries</code> 4. Accumulate tokens and cost 5. Update stage-specific costs 6. Release lock 7. Return calculated cost</p> <p>Parameters: - <code>tokens_in: int</code> - Input tokens used - <code>tokens_out: int</code> - Output tokens generated - <code>model: str</code> - Model identifier - <code>timestamp: float</code> - Request timestamp - <code>stage: Optional[str] = None</code> - Stage name for breakdown</p> <p>Returns: <code>Decimal</code> - Cost for this entry</p> <p>Thread Safety: Entire operation protected by lock to ensure atomic update</p> <p>Example: <pre><code>tracker = CostTracker(\n    input_cost_per_1k=Decimal(\"0.00005\"),  # $0.05 per 1M input tokens\n    output_cost_per_1k=Decimal(\"0.00008\"),  # $0.08 per 1M output tokens\n)\n\ncost = tracker.add(\n    tokens_in=1000,\n    tokens_out=500,\n    model=\"gpt-4o-mini\",\n    timestamp=time.time(),\n    stage=\"llm_invocation\"\n)\n# cost = (1000/1000 * 0.00005) + (500/1000 * 0.00008)\n#      = 0.00005 + 0.00004\n#      = 0.00009 ($0.00009)\n</code></pre></p>"},{"location":"architecture/technical-reference/#calculate_costtokens_in-tokens_out-decimal","title":"<code>calculate_cost(tokens_in, tokens_out) -&gt; Decimal</code>","text":"<p>Purpose: Calculate cost for given token counts (without recording)</p> <p>Algorithm: <pre><code>input_cost = (Decimal(tokens_in) / 1000) * self.input_cost_per_1k\noutput_cost = (Decimal(tokens_out) / 1000) * self.output_cost_per_1k\nreturn input_cost + output_cost\n</code></pre></p> <p>Use Case: Estimate cost before making request</p> <p>Example: <pre><code>estimated = tracker.calculate_cost(tokens_in=500, tokens_out=200)\nif estimated &gt; Decimal(\"0.01\"):  # More than 1 cent\n    print(\"Expensive request!\")\n</code></pre></p>"},{"location":"architecture/technical-reference/#total_cost-property","title":"<code>total_cost</code> (Property)","text":"<p>Purpose: Get total accumulated cost (thread-safe)</p> <p>Returns: <code>Decimal</code> - Total cost</p> <p>Thread Safety: Acquires lock, reads <code>_total_cost</code>, releases lock</p>"},{"location":"architecture/technical-reference/#total_tokens-input_tokens-output_tokens-properties","title":"<code>total_tokens</code>, <code>input_tokens</code>, <code>output_tokens</code> (Properties)","text":"<p>Purpose: Get token counts (thread-safe)</p> <p>Returns: <code>int</code> - Token count</p>"},{"location":"architecture/technical-reference/#get_estimaterows-costestimate","title":"<code>get_estimate(rows) -&gt; CostEstimate</code>","text":"<p>Purpose: Create cost estimate object from current tracking</p> <p>Returns: <code>CostEstimate</code> model with: - <code>total_cost</code> - <code>total_tokens</code> - <code>input_tokens</code> - <code>output_tokens</code> - <code>rows</code> - Number of rows processed - <code>breakdown_by_stage</code> - Dict of stage costs - <code>confidence=\"actual\"</code> - These are actual costs, not estimates</p>"},{"location":"architecture/technical-reference/#reset-none_1","title":"<code>reset() -&gt; None</code>","text":"<p>Purpose: Clear all tracking data (thread-safe)</p> <p>Use Case: Reset between test runs or pipeline executions</p>"},{"location":"architecture/technical-reference/#get_stage_costs-dictstr-decimal","title":"<code>get_stage_costs() -&gt; Dict[str, Decimal]</code>","text":"<p>Purpose: Get cost breakdown by stage</p> <p>Returns: Dictionary mapping stage names to costs</p> <p>Example: <pre><code>stage_costs = tracker.get_stage_costs()\n# {\"llm_invocation\": Decimal(\"0.15\"), \"retry\": Decimal(\"0.02\")}\n</code></pre></p>"},{"location":"architecture/technical-reference/#dependencies_3","title":"Dependencies","text":"<pre><code>import threading                      # Thread safety\nfrom dataclasses import dataclass     # CostEntry\nfrom decimal import Decimal           # Precise financial calculations\nfrom typing import Dict, Optional\nfrom src.core.models import CostEstimate  # Result model\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_3","title":"Used By","text":"<ul> <li><code>LLMInvocationStage</code> - Track costs during execution</li> <li><code>PipelineExecutor</code> - Get total costs for result</li> <li><code>BudgetController</code> - Check against budget limits</li> </ul>"},{"location":"architecture/technical-reference/#thread-safety-example","title":"Thread Safety Example","text":"<pre><code># Two threads executing concurrently:\n# Thread 1: tracker.add(tokens_in=1000, tokens_out=500, ...)\n# Thread 2: tracker.add(tokens_in=800, tokens_out=300, ...)\n\n# Without lock: Race condition\n# 1. T1 reads _total_cost: 0.00\n# 2. T2 reads _total_cost: 0.00\n# 3. T1 writes _total_cost: 0.10\n# 4. T2 writes _total_cost: 0.08  # LOST UPDATE! T1's cost is lost\n# Final: 0.08 (WRONG, should be 0.18)\n\n# With lock: Correct\n# 1. T1 acquires lock\n# 2. T1 reads _total_cost: 0.00, calculates 0.10, writes 0.10\n# 3. T1 releases lock\n# 4. T2 acquires lock\n# 5. T2 reads _total_cost: 0.10, calculates 0.08, writes 0.18\n# 6. T2 releases lock\n# Final: 0.18 (CORRECT)\n</code></pre>"},{"location":"architecture/technical-reference/#decimal-precision-example","title":"Decimal Precision Example","text":"<pre><code># Why Decimal is necessary:\nfrom decimal import Decimal\n\n# Float (WRONG):\nfloat_cost = 0.1 + 0.2  # 0.30000000000000004 (WRONG!)\n\n# Decimal (CORRECT):\ndecimal_cost = Decimal(\"0.1\") + Decimal(\"0.2\")  # 0.3 (CORRECT!)\n\n# Real-world impact:\n# If processing 1,000,000 rows with $0.0001 per row:\n# Float: $100.00000000014 (accumulated error)\n# Decimal: $100.00 (exact)\n</code></pre>"},{"location":"architecture/technical-reference/#testing_1","title":"Testing","text":"<pre><code>def test_cost_tracker():\n    tracker = CostTracker(\n        input_cost_per_1k=Decimal(\"0.00005\"),\n        output_cost_per_1k=Decimal(\"0.00008\"),\n    )\n\n    # Add cost\n    cost1 = tracker.add(tokens_in=1000, tokens_out=500, model=\"test\", timestamp=0.0)\n    assert cost1 == Decimal(\"0.00009\")\n\n    # Check accumulation\n    assert tracker.total_cost == Decimal(\"0.00009\")\n    assert tracker.total_tokens == 1500\n\n    # Add more\n    cost2 = tracker.add(tokens_in=2000, tokens_out=1000, model=\"test\", timestamp=1.0)\n    assert tracker.total_cost == Decimal(\"0.00027\")  # 0.00009 + 0.00018\n</code></pre>"},{"location":"architecture/technical-reference/#known-limitations_3","title":"Known Limitations","text":"<ul> <li>Stores all entries in memory (could grow large for long runs)</li> <li>No persistence (lost on crash)</li> <li>No cost cap enforcement (that's BudgetController's job)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_3","title":"Future Improvements","text":"<ul> <li>[ ] Add entry pruning for long-running pipelines</li> <li>[ ] Optional persistence to disk</li> <li>[ ] Add cost forecasting based on trends</li> <li>[ ] Support tiered pricing (cost changes with volume)</li> </ul>"},{"location":"architecture/technical-reference/#35-utilsbudget_controllerpy","title":"3.5 <code>utils/budget_controller.py</code>","text":""},{"location":"architecture/technical-reference/#purpose_4","title":"Purpose","text":"<p>Enforce budget limits and provide warnings during execution.</p>"},{"location":"architecture/technical-reference/#classes_3","title":"Classes","text":""},{"location":"architecture/technical-reference/#budgetexceedederror-exception","title":"<code>BudgetExceededError</code> (Exception)","text":"<p>Inheritance: <code>Exception</code></p> <p>Purpose: Raised when budget limit is exceeded</p> <p>Usage: Allows caller to catch and handle budget overruns</p>"},{"location":"architecture/technical-reference/#_1","title":"Technical Reference","text":"<p><code>BudgetController</code> (Class)</p> <p>Responsibility: Monitor costs and enforce budget limits</p> <p>Single Responsibility: ONLY handles budget management (cost tracking is CostTracker's job)</p> <p>Separation of Concerns: - <code>CostTracker</code>: Accumulates costs - <code>BudgetController</code>: Enforces limits</p> <p>Attributes: <pre><code>max_budget: Optional[Decimal]  # Maximum allowed budget (None = no limit)\nwarn_at_75: bool              # Warn at 75% of budget\nwarn_at_90: bool              # Warn at 90% of budget\nfail_on_exceed: bool          # Raise error if budget exceeded\n_warned_75: bool              # Track if 75% warning already shown\n_warned_90: bool              # Track if 90% warning already shown\n</code></pre></p> <p>Design Decision: Separate warning flags from budget - Why: Show each warning only once - Alternative: Check every time (would spam logs)</p> <p>Methods:</p>"},{"location":"architecture/technical-reference/#__init__max_budget-warn_at_75-warn_at_90-fail_on_exceed","title":"<code>__init__(max_budget, warn_at_75, warn_at_90, fail_on_exceed)</code>","text":"<p>Purpose: Initialize budget controller with limits</p> <p>Parameters: - <code>max_budget: Optional[Decimal] = None</code> - Maximum budget in USD - <code>warn_at_75: bool = True</code> - Warn at 75% usage - <code>warn_at_90: bool = True</code> - Warn at 90% usage - <code>fail_on_exceed: bool = True</code> - Raise error when exceeded</p> <p>Default Behavior: Warn twice, then fail</p> <p>Flexible Configuration: <pre><code># Strict (default): Fail on exceed\ncontroller = BudgetController(max_budget=Decimal(\"10.0\"))\n\n# Warn only: Don't fail\ncontroller = BudgetController(max_budget=Decimal(\"10.0\"), fail_on_exceed=False)\n\n# No warnings: Just hard limit\ncontroller = BudgetController(\n    max_budget=Decimal(\"10.0\"),\n    warn_at_75=False,\n    warn_at_90=False,\n)\n</code></pre></p>"},{"location":"architecture/technical-reference/#check_budgetcurrent_cost-decimal-none","title":"<code>check_budget(current_cost: Decimal) -&gt; None</code>","text":"<p>Purpose: Check if cost is within budget, warn or fail as configured</p> <p>Algorithm: 1. If <code>max_budget</code> is None, return (no limit) 2. Calculate usage ratio: <code>current_cost / max_budget</code> 3. If &gt;= 0.75 and not warned: Log warning, set <code>_warned_75 = True</code> 4. If &gt;= 0.90 and not warned: Log warning, set <code>_warned_90 = True</code> 5. If &gt; <code>max_budget</code>:    - Log error    - If <code>fail_on_exceed</code>: raise <code>BudgetExceededError</code></p> <p>Parameters: - <code>current_cost: Decimal</code> - Current accumulated cost</p> <p>Raises: <code>BudgetExceededError</code> if cost exceeds budget and <code>fail_on_exceed=True</code></p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\n\n# Check after each operation\ncontroller.check_budget(Decimal(\"5.0\"))   # OK\ncontroller.check_budget(Decimal(\"7.5\"))   # Logs: \"75% used\"\ncontroller.check_budget(Decimal(\"9.0\"))   # Logs: \"90% used\"\ncontroller.check_budget(Decimal(\"10.5\"))  # Raises BudgetExceededError\n</code></pre></p> <p>Design Decision: Warnings at 75% and 90% - Why: Give user time to react before hitting limit - 75%: Early warning - 90%: Final warning before failure - Alternative: More granular warnings (every 10%) would spam logs</p>"},{"location":"architecture/technical-reference/#get_remainingcurrent_cost-decimal-optionaldecimal","title":"<code>get_remaining(current_cost: Decimal) -&gt; Optional[Decimal]</code>","text":"<p>Purpose: Calculate remaining budget</p> <p>Returns: - <code>Decimal</code> - Remaining budget - <code>None</code> if no budget limit set</p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\nremaining = controller.get_remaining(Decimal(\"7.5\"))\n# remaining = Decimal(\"2.5\")\n</code></pre></p>"},{"location":"architecture/technical-reference/#get_usage_percentagecurrent_cost-decimal-optionalfloat","title":"<code>get_usage_percentage(current_cost: Decimal) -&gt; Optional[float]</code>","text":"<p>Purpose: Calculate budget usage as percentage</p> <p>Returns: - <code>float</code> - Usage percentage (0-100+) - <code>None</code> if no budget limit set</p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\npct = controller.get_usage_percentage(Decimal(\"7.5\"))\n# pct = 75.0\n</code></pre></p>"},{"location":"architecture/technical-reference/#can_affordestimated_cost-decimal-current_cost-decimal-bool","title":"<code>can_afford(estimated_cost: Decimal, current_cost: Decimal) -&gt; bool</code>","text":"<p>Purpose: Check if estimated additional cost fits within budget</p> <p>Algorithm: <pre><code>if self.max_budget is None:\n    return True\nreturn (current_cost + estimated_cost) &lt;= self.max_budget\n</code></pre></p> <p>Use Case: Pre-flight check before expensive operation</p> <p>Example: <pre><code>controller = BudgetController(max_budget=Decimal(\"10.0\"))\ncurrent = Decimal(\"9.0\")\nestimated_next = Decimal(\"0.5\")\n\nif controller.can_afford(estimated_next, current):\n    proceed_with_operation()\nelse:\n    print(\"Would exceed budget, skipping\")\n</code></pre></p>"},{"location":"architecture/technical-reference/#dependencies_4","title":"Dependencies","text":"<pre><code>from decimal import Decimal       # Precise financial calculations\nfrom typing import Optional\nimport structlog                  # Structured logging\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_4","title":"Used By","text":"<ul> <li><code>PipelineExecutor</code> - Check budget during execution</li> <li>Any component needing cost enforcement</li> </ul>"},{"location":"architecture/technical-reference/#integration-with-costtracker","title":"Integration with CostTracker","text":"<pre><code># Typical usage pattern:\ncost_tracker = CostTracker(...)\nbudget_controller = BudgetController(max_budget=Decimal(\"10.0\"))\n\nfor row in data:\n    # Make LLM call\n    response = llm.invoke(prompt)\n\n    # Track cost\n    cost = cost_tracker.add(tokens_in=..., tokens_out=...)\n\n    # Check budget\n    budget_controller.check_budget(cost_tracker.total_cost)\n</code></pre>"},{"location":"architecture/technical-reference/#testing_2","title":"Testing","text":"<pre><code>def test_budget_warnings():\n    controller = BudgetController(max_budget=Decimal(\"10.0\"))\n\n    # Should not warn\n    controller.check_budget(Decimal(\"5.0\"))\n\n    # Should warn at 75%\n    controller.check_budget(Decimal(\"7.5\"))\n\n    # Should warn at 90%\n    controller.check_budget(Decimal(\"9.0\"))\n\n    # Should raise error\n    with pytest.raises(BudgetExceededError):\n        controller.check_budget(Decimal(\"10.1\"))\n</code></pre>"},{"location":"architecture/technical-reference/#error-messages","title":"Error Messages","text":"<pre><code># 75% warning:\n\"Budget warning: 75% used ($7.50 / $10.00)\"\n\n# 90% warning:\n\"Budget warning: 90% used ($9.00 / $10.00)\"\n\n# Budget exceeded:\n\"Budget exceeded: $10.50 &gt; $10.00\"\n</code></pre>"},{"location":"architecture/technical-reference/#known-limitations_4","title":"Known Limitations","text":"<ul> <li>No budget rollover (each execution is independent)</li> <li>No budget sharing across pipelines</li> <li>Fixed warning thresholds (75%, 90%)</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_4","title":"Future Improvements","text":"<ul> <li>[ ] Configurable warning thresholds</li> <li>[ ] Support budget pools (shared across pipelines)</li> <li>[ ] Budget rollover support</li> <li>[ ] Budget forecasting (estimate when limit will be hit)</li> </ul> <p>This is Part 1 of the Technical Reference. Continue reading for Layer 0 (remaining utils), Core Models, and all other layers...</p>"},{"location":"architecture/technical-reference/#part-5-layer-1-infrastructure-adapters-llm-providers_1","title":"Part 5: Layer 1 - Infrastructure Adapters (LLM Providers)","text":""},{"location":"architecture/technical-reference/#51-llm-provider-overview_1","title":"5.1 LLM Provider Overview","text":"<p>Ondine supports multiple LLM providers through the Adapter pattern, allowing easy switching between providers without changing core logic.</p>"},{"location":"architecture/technical-reference/#supported-providers_1","title":"Supported Providers","text":"Provider Category Platform Cost Use Case OpenAI Cloud API All $$ Production, high quality Azure OpenAI Cloud API All $$ Enterprise, compliance Anthropic Cloud API All $$$ Claude models, long context Groq Cloud API All Free tier Fast inference, development MLX Local macOS (Apple Silicon) Free Privacy, offline, no costs"},{"location":"architecture/technical-reference/#provider-selection-guide_1","title":"Provider Selection Guide","text":"<p>Choose OpenAI if: Production quality, mature ecosystem, GPT-4 Choose Azure if: Enterprise compliance, private deployments Choose Anthropic if: Claude models, 100K+ context Choose Groq if: Fast inference, free tier, development Choose MLX if: Apple Silicon Mac, privacy, free, offline capable</p>"},{"location":"architecture/technical-reference/#52-openai-compatible-provider-custom-apis_1","title":"5.2 OpenAI-Compatible Provider (Custom APIs)","text":""},{"location":"architecture/technical-reference/#purpose_5","title":"Purpose","text":"<p>Enable integration with any LLM API that implements the OpenAI chat completions format.</p>"},{"location":"architecture/technical-reference/#class-openaicompatibleclient_1","title":"Class: <code>OpenAICompatibleClient</code>","text":"<p>Inheritance: <code>LLMClient</code> (Adapter pattern)</p> <p>Platform: All (platform-agnostic)</p> <p>Responsibility: Connect to custom OpenAI-compatible API endpoints</p> <p>Supports: - Ollama (local LLM server) - vLLM (self-hosted inference) - Together.AI (cloud API) - Anyscale (cloud API) - LocalAI (self-hosted) - Any custom OpenAI-compatible API</p>"},{"location":"architecture/technical-reference/#configuration_1","title":"Configuration","text":"<p>Required Fields: - <code>provider: openai_compatible</code> - <code>base_url</code>: Custom API endpoint URL - <code>model</code>: Model identifier</p> <p>Optional Fields: - <code>provider_name</code>: Custom name for logging/metrics - <code>api_key</code>: Authentication (optional for local) - <code>input_cost_per_1k_tokens</code>: Custom pricing - <code>output_cost_per_1k_tokens</code>: Custom pricing</p>"},{"location":"architecture/technical-reference/#design-decision-reuse-openai-client","title":"Design Decision: Reuse OpenAI Client","text":"<p>Instead of reimplementing HTTP, leverage llama-index's OpenAI client:</p> <pre><code>self.client = OpenAI(\n    model=spec.model,\n    api_key=api_key or \"dummy\",\n    api_base=spec.base_url,  # Custom URL\n)\n</code></pre> <p>Rationale: - DRY: Reuse well-tested code - Reliability: Edge cases handled - Compatibility: Ensures OpenAI format - Maintainability: Benefit from upstream updates</p>"},{"location":"architecture/technical-reference/#example-togetherai","title":"Example: Together.AI","text":"<pre><code>llm:\n  provider: openai_compatible\n  provider_name: \"Together.AI\"\n  model: meta-llama/Llama-3.1-70B\n  base_url: https://api.together.xyz/v1\n  api_key: \\${TOGETHER_API_KEY}\n  input_cost_per_1k_tokens: 0.0006\n</code></pre>"},{"location":"architecture/technical-reference/#validation_1","title":"Validation","text":"<pre><code>@model_validator(mode=\"after\")\ndef validate_provider_requirements(self):\n    if self.provider == OPENAI_COMPATIBLE and not self.base_url:\n        raise ValueError(\"base_url required\")\n    return self\n</code></pre>"},{"location":"architecture/technical-reference/#53-mlx-provider-apple-silicon","title":"5.3 MLX Provider (Apple Silicon)","text":""},{"location":"architecture/technical-reference/#purpose_6","title":"Purpose","text":"<p>Enable fast, free, local LLM inference on Apple Silicon using Apple's MLX framework.</p>"},{"location":"architecture/technical-reference/#class-mlxclient","title":"Class: <code>MLXClient</code>","text":"<p>Inheritance: <code>LLMClient</code> (Adapter pattern)</p> <p>Platform: macOS with M1/M2/M3/M4 chips only</p> <p>Responsibility: Local in-process LLM inference using MLX framework</p> <p>Key Features: - In-process (no server management) - Model caching (load once, use many times) - Lazy imports (only when MLX provider used) - Dependency injection (clean testing) - Free inference ($0 cost)</p>"},{"location":"architecture/technical-reference/#architecture_1","title":"Architecture","text":"<pre><code>class MLXClient(LLMClient):\n    def __init__(self, spec: LLMSpec, _mlx_lm_module=None):\n        # Lazy import with helpful error\n        # Load model once (cached in instance)\n\n    def invoke(self, prompt: str, **kwargs) -&gt; LLMResponse:\n        # Use cached model for inference\n        # No server calls, all local\n</code></pre> <p>Design Decision: Dependency Injection</p> <p>The <code>_mlx_lm_module</code> parameter enables clean testing:</p> <pre><code># Production usage (normal)\nclient = MLXClient(spec)  # Imports mlx_lm automatically\n\n# Testing usage (mocked)\nmock_mlx = MagicMock()\nclient = MLXClient(spec, _mlx_lm_module=mock_mlx)  # Inject mock\n</code></pre> <p>Rationale: - Testable: No sys.modules hacks - Clear: Explicit what's being injected - Backward compatible: Parameter is optional</p>"},{"location":"architecture/technical-reference/#model-caching-strategy","title":"Model Caching Strategy","text":"<p>Problem: Loading MLX models is expensive (~0.5-2 seconds)</p> <p>Solution: Load once in <code>__init__()</code>, cache in instance</p> <p>Impact: - First invocation: ~0.7s (load) + ~0.7s (inference) = 1.4s - Subsequent calls: ~0.7s (inference only) - For 100 rows: Save ~70 seconds vs reload each time!</p>"},{"location":"architecture/technical-reference/#token-estimation_1","title":"Token Estimation","text":"<p>Primary: Use MLX tokenizer (model-specific, accurate) <pre><code>tokens = len(self.mlx_tokenizer.encode(text))\n</code></pre></p> <p>Fallback: Word count (if tokenizer fails) <pre><code>tokens = len(text.split())\n</code></pre></p> <p>Rationale: Graceful degradation, never fail on token estimation</p>"},{"location":"architecture/technical-reference/#error-handling","title":"Error Handling","text":"<p>Import Error: <pre><code>ImportError: MLX not installed. Install with:\n  pip install ondine[mlx]\nor:\n  pip install mlx mlx-lm\n\nNote: MLX only works on Apple Silicon (M1/M2/M3/M4 chips)\n</code></pre></p> <p>Model Load Error: <pre><code>Exception: Failed to load MLX model 'model-name'.\nEnsure the model exists on HuggingFace and you have access.\nError: [original error]\n</code></pre></p> <p>Design: Actionable error messages with context</p>"},{"location":"architecture/technical-reference/#performance-characteristics_1","title":"Performance Characteristics","text":"<p>Tested on Apple Silicon (M2): - Model: <code>mlx-community/Qwen3-1.7B-4bit</code> - Load time: 0.74s (once per pipeline) - Inference: 0.67s/prompt - Throughput: 1.49 prompts/sec - Memory: ~2GB (model in RAM)</p> <p>Comparison: - vs Cloud APIs: 10x faster (no network), free - vs vLLM: Simpler (no server), Mac-compatible - vs Ollama: Similar speed, more control</p>"},{"location":"architecture/technical-reference/#thread-safety_1","title":"Thread Safety","text":"<ul> <li>Not thread-safe: MLX models are not thread-safe</li> <li>Recommendation: Use <code>concurrency=1</code> in ProcessingSpec</li> <li>Why: MLX optimized for Apple Neural Engine (single-threaded)</li> </ul>"},{"location":"architecture/technical-reference/#dependencies_5","title":"Dependencies","text":"<pre><code>import mlx_lm  # Lazy imported\n# mlx_lm depends on:\n# - mlx&gt;=0.29.0\n# - mlx-metal (GPU acceleration)\n# - transformers (HuggingFace)\n</code></pre>"},{"location":"architecture/technical-reference/#used-by_5","title":"Used By","text":"<ul> <li><code>create_llm_client()</code> factory</li> <li>Any pipeline with <code>provider: mlx</code></li> <li>Examples: <code>10_mlx_qwen3_local.py</code>, <code>10_mlx_qwen3_local.yaml</code></li> </ul>"},{"location":"architecture/technical-reference/#testing-strategy_1","title":"Testing Strategy","text":"<p>Dependency Injection Pattern: <pre><code># Create mock module\nmock_mlx = MagicMock()\nmock_mlx.load.return_value = (mock_model, mock_tokenizer)\nmock_mlx.generate.return_value = \"response\"\n\n# Inject for testing\nclient = MLXClient(spec, _mlx_lm_module=mock_mlx)\n</code></pre></p> <p>Coverage: - 14 unit tests - Model loading/caching - Token estimation with fallback - Error handling - Factory integration</p>"},{"location":"architecture/technical-reference/#known-limitations_5","title":"Known Limitations","text":"<ul> <li>macOS only (MLX doesn't work on Linux/Windows)</li> <li>Single-threaded (not optimized for concurrency)</li> <li>Model download required (1-5GB per model)</li> <li>Requires HuggingFace token for some models</li> </ul>"},{"location":"architecture/technical-reference/#future-improvements_5","title":"Future Improvements","text":"<ul> <li>[ ] Add MLX streaming support</li> <li>[ ] Support custom MLX generation parameters</li> <li>[ ] Add model warmup option</li> <li>[ ] Cache models globally (shared across pipelines)</li> </ul>"},{"location":"architecture/technical-reference/#54-llm-provider-presets","title":"5.4 LLM Provider Presets","text":""},{"location":"architecture/technical-reference/#purpose_7","title":"Purpose","text":"<p>Simplify LLM provider configuration by providing pre-configured specifications for common providers, eliminating boilerplate and configuration errors.</p>"},{"location":"architecture/technical-reference/#class-llmproviderpresets","title":"Class: <code>LLMProviderPresets</code>","text":"<p>Location: <code>ondine/core/specifications.py</code> (lines 301-453)</p> <p>Responsibility: Provide pre-validated LLMSpec instances for popular providers</p> <p>Design Pattern: Static Registry Pattern</p> <p>Key Features: - Zero boilerplate for common providers (80% code reduction) - Pre-validated configurations (correct URLs, pricing) - No hardcoded API keys (security by design) - IDE autocomplete support - Pydantic validation throughout</p>"},{"location":"architecture/technical-reference/#available-presets","title":"Available Presets","text":""},{"location":"architecture/technical-reference/#openai-presets","title":"OpenAI Presets","text":"<pre><code>GPT4O_MINI = LLMSpec(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    input_cost_per_1k_tokens=Decimal(\"0.00015\"),\n    output_cost_per_1k_tokens=Decimal(\"0.0006\"),\n)\n\nGPT4O = LLMSpec(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o\",\n    input_cost_per_1k_tokens=Decimal(\"0.0025\"),\n    output_cost_per_1k_tokens=Decimal(\"0.01\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#togetherai-presets","title":"Together.AI Presets","text":"<pre><code>TOGETHER_AI_LLAMA_70B = LLMSpec(\n    provider=LLMProvider.OPENAI_COMPATIBLE,\n    provider_name=\"Together.AI\",\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n    base_url=\"https://api.together.xyz/v1\",\n    input_cost_per_1k_tokens=Decimal(\"0.0006\"),\n)\n\nTOGETHER_AI_LLAMA_8B = LLMSpec(\n    provider=LLMProvider.OPENAI_COMPATIBLE,\n    provider_name=\"Together.AI\",\n    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    base_url=\"https://api.together.xyz/v1\",\n    input_cost_per_1k_tokens=Decimal(\"0.0001\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#ollama-local-presets-free","title":"Ollama Local Presets (Free)","text":"<pre><code>OLLAMA_LLAMA_70B = LLMSpec(\n    provider=LLMProvider.OPENAI_COMPATIBLE,\n    provider_name=\"Ollama-Local\",\n    model=\"llama3.1:70b\",\n    base_url=\"http://localhost:11434/v1\",\n    input_cost_per_1k_tokens=Decimal(\"0.0\"),  # FREE!\n    output_cost_per_1k_tokens=Decimal(\"0.0\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#groq-anthropic-presets","title":"Groq &amp; Anthropic Presets","text":"<pre><code>GROQ_LLAMA_70B = LLMSpec(\n    provider=LLMProvider.GROQ,\n    model=\"llama-3.1-70b-versatile\",\n    input_cost_per_1k_tokens=Decimal(\"0.00059\"),\n)\n\nCLAUDE_SONNET_4 = LLMSpec(\n    provider=LLMProvider.ANTHROPIC,\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=8192,\n    input_cost_per_1k_tokens=Decimal(\"0.003\"),\n)\n</code></pre>"},{"location":"architecture/technical-reference/#factory-method-create_custom_openai_compatible","title":"Factory Method: <code>create_custom_openai_compatible()</code>","text":"<p>Purpose: Simplify custom provider configuration (vLLM, LocalAI, etc.)</p> <pre><code>@classmethod\ndef create_custom_openai_compatible(\n    cls,\n    provider_name: str,\n    model: str,\n    base_url: str,\n    input_cost_per_1k: float = 0.0,\n    output_cost_per_1k: float = 0.0,\n    **kwargs\n) -&gt; LLMSpec:\n    \"\"\"Factory for custom OpenAI-compatible providers.\"\"\"\n    return LLMSpec(\n        provider=LLMProvider.OPENAI_COMPATIBLE,\n        provider_name=provider_name,\n        model=model,\n        base_url=base_url,\n        input_cost_per_1k_tokens=Decimal(str(input_cost_per_1k)),\n        output_cost_per_1k_tokens=Decimal(str(output_cost_per_1k)),\n        **kwargs\n    )\n</code></pre>"},{"location":"architecture/technical-reference/#usage-with-pipelinebuilderwith_llm_spec","title":"Usage with <code>PipelineBuilder.with_llm_spec()</code>","text":"<p>New Method: <code>with_llm_spec(spec: LLMSpec) -&gt; PipelineBuilder</code></p> <p>Location: <code>ondine/api/pipeline_builder.py</code> (lines 260-311)</p> <p>Purpose: Accept pre-built LLMSpec objects instead of individual parameters</p> <p>Example Usage:</p> <pre><code>from ondine.core.specifications import LLMProviderPresets\n\n# Simple preset usage\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n    .build()\n)\n\n# Override preset settings\ncustom = LLMProviderPresets.GPT4O_MINI.model_copy(\n    update={\"temperature\": 0.9, \"max_tokens\": 500}\n)\npipeline.with_llm_spec(custom)\n\n# Custom provider via factory\ncustom_vllm = LLMProviderPresets.create_custom_openai_compatible(\n    provider_name=\"My vLLM Server\",\n    model=\"mistral-7b-instruct\",\n    base_url=\"http://my-server:8000/v1\",\n    temperature=0.7\n)\npipeline.with_llm_spec(custom_vllm)\n</code></pre>"},{"location":"architecture/technical-reference/#comparison-before-vs-after","title":"Comparison: Before vs After","text":"<p>Before (parameter-based): <pre><code>.with_llm(\n    provider=\"openai_compatible\",\n    provider_name=\"Together.AI\",\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=\"${TOGETHER_API_KEY}\",\n    input_cost_per_1k_tokens=0.0006,\n    output_cost_per_1k_tokens=0.0006\n)\n</code></pre></p> <p>After (preset-based): <pre><code>.with_llm_spec(LLMProviderPresets.TOGETHER_AI_LLAMA_70B)\n</code></pre></p> <p>Result: 80% code reduction, zero configuration errors</p>"},{"location":"architecture/technical-reference/#design-decisions","title":"Design Decisions","text":"<p>Why Static Class Instead of Enum? - Allows class methods (factory) - Better IDE autocomplete - Can include documentation in docstrings - Easier to extend without breaking existing code</p> <p>Why No Hardcoded API Keys? - Security: API keys should never be in code - All presets have <code>api_key=None</code> by default - Users must provide via environment variables or <code>model_copy(update={\"api_key\": \"...\"})</code></p> <p>Why Pydantic <code>model_copy()</code> for Overrides? - Immutability: Original presets unchanged - Type safety: Validation on override - Pythonic: Standard Pydantic pattern - Flexible: Override any field</p>"},{"location":"architecture/technical-reference/#security-validation","title":"Security Validation","text":"<p>Test: All presets must have <code>api_key=None</code> <pre><code>def test_presets_have_no_api_keys(self):\n    \"\"\"Security requirement: No hardcoded API keys.\"\"\"\n    presets = [\n        LLMProviderPresets.GPT4O_MINI,\n        LLMProviderPresets.TOGETHER_AI_LLAMA_70B,\n        # ... all presets\n    ]\n    for preset in presets:\n        assert preset.api_key is None\n</code></pre></p>"},{"location":"architecture/technical-reference/#backward-compatibility","title":"Backward Compatibility","text":"<p>100% backward compatible: Existing <code>with_llm()</code> method unchanged <pre><code># Old way still works\npipeline.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n# New way\npipeline.with_llm_spec(LLMProviderPresets.GPT4O_MINI)\n\n# Both methods can be mixed (last call wins)\n</code></pre></p>"},{"location":"architecture/technical-reference/#testing-coverage","title":"Testing Coverage","text":"<p>26 unit tests (100% pass): - Preset configuration validation - Security checks (no API keys) - Type safety verification - <code>with_llm_spec()</code> method tests - Override via <code>model_copy()</code> tests - Factory method tests - Backward compatibility tests</p>"},{"location":"architecture/technical-reference/#examples","title":"Examples","text":"<p>Complete example: <code>examples/14_provider_presets.py</code> - Demonstrates all preset usage patterns - Shows customization via <code>model_copy()</code> - Compares old vs new approach - Lists all available presets</p>"},{"location":"architecture/technical-reference/#future-improvements_6","title":"Future Improvements","text":"<ul> <li>[ ] Add more provider presets (Cohere, AI21, Mistral)</li> <li>[ ] Add model size variants (405B, 8B, etc.)</li> <li>[ ] Version-specific presets with deprecation warnings</li> <li>[ ] Auto-update pricing from provider APIs</li> <li>[ ] YAML preset files for user-defined presets</li> </ul>"},{"location":"architecture/technical-reference/#56-observability-module","title":"5.6 Observability Module","text":""},{"location":"architecture/technical-reference/#purpose_8","title":"Purpose","text":"<p>Provide distributed tracing with OpenTelemetry for production debugging and performance monitoring.</p>"},{"location":"architecture/technical-reference/#module-ondineobservability","title":"Module: <code>ondine/observability/</code>","text":"<p>Location: <code>ondine/observability/</code> (4 files, 140 lines)</p> <p>Responsibility: Optional distributed tracing via OpenTelemetry</p> <p>Key Features: - Opt-in tracing (disabled by default) - PII-safe by default (prompts sanitized) - Console &amp; Jaeger exporters - Per-stage latency tracking - LLM token/cost tracking (ready for Phase 3)</p>"},{"location":"architecture/technical-reference/#classes_4","title":"Classes","text":""},{"location":"architecture/technical-reference/#tracingobserver-class","title":"<code>TracingObserver</code> (Class)","text":"<p>Inheritance: <code>ExecutionObserver</code></p> <p>Responsibility: Create OpenTelemetry spans for pipeline execution</p> <p>Pattern: Observer Pattern (non-invasive instrumentation)</p> <p>Attributes: <pre><code>_include_prompts: bool           # If True, include prompts in spans (PII risk)\n_spans: dict[str, trace.Span]    # Active spans by name\n</code></pre></p> <p>Methods: - <code>on_pipeline_start()</code> - Create root span - <code>on_stage_start()</code> - Create nested stage span - <code>on_stage_complete()</code> - Close span with success metrics - <code>on_stage_error()</code> - Close span with error details - <code>on_pipeline_complete()</code> - Close root span - <code>on_pipeline_error()</code> - Close root span with error</p> <p>Span Hierarchy: <pre><code>pipeline.execute (root)\n\u251c\u2500\u2500 stage.DataLoader\n\u251c\u2500\u2500 stage.PromptFormatter\n\u251c\u2500\u2500 stage.LLMInvocation\n\u251c\u2500\u2500 stage.ResponseParser\n\u2514\u2500\u2500 stage.ResultWriter\n</code></pre></p>"},{"location":"architecture/technical-reference/#functions","title":"Functions","text":""},{"location":"architecture/technical-reference/#enable_tracingexporter-endpoint-service_name","title":"<code>enable_tracing(exporter, endpoint, service_name)</code>","text":"<p>Purpose: Enable distributed tracing (opt-in)</p> <p>Parameters: - <code>exporter: str = \"console\"</code> - Exporter type (console or jaeger) - <code>endpoint: str | None = None</code> - Jaeger endpoint URL - <code>service_name: str = \"ondine-pipeline\"</code> - Service name for traces</p> <p>Example:</p> <pre><code>from ondine.observability import enable_tracing\n\n# Console (development)\nenable_tracing(exporter=\"console\")\n\n# Jaeger (production)\nenable_tracing(exporter=\"jaeger\", endpoint=\"http://localhost:14268/api/traces\")\n</code></pre>"},{"location":"architecture/technical-reference/#disable_tracing","title":"<code>disable_tracing()</code>","text":"<p>Purpose: Disable tracing and cleanup resources</p>"},{"location":"architecture/technical-reference/#is_tracing_enabled-bool","title":"<code>is_tracing_enabled() -&gt; bool</code>","text":"<p>Purpose: Check if tracing is currently enabled</p>"},{"location":"architecture/technical-reference/#pii-sanitization","title":"PII Sanitization","text":""},{"location":"architecture/technical-reference/#sanitize_promptprompt-include_prompts-str","title":"<code>sanitize_prompt(prompt, include_prompts) -&gt; str</code>","text":"<p>Purpose: Sanitize prompt text (hash by default)</p> <p>Algorithm: <pre><code>if include_prompts:\n    return prompt  # Opt-in: include actual prompt\nelse:\n    return f\"&lt;sanitized-{hash(prompt) % 10000}&gt;\"  # Default: hash only\n</code></pre></p> <p>Design Decision: PII-safe by default - Users must explicitly opt-in to include prompts - Prevents accidental PII exposure in traces - Hash allows duplicate detection without exposing content</p>"},{"location":"architecture/technical-reference/#dependencies_6","title":"Dependencies","text":"<pre><code>opentelemetry-api&gt;=1.20.0        # Tracing API\nopentelemetry-sdk&gt;=1.20.0        # SDK implementation\nopentelemetry-exporter-jaeger&gt;=1.20.0  # Jaeger export\n</code></pre> <p>Installation: <code>pip install ondine[observability]</code></p>"},{"location":"architecture/technical-reference/#graceful-degradation","title":"Graceful Degradation","text":"<p>If OpenTelemetry not installed:</p> <pre><code>from ondine.observability import enable_tracing\n\nenable_tracing()  # Raises helpful ImportError with install instructions\nis_tracing_enabled()  # Returns False (always)\n</code></pre>"},{"location":"architecture/technical-reference/#thread-safety_2","title":"Thread Safety","text":"<ul> <li>Thread-safe: Yes (OpenTelemetry handles concurrency)</li> <li>Span creation/completion uses OpenTelemetry's thread-local context</li> </ul>"},{"location":"architecture/technical-reference/#performance_1","title":"Performance","text":"<ul> <li>Overhead: &lt;2% (measured with 10K row pipeline)</li> <li>Span creation: ~1-2ms per span</li> <li>Export: Async batch processing (non-blocking)</li> </ul>"},{"location":"architecture/technical-reference/#testing-coverage_1","title":"Testing Coverage","text":"<p>14 unit tests (100% passing): - Tracing enable/disable - PII sanitization - Observer integration - Export failure handling - Span lifecycle</p> <p>Integration tests: Ready for Phase 3 (LLM instrumentation)</p>"},{"location":"architecture/technical-reference/#usage-example","title":"Usage Example","text":"<pre><code>from ondine import PipelineBuilder\nfrom ondine.observability import enable_tracing\n\n# Enable tracing\nenable_tracing(exporter=\"console\")\n\n# Build and execute pipeline (traces automatically created)\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\nresult = pipeline.execute()  # Traces exported\n</code></pre>"},{"location":"architecture/technical-reference/#future-enhancements-phase-3","title":"Future Enhancements (Phase 3)","text":"<ul> <li>[ ] Instrument <code>LLMClient.invoke()</code> for LLM call tracing</li> <li>[ ] Add OTLP exporter (modern alternative to Jaeger)</li> <li>[ ] Add metrics integration</li> <li>[ ] Performance profiling</li> </ul> <p>Document Status: IN PROGRESS (Layer 0 complete, Layer 1 documented with MLX, Presets, and Observability)</p> <p>Next Sections: - 3.6 <code>utils/logging_utils.py</code> - 3.7 <code>utils/metrics_exporter.py</code> - 3.8 <code>utils/input_preprocessing.py</code> - Part 4: Core Models &amp; Specifications - Part 5: Complete Layer 1 (remaining providers) - Part 6+: Remaining layers...</p>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding Ondine's architecture will help you build more sophisticated pipelines and debug issues effectively.</p>"},{"location":"getting-started/core-concepts/#architecture-overview","title":"Architecture Overview","text":"<p>Ondine is built on a layered architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   High-Level APIs (QuickPipeline)      \u2502  User-friendly interfaces\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Pipeline Builder &amp; Configuration     \u2502  Fluent API, YAML config\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Pipeline Orchestration               \u2502  Execution strategies\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Pipeline Stages                      \u2502  Composable processing units\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Adapters (LLM, Storage, IO)         \u2502  External integrations\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/core-concepts/#key-components","title":"Key Components","text":""},{"location":"getting-started/core-concepts/#1-pipeline","title":"1. Pipeline","text":"<p>The <code>Pipeline</code> is the central execution unit. It orchestrates the flow of data through stages.</p> <pre><code>from ondine import Pipeline\n\n# Pipelines are built via PipelineBuilder\npipeline = PipelineBuilder.create()...build()\n\n# Execute synchronously\nresult = pipeline.execute()\n\n# Execute asynchronously\nresult = await pipeline.execute_async()\n</code></pre> <p>Key characteristics: - Immutable once built (thread-safe) - Encapsulates all configuration - Handles checkpointing and recovery - Tracks costs and metrics</p>"},{"location":"getting-started/core-concepts/#2-pipeline-builder","title":"2. Pipeline Builder","text":"<p>The <code>PipelineBuilder</code> provides a fluent API for constructing pipelines:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    # Data source\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n\n    # Prompt configuration\n    .with_prompt(\"Process: {text}\")\n\n    # LLM configuration\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n\n    # Processing configuration\n    .with_batch_size(100)\n    .with_concurrency(5)\n    .with_retry_policy(max_retries=3)\n\n    # Build immutable pipeline\n    .build()\n)\n</code></pre> <p>Builder methods: - Data: <code>from_csv()</code>, <code>from_dataframe()</code>, <code>from_parquet()</code>, <code>from_excel()</code> - Prompt: <code>with_prompt()</code>, <code>with_system_prompt()</code> - LLM: <code>with_llm()</code>, <code>with_llm_spec()</code> - Processing: <code>with_batch_size()</code>, <code>with_concurrency()</code>, <code>with_rate_limit()</code> - Reliability: <code>with_retry_policy()</code>, <code>with_checkpoint()</code> - Cost: <code>with_max_budget()</code> - Execution: <code>with_async_execution()</code>, <code>with_streaming()</code></p>"},{"location":"getting-started/core-concepts/#3-pipeline-stages","title":"3. Pipeline Stages","text":"<p>Stages are composable processing units that form a pipeline:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Data     \u2502\u2500\u2500\u2500\u25b6\u2502   Prompt    \u2502\u2500\u2500\u2500\u25b6\u2502     LLM      \u2502\u2500\u2500\u2500\u25b6\u2502   Response   \u2502\n\u2502   Loader   \u2502    \u2502  Formatter  \u2502    \u2502  Invocation  \u2502    \u2502    Parser    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Built-in stages: - <code>DataLoaderStage</code>: Load data from files/dataframes - <code>PromptFormatterStage</code>: Format prompts with variables - <code>LLMInvocationStage</code>: Call LLM APIs - <code>ResponseParserStage</code>: Parse and validate responses - <code>ResultWriterStage</code>: Write results to storage</p> <p>Custom stages: You can create custom stages by extending <code>PipelineStage</code>:</p> <pre><code>from ondine.stages import PipelineStage\n\nclass MyCustomStage(PipelineStage):\n    def process(self, input_data, context):\n        # Your processing logic\n        return processed_data\n\n    def validate_input(self, input_data):\n        # Validation logic\n        return ValidationResult(valid=True)\n</code></pre>"},{"location":"getting-started/core-concepts/#4-specifications","title":"4. Specifications","text":"<p>Specifications are Pydantic models that define configuration:</p>"},{"location":"getting-started/core-concepts/#datasetspec","title":"DatasetSpec","text":"<p>Defines input data configuration:</p> <pre><code>from ondine.core.specifications import DatasetSpec\n\nspec = DatasetSpec(\n    source=\"data.csv\",\n    input_columns=[\"text\"],\n    output_columns=[\"result\"],\n    format=\"csv\"\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#promptspec","title":"PromptSpec","text":"<p>Defines prompt templates:</p> <pre><code>from ondine.core.specifications import PromptSpec\n\nspec = PromptSpec(\n    template=\"Summarize: {text}\",\n    system_prompt=\"You are a helpful assistant.\"\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#llmspec","title":"LLMSpec","text":"<p>Defines LLM provider configuration:</p> <pre><code>from ondine.core.specifications import LLMSpec\n\nspec = LLMSpec(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=1000,\n    api_key=\"sk-...\"  # Or use environment variable\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#processingspec","title":"ProcessingSpec","text":"<p>Defines execution configuration:</p> <pre><code>from ondine.core.specifications import ProcessingSpec\n\nspec = ProcessingSpec(\n    batch_size=100,\n    concurrency=5,\n    max_retries=3,\n    checkpoint_interval=500,\n    rate_limit=60  # requests per minute\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#5-execution-strategies","title":"5. Execution Strategies","text":"<p>Ondine supports multiple execution modes:</p>"},{"location":"getting-started/core-concepts/#synchronous-default","title":"Synchronous (Default)","text":"<p>Single-threaded, sequential processing:</p> <pre><code>result = pipeline.execute()\n</code></pre> <p>Use when: Dataset fits in memory, simplicity is priority.</p>"},{"location":"getting-started/core-concepts/#asynchronous","title":"Asynchronous","text":"<p>Concurrent processing with async/await:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_async_execution(max_concurrency=10)\n    .build()\n)\n\nresult = await pipeline.execute_async()\n</code></pre> <p>Use when: Need high throughput, LLM API supports async.</p>"},{"location":"getting-started/core-concepts/#streaming","title":"Streaming","text":"<p>Memory-efficient processing for large datasets:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_streaming(chunk_size=1000)\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre> <p>Use when: Dataset is large (100K+ rows), memory is limited.</p> <p>See Execution Modes Guide for detailed comparison.</p>"},{"location":"getting-started/core-concepts/#6-adapters","title":"6. Adapters","text":"<p>Adapters abstract external dependencies:</p>"},{"location":"getting-started/core-concepts/#llm-client","title":"LLM Client","text":"<p>Adapts different LLM providers to a common interface:</p> <pre><code>from ondine.adapters import LLMClient\n\n# Automatically selected based on provider\nclient = create_llm_client(llm_spec)\nresponse = client.complete(prompt, temperature=0.7)\n</code></pre> <p>Supported providers: - OpenAI - Azure OpenAI - Anthropic Claude - Groq - MLX (local Apple Silicon) - Custom OpenAI-compatible APIs</p>"},{"location":"getting-started/core-concepts/#storage","title":"Storage","text":"<p>Handles checkpoint persistence:</p> <pre><code>from ondine.adapters import CheckpointStorage\n\nstorage = CheckpointStorage(path=\"./checkpoints\")\nstorage.save(state)\nstate = storage.load()\n</code></pre>"},{"location":"getting-started/core-concepts/#data-io","title":"Data IO","text":"<p>Handles various data formats:</p> <pre><code>from ondine.adapters import DataIO\n\n# Supports CSV, Parquet, Excel, JSON\ndata = DataIO.read(\"data.csv\")\nDataIO.write(data, \"output.parquet\")\n</code></pre>"},{"location":"getting-started/core-concepts/#execution-flow","title":"Execution Flow","text":"<p>Here's what happens when you call <code>pipeline.execute()</code>:</p> <ol> <li>Validation: Validate configuration and input data</li> <li>Cost Estimation: Calculate expected cost and token usage</li> <li>Checkpoint Check: Look for existing checkpoint to resume</li> <li>Data Loading: Load input data (streaming or in-memory)</li> <li>Prompt Formatting: Format prompts with input variables</li> <li>LLM Invocation: Call LLM API with rate limiting and retries</li> <li>Response Parsing: Parse and validate LLM responses</li> <li>Result Writing: Write results to output (file or DataFrame)</li> <li>Metrics Collection: Aggregate costs, tokens, timing</li> <li>Checkpoint Cleanup: Remove checkpoint on successful completion</li> </ol>"},{"location":"getting-started/core-concepts/#error-handling","title":"Error Handling","text":"<p>Ondine provides robust error handling:</p>"},{"location":"getting-started/core-concepts/#automatic-retries","title":"Automatic Retries","text":"<p>Failed requests are automatically retried with exponential backoff:</p> <pre><code>.with_retry_policy(\n    max_retries=3,\n    backoff_factor=2.0,\n    retry_on=[RateLimitError, NetworkError]\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#checkpointing","title":"Checkpointing","text":"<p>Long-running jobs can be resumed on failure:</p> <pre><code>.with_checkpoint(\"./checkpoints\", interval=100)\n</code></pre>"},{"location":"getting-started/core-concepts/#error-policies","title":"Error Policies","text":"<p>Control how errors are handled:</p> <pre><code>.with_error_policy(\"continue\")  # Continue on errors\n.with_error_policy(\"stop\")      # Stop on first error\n</code></pre>"},{"location":"getting-started/core-concepts/#cost-tracking","title":"Cost Tracking","text":"<p>Ondine tracks costs in real-time:</p> <pre><code>result = pipeline.execute()\n\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\nprint(f\"Input tokens: {result.costs.input_tokens}\")\nprint(f\"Output tokens: {result.costs.output_tokens}\")\nprint(f\"Cost per row: ${result.costs.total_cost / result.metrics.processed_rows:.6f}\")\n</code></pre>"},{"location":"getting-started/core-concepts/#budget-control","title":"Budget Control","text":"<p>Set maximum budget limits:</p> <pre><code>from decimal import Decimal\n\npipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_max_budget(Decimal(\"10.0\"))  # Max $10 USD\n    .build()\n)\n\n# Execution stops if budget exceeded\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/core-concepts/#observability","title":"Observability","text":"<p>Monitor pipeline execution:</p>"},{"location":"getting-started/core-concepts/#progress-bars","title":"Progress Bars","text":"<p>Automatic progress tracking with tqdm:</p> <pre><code>Processing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:45&lt;00:00, 22.1rows/s]\n</code></pre>"},{"location":"getting-started/core-concepts/#structured-logging","title":"Structured Logging","text":"<p>JSON-formatted logs with structlog:</p> <pre><code>from ondine.utils import configure_logging\n\nconfigure_logging(level=\"INFO\", json_format=True)\n</code></pre>"},{"location":"getting-started/core-concepts/#metrics-export","title":"Metrics Export","text":"<p>Export metrics to Prometheus:</p> <pre><code>from ondine.utils import MetricsExporter\n\nexporter = MetricsExporter(port=9090)\nexporter.start()\n</code></pre>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Execution Modes - Choose the right execution strategy</li> <li>Structured Output - Type-safe response parsing</li> <li>Cost Control - Optimize costs and set budgets</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>pip or uv package manager</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install ondine\n</code></pre>"},{"location":"getting-started/installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>uv is a fast Python package installer and resolver:</p> <pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install ondine\nuv pip install ondine\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Ondine provides optional dependency groups for specific features:</p>"},{"location":"getting-started/installation/#mlx-apple-silicon-local-inference","title":"MLX (Apple Silicon Local Inference)","text":"<p>For running models locally on Apple Silicon (M1/M2/M3/M4) with MLX:</p> <pre><code>pip install ondine[mlx]\n</code></pre> <p>Requirements: - macOS with Apple Silicon (M1/M2/M3/M4) - 16GB+ RAM recommended</p> <p>Supported models: - Qwen-2.5 series - Llama models - Mistral models - Any MLX-compatible model from Hugging Face</p>"},{"location":"getting-started/installation/#observability","title":"Observability","text":"<p>For OpenTelemetry-based observability and tracing:</p> <pre><code>pip install ondine[observability]\n</code></pre> <p>Features: - Distributed tracing with Jaeger - Custom metrics export - Performance monitoring</p>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>For contributing or development:</p> <pre><code>pip install ondine[dev]\n</code></pre> <p>Includes: - pytest and test utilities - ruff for linting - mypy for type checking - pre-commit hooks - Security scanners (bandit, pip-audit)</p>"},{"location":"getting-started/installation/#install-all-optional-dependencies","title":"Install All Optional Dependencies","text":"<pre><code>pip install ondine[mlx,observability,dev]\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that Ondine is installed correctly:</p> <pre><code>import ondine\n\nprint(ondine.__version__)\nprint(\"Ondine installed successfully!\")\n</code></pre> <p>Or use the CLI:</p> <pre><code>ondine --version\n</code></pre>"},{"location":"getting-started/installation/#api-keys-setup","title":"API Keys Setup","text":"<p>Ondine requires API keys for LLM providers. Set them as environment variables:</p>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"getting-started/installation/#azure-openai","title":"Azure OpenAI","text":"<pre><code>export AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://...\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-15-preview\"\n</code></pre>"},{"location":"getting-started/installation/#anthropic-claude","title":"Anthropic Claude","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"getting-started/installation/#groq","title":"Groq","text":"<pre><code>export GROQ_API_KEY=\"gsk_...\"\n</code></pre>"},{"location":"getting-started/installation/#environment-file","title":"Environment File","text":"<p>For convenience, create a <code>.env</code> file in your project root:</p> <pre><code># .env\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGROQ_API_KEY=gsk_...\n</code></pre> <p>Ondine automatically loads <code>.env</code> files using python-dotenv.</p>"},{"location":"getting-started/installation/#upgrade","title":"Upgrade","text":"<p>To upgrade to the latest version:</p> <pre><code>pip install --upgrade ondine\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-error-no-module-named-ondine","title":"Import Error: No module named 'ondine'","text":"<p>Make sure you're in the correct Python environment:</p> <pre><code>python -c \"import sys; print(sys.executable)\"\npip list | grep ondine\n</code></pre>"},{"location":"getting-started/installation/#mlx-installation-issues","title":"MLX Installation Issues","text":"<p>MLX only works on Apple Silicon. If you get import errors:</p> <ol> <li>Verify you're on Apple Silicon: <code>uname -m</code> should show <code>arm64</code></li> <li>Install Xcode Command Line Tools: <code>xcode-select --install</code></li> <li>Try reinstalling: <code>pip uninstall mlx mlx-lm &amp;&amp; pip install ondine[mlx]</code></li> </ol>"},{"location":"getting-started/installation/#api-key-not-found","title":"API Key Not Found","text":"<p>If you get authentication errors:</p> <ol> <li>Check environment variables: <code>echo $OPENAI_API_KEY</code></li> <li>Verify <code>.env</code> file is in the correct directory</li> <li>Restart your Python session after setting environment variables</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Build your first pipeline</li> <li>Core Concepts - Understand the architecture</li> <li>Provider Configuration - Configure LLM providers</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get started with Ondine in 5 minutes. This guide walks you through your first pipeline.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Ondine installed (<code>pip install ondine</code>)</li> <li>OpenAI API key (or another LLM provider)</li> </ul>"},{"location":"getting-started/quickstart/#your-first-pipeline","title":"Your First Pipeline","text":""},{"location":"getting-started/quickstart/#1-setup","title":"1. Setup","text":"<p>Create a new Python file and set your API key:</p> <pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Or use .env file\n</code></pre>"},{"location":"getting-started/quickstart/#2-prepare-sample-data","title":"2. Prepare Sample Data","text":"<p>Create a simple CSV file or use a pandas DataFrame:</p> <pre><code>import pandas as pd\n\n# Sample data\ndata = pd.DataFrame({\n    \"product\": [\n        \"iPhone 15 Pro Max 256GB\",\n        \"Samsung Galaxy S24 Ultra\",\n        \"Google Pixel 8 Pro\"\n    ]\n})\n\n# Save to CSV\ndata.to_csv(\"products.csv\", index=False)\n</code></pre>"},{"location":"getting-started/quickstart/#3-quick-api-simplest","title":"3. Quick API (Simplest)","text":"<p>The fastest way to process data:</p> <pre><code>from ondine import QuickPipeline\n\n# Create and run pipeline\npipeline = QuickPipeline.create(\n    data=\"products.csv\",\n    prompt=\"Extract the brand name from: {product}\",\n    model=\"gpt-4o-mini\"\n)\n\nresult = pipeline.execute()\n\n# View results\nprint(result.data)\nprint(f\"Cost: ${result.costs.total_cost:.4f}\")\n</code></pre> <p>Output: <pre><code>   product                      response\n0  iPhone 15 Pro Max 256GB       Apple\n1  Samsung Galaxy S24 Ultra      Samsung\n2  Google Pixel 8 Pro            Google\n\nCost: $0.0012\n</code></pre></p>"},{"location":"getting-started/quickstart/#4-builder-api-more-control","title":"4. Builder API (More Control)","text":"<p>For explicit configuration:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"product\"],\n        output_columns=[\"brand\"]\n    )\n    .with_prompt(\"Extract the brand name from: {product}\")\n    .with_llm(\n        provider=\"openai\",\n        model=\"gpt-4o-mini\",\n        temperature=0.0\n    )\n    .with_batch_size(100)\n    .with_concurrency(5)\n    .build()\n)\n\n# Estimate cost before running\nestimate = pipeline.estimate_cost()\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\n\n# Execute\nresult = pipeline.execute()\nprint(result.data)\n</code></pre>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#pattern-1-data-cleaning","title":"Pattern 1: Data Cleaning","text":"<pre><code>from ondine import QuickPipeline\n\npipeline = QuickPipeline.create(\n    data=\"messy_data.csv\",\n    prompt=\"\"\"\n    Clean and standardize this text:\n    {text}\n\n    Remove special characters, fix capitalization, trim whitespace.\n    \"\"\",\n    model=\"gpt-4o-mini\"\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-2-classification","title":"Pattern 2: Classification","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"reviews.csv\",\n        input_columns=[\"review_text\"],\n        output_columns=[\"sentiment\"]\n    )\n    .with_prompt(\"\"\"\n    Classify the sentiment of this review as: positive, negative, or neutral\n\n    Review: {review_text}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#pattern-3-structured-extraction","title":"Pattern 3: Structured Extraction","text":"<pre><code>from ondine import PipelineBuilder\nfrom ondine.stages.parser_factory import JSONParser\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"descriptions.csv\",\n        input_columns=[\"description\"],\n        output_columns=[\"brand\", \"model\", \"price\"]\n    )\n    .with_prompt(\"\"\"\n    Extract product information as JSON:\n    {{\n      \"brand\": \"...\",\n      \"model\": \"...\",\n      \"price\": \"...\"\n    }}\n\n    Description: {description}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(JSONParser())\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-results","title":"Understanding the Results","text":"<p>The <code>execute()</code> method returns an <code>ExecutionResult</code> object:</p> <pre><code>result = pipeline.execute()\n\n# Access the processed data\nprint(result.data)              # pandas DataFrame with results\n\n# View metrics\nprint(result.metrics.processed_rows)    # Number of rows processed\nprint(result.metrics.successful_rows)   # Successfully processed\nprint(result.metrics.failed_rows)       # Failed rows\nprint(result.metrics.elapsed_time)      # Total time in seconds\n\n# Check costs\nprint(result.costs.total_cost)          # Total cost in USD\nprint(result.costs.input_tokens)        # Input tokens used\nprint(result.costs.output_tokens)       # Output tokens generated\n</code></pre>"},{"location":"getting-started/quickstart/#cost-estimation","title":"Cost Estimation","text":"<p>Always estimate costs before processing large datasets:</p> <pre><code>pipeline = PipelineBuilder.create()...build()\n\n# Get cost estimate\nestimate = pipeline.estimate_cost()\n\nprint(f\"Estimated rows: {estimate.total_rows}\")\nprint(f\"Estimated tokens: {estimate.estimated_tokens}\")\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\n\n# Proceed only if acceptable\nif estimate.total_cost &lt; 10.0:\n    result = pipeline.execute()\nelse:\n    print(\"Cost too high, aborting\")\n</code></pre>"},{"location":"getting-started/quickstart/#error-handling","title":"Error Handling","text":"<p>Ondine automatically retries failed requests:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_retry_policy(max_retries=3, backoff_factor=2.0)\n    .build()\n)\n\nresult = pipeline.execute()\n\n# Check for failures\nif result.metrics.failed_rows &gt; 0:\n    print(f\"Failed rows: {result.metrics.failed_rows}\")\n    print(result.data[result.data['response'].isna()])\n</code></pre>"},{"location":"getting-started/quickstart/#checkpointing","title":"Checkpointing","text":"<p>For long-running jobs, enable checkpointing to resume on crashes:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"large_dataset.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_checkpoint(\"./checkpoints\", interval=100)  # Save every 100 rows\n    .build()\n)\n\n# If interrupted, re-run same command - it will resume from checkpoint\nresult = pipeline.execute()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have a working pipeline, explore:</p> <ul> <li>Core Concepts - Understand the architecture</li> <li>Execution Modes - Async and streaming execution</li> <li>Structured Output - Type-safe Pydantic models</li> <li>Cost Control - Budget limits and optimization</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"guides/cost-control/","title":"Cost Control","text":"<p>Ondine provides comprehensive cost management features to prevent budget overruns and optimize spending on LLM APIs.</p>"},{"location":"guides/cost-control/#pre-execution-cost-estimation","title":"Pre-Execution Cost Estimation","text":"<p>Always estimate costs before processing large datasets:</p> <pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"summary\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Get cost estimate\nestimate = pipeline.estimate_cost()\n\nprint(f\"Total rows: {estimate.total_rows}\")\nprint(f\"Estimated tokens: {estimate.estimated_tokens}\")\nprint(f\"Estimated cost: ${estimate.total_cost:.4f}\")\nprint(f\"Cost per row: ${estimate.cost_per_row:.6f}\")\n</code></pre>"},{"location":"guides/cost-control/#budget-limits","title":"Budget Limits","text":"<p>Set maximum budget to prevent overspending:</p> <pre><code>from decimal import Decimal\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_max_budget(Decimal(\"10.0\"))  # Max $10 USD\n    .build()\n)\n\n# Execution stops if budget exceeded\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/cost-control/#real-time-cost-tracking","title":"Real-Time Cost Tracking","text":"<p>Monitor costs during execution:</p> <pre><code>result = pipeline.execute()\n\n# View detailed costs\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\nprint(f\"Input tokens: {result.costs.input_tokens:,}\")\nprint(f\"Output tokens: {result.costs.output_tokens:,}\")\nprint(f\"Total tokens: {result.costs.total_tokens:,}\")\nprint(f\"Cost per row: ${result.costs.total_cost / result.metrics.processed_rows:.6f}\")\n</code></pre>"},{"location":"guides/cost-control/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"guides/cost-control/#1-choose-cost-effective-models","title":"1. Choose Cost-Effective Models","text":"<pre><code># Expensive: GPT-4\n.with_llm(provider=\"openai\", model=\"gpt-4\")  # ~$0.03/1K tokens\n\n# Cost-effective: GPT-4o-mini\n.with_llm(provider=\"openai\", model=\"gpt-4o-mini\")  # ~$0.0001/1K tokens\n\n# Free: Local MLX (Apple Silicon)\n.with_llm(provider=\"mlx\", model=\"mlx-community/Qwen2.5-7B-Instruct-4bit\")  # $0\n</code></pre>"},{"location":"guides/cost-control/#2-optimize-prompts","title":"2. Optimize Prompts","text":"<p>Shorter prompts = lower costs:</p> <pre><code># Expensive: Verbose prompt\nprompt = \"\"\"\nYou are a helpful assistant specialized in text summarization.\nPlease carefully read the following text and provide a comprehensive\nsummary that captures the main points while being concise.\n\nText: {text}\n\nPlease provide your summary below:\n\"\"\"\n\n# Cost-effective: Concise prompt\nprompt = \"Summarize in 1 sentence: {text}\"\n</code></pre>"},{"location":"guides/cost-control/#3-use-temperature0-for-deterministic-tasks","title":"3. Use Temperature=0 for Deterministic Tasks","text":"<p>Lower temperature often produces shorter, more focused responses:</p> <pre><code>.with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n</code></pre>"},{"location":"guides/cost-control/#4-set-max-tokens","title":"4. Set Max Tokens","text":"<p>Limit response length:</p> <pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    max_tokens=100  # Limit response to 100 tokens\n)\n</code></pre>"},{"location":"guides/cost-control/#5-batch-processing","title":"5. Batch Processing","text":"<p>Process multiple items per request when possible:</p> <pre><code>.with_batch_size(100)  # Process 100 rows per batch\n</code></pre>"},{"location":"guides/cost-control/#6-use-cheaper-providers","title":"6. Use Cheaper Providers","text":"<p>Consider alternative providers for cost savings:</p> Provider Cost (per 1M tokens) Speed OpenAI GPT-4o-mini $0.15 Fast Groq (Llama) $0.05-0.10 Very Fast Together.AI $0.20-0.60 Fast Local MLX $0 Medium"},{"location":"guides/cost-control/#cost-reporting","title":"Cost Reporting","text":""},{"location":"guides/cost-control/#summary-report","title":"Summary Report","text":"<pre><code>result = pipeline.execute()\n\nprint(\"\\n=== Cost Summary ===\")\nprint(f\"Rows processed: {result.metrics.processed_rows}\")\nprint(f\"Total cost: ${result.costs.total_cost:.4f}\")\nprint(f\"Average cost/row: ${result.costs.total_cost / result.metrics.processed_rows:.6f}\")\nprint(f\"Input tokens: {result.costs.input_tokens:,}\")\nprint(f\"Output tokens: {result.costs.output_tokens:,}\")\n</code></pre>"},{"location":"guides/cost-control/#export-to-csv","title":"Export to CSV","text":"<pre><code>import pandas as pd\n\n# Create cost report\ncost_report = pd.DataFrame([{\n    \"date\": pd.Timestamp.now(),\n    \"rows\": result.metrics.processed_rows,\n    \"total_cost\": result.costs.total_cost,\n    \"input_tokens\": result.costs.input_tokens,\n    \"output_tokens\": result.costs.output_tokens,\n    \"provider\": \"openai\",\n    \"model\": \"gpt-4o-mini\"\n}])\n\n# Append to running cost log\ncost_report.to_csv(\"cost_log.csv\", mode=\"a\", header=False, index=False)\n</code></pre>"},{"location":"guides/cost-control/#budget-aware-workflows","title":"Budget-Aware Workflows","text":""},{"location":"guides/cost-control/#estimate-before-execution","title":"Estimate Before Execution","text":"<pre><code>def safe_execute(pipeline, max_cost=10.0):\n    estimate = pipeline.estimate_cost()\n\n    if estimate.total_cost &gt; max_cost:\n        print(f\"Estimated cost ${estimate.total_cost:.2f} exceeds budget ${max_cost:.2f}\")\n        return None\n\n    print(f\"Proceeding with estimated cost: ${estimate.total_cost:.2f}\")\n    return pipeline.execute()\n\nresult = safe_execute(pipeline, max_cost=5.0)\n</code></pre>"},{"location":"guides/cost-control/#incremental-processing-with-cost-checks","title":"Incremental Processing with Cost Checks","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"large_data.csv\", ...)\n    .with_streaming(chunk_size=1000)\n    .with_max_budget(Decimal(\"20.0\"))\n    .build()\n)\n\ntotal_cost = 0.0\nfor chunk_result in pipeline.execute_stream():\n    total_cost += chunk_result.costs.total_cost\n    print(f\"Chunk cost: ${chunk_result.costs.total_cost:.4f}, \"\n          f\"Total so far: ${total_cost:.4f}\")\n\n    if total_cost &gt; 15.0:\n        print(\"Approaching budget limit, stopping\")\n        break\n</code></pre>"},{"location":"guides/cost-control/#cost-tracking-across-runs","title":"Cost Tracking Across Runs","text":"<p>Track costs across multiple pipeline runs:</p> <pre><code>from ondine.utils import CostTracker\n\ntracker = CostTracker()\n\n# Run multiple pipelines\nfor config in pipeline_configs:\n    pipeline = build_pipeline(config)\n    result = pipeline.execute()\n\n    tracker.add(\n        provider=config[\"provider\"],\n        model=config[\"model\"],\n        cost=result.costs.total_cost,\n        tokens=result.costs.total_tokens\n    )\n\n# View summary\nsummary = tracker.summary()\nprint(f\"Total spend: ${summary['total_cost']:.2f}\")\nprint(f\"Total tokens: {summary['total_tokens']:,}\")\n</code></pre>"},{"location":"guides/cost-control/#related","title":"Related","text":"<ul> <li>Execution Modes - Choose efficient execution strategy</li> <li>API Reference: CostTracker</li> <li>Structured Output - Optimize response parsing</li> </ul>"},{"location":"guides/execution-modes/","title":"Execution Modes","text":"<p>Ondine supports three execution modes optimized for different use cases. Choosing the right mode impacts performance, memory usage, and throughput.</p>"},{"location":"guides/execution-modes/#overview","title":"Overview","text":"Mode Best For Memory Usage Throughput Complexity Standard Small datasets (&lt; 50K rows) High Low-Medium Simple Async High throughput needs High High Medium Streaming Large datasets (100K+ rows) Constant Medium Medium"},{"location":"guides/execution-modes/#standard-execution-default","title":"Standard Execution (Default)","text":"<p>Synchronous, single-threaded processing. The simplest mode.</p>"},{"location":"guides/execution-modes/#usage","title":"Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = PipelineBuilder.create().from_csv(...).build()\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/execution-modes/#characteristics","title":"Characteristics","text":"<ul> <li>Execution: Sequential row-by-row or batch-by-batch</li> <li>Memory: Loads entire dataset into memory</li> <li>Concurrency: None (single-threaded)</li> <li>Return: Complete <code>ExecutionResult</code> with full DataFrame</li> </ul>"},{"location":"guides/execution-modes/#when-to-use","title":"When to Use","text":"<ul> <li>Dataset fits comfortably in memory (&lt; 50K rows typical)</li> <li>Straightforward processing without complex coordination</li> <li>Debugging or testing pipelines</li> <li>Simple scripts and notebooks</li> </ul>"},{"location":"guides/execution-modes/#when-not-to-use","title":"When NOT to Use","text":"<ul> <li>Dataset is large (&gt; 100K rows)</li> <li>Need maximum throughput</li> <li>Memory is constrained</li> </ul>"},{"location":"guides/execution-modes/#example","title":"Example","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_batch_size(100)\n    .build()\n)\n\n# Simple synchronous execution\nresult = pipeline.execute()\nprint(f\"Processed {result.metrics.processed_rows} rows\")\n</code></pre>"},{"location":"guides/execution-modes/#async-execution-concurrent","title":"Async Execution (Concurrent)","text":"<p>Asynchronous processing with configurable concurrency. Maximizes throughput.</p>"},{"location":"guides/execution-modes/#usage_1","title":"Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(...)\n    .with_async_execution(max_concurrency=10)\n    .build()\n)\n\nresult = await pipeline.execute_async()\n</code></pre>"},{"location":"guides/execution-modes/#characteristics_1","title":"Characteristics","text":"<ul> <li>Execution: Concurrent async/await with controlled parallelism</li> <li>Memory: Loads entire dataset into memory</li> <li>Concurrency: Configurable (e.g., 10-50 concurrent requests)</li> <li>Return: Complete <code>ExecutionResult</code> with full DataFrame</li> </ul>"},{"location":"guides/execution-modes/#when-to-use_1","title":"When to Use","text":"<ul> <li>Need high throughput (processing many rows quickly)</li> <li>LLM API supports async (most modern APIs do)</li> <li>Running in async context (FastAPI, aiohttp, async scripts)</li> <li>Dataset fits in memory but need speed</li> <li>Provider has high rate limits</li> </ul>"},{"location":"guides/execution-modes/#when-not-to-use_1","title":"When NOT to Use","text":"<ul> <li>Running in synchronous context (use standard mode instead)</li> <li>Dataset is very large (&gt; 100K rows) - consider streaming</li> <li>Provider has strict rate limits (async may hit limits faster)</li> <li>Memory is constrained</li> </ul>"},{"location":"guides/execution-modes/#example_1","title":"Example","text":"<pre><code>import asyncio\nfrom ondine import PipelineBuilder\n\nasync def process_data():\n    pipeline = (\n        PipelineBuilder.create()\n        .from_csv(\"large_data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n        .with_prompt(\"Analyze: {text}\")\n        .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n        .with_async_execution(max_concurrency=20)  # 20 concurrent requests\n        .with_rate_limit(100)  # Respect API limits\n        .build()\n    )\n\n    result = await pipeline.execute_async()\n    print(f\"Processed {result.metrics.processed_rows} rows\")\n    print(f\"Time: {result.metrics.elapsed_time:.2f}s\")\n    return result\n\n# Run async pipeline\nresult = asyncio.run(process_data())\n</code></pre>"},{"location":"guides/execution-modes/#concurrency-guidelines","title":"Concurrency Guidelines","text":"Provider Recommended Max Concurrency OpenAI (Tier 1) 10-20 OpenAI (Tier 4+) 50-100 Anthropic 10-20 Groq 30-50 Azure OpenAI 10-30 (varies by deployment) Local MLX 1 (no concurrency benefit)"},{"location":"guides/execution-modes/#streaming-execution-memory-efficient","title":"Streaming Execution (Memory-Efficient)","text":"<p>Process data in chunks with constant memory footprint. Best for very large datasets.</p>"},{"location":"guides/execution-modes/#usage_2","title":"Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(...)\n    .with_streaming(chunk_size=1000)\n    .build()\n)\n\nfor chunk_result in pipeline.execute_stream():\n    # Process each chunk as it completes\n    print(f\"Processed chunk: {len(chunk_result.data)} rows\")\n    chunk_result.data.to_csv(\"output.csv\", mode=\"a\", header=False)\n</code></pre>"},{"location":"guides/execution-modes/#characteristics_2","title":"Characteristics","text":"<ul> <li>Execution: Processes data in fixed-size chunks</li> <li>Memory: Constant footprint (1-2 chunks in memory max)</li> <li>Concurrency: Can combine with async for concurrent chunks</li> <li>Return: Iterator yielding <code>ExecutionResult</code> per chunk</li> </ul>"},{"location":"guides/execution-modes/#when-to-use_2","title":"When to Use","text":"<ul> <li>Large datasets (100K+ rows)</li> <li>Limited memory (processing datasets larger than available RAM)</li> <li>Need constant memory footprint</li> <li>Want early/incremental results</li> <li>Processing takes hours/days</li> </ul>"},{"location":"guides/execution-modes/#when-not-to-use_2","title":"When NOT to Use","text":"<ul> <li>Dataset under 50K rows (overhead not justified)</li> <li>Need entire dataset in memory for post-processing</li> <li>Pipeline has dependencies between rows</li> <li>Checkpointing is sufficient for your use case</li> </ul>"},{"location":"guides/execution-modes/#example_2","title":"Example","text":"<pre><code>from ondine import PipelineBuilder\nimport pandas as pd\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"huge_dataset.csv\",  # 500K rows\n        input_columns=[\"text\"],\n        output_columns=[\"summary\"]\n    )\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_streaming(chunk_size=5000)  # Process 5K rows at a time\n    .build()\n)\n\n# Process in streaming fashion\nall_results = []\nfor i, chunk_result in enumerate(pipeline.execute_stream()):\n    print(f\"Chunk {i+1}: {len(chunk_result.data)} rows, \"\n          f\"Cost: ${chunk_result.costs.total_cost:.4f}\")\n\n    # Write incrementally\n    mode = \"w\" if i == 0 else \"a\"\n    header = i == 0\n    chunk_result.data.to_csv(\"output.csv\", mode=mode, header=header, index=False)\n\n    all_results.append(chunk_result)\n\n# Aggregate metrics\ntotal_rows = sum(r.metrics.processed_rows for r in all_results)\ntotal_cost = sum(r.costs.total_cost for r in all_results)\nprint(f\"Total: {total_rows} rows, ${total_cost:.2f}\")\n</code></pre>"},{"location":"guides/execution-modes/#chunk-size-guidelines","title":"Chunk Size Guidelines","text":"Dataset Size Recommended Chunk Size 10K-50K 1,000 50K-100K 2,500 100K-500K 5,000 500K-1M 10,000 1M+ 25,000 <p>Larger chunks = fewer overhead, smaller chunks = finer progress tracking.</p>"},{"location":"guides/execution-modes/#streaming-async-maximum-efficiency","title":"Streaming + Async (Maximum Efficiency)","text":"<p>Combine streaming with async for both memory efficiency and high throughput:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"huge_dataset.csv\", ...)\n    .with_streaming(chunk_size=5000)\n    .with_async_execution(max_concurrency=20)\n    .build()\n)\n\nasync for chunk_result in pipeline.execute_stream_async():\n    # Each chunk processed with 20 concurrent requests\n    print(f\"Chunk done: {len(chunk_result.data)} rows\")\n</code></pre>"},{"location":"guides/execution-modes/#comparison-example","title":"Comparison Example","text":"<p>Processing the same 10K row dataset with different modes:</p>"},{"location":"guides/execution-modes/#standard-mode","title":"Standard Mode","text":"<pre><code># Loads all 10K rows into memory, processes sequentially\nresult = pipeline.execute()\n# Time: ~120s, Memory: ~500MB peak\n</code></pre>"},{"location":"guides/execution-modes/#async-mode","title":"Async Mode","text":"<pre><code># Loads all 10K rows into memory, 20 concurrent requests\nresult = await pipeline.execute_async()\n# Time: ~15s, Memory: ~500MB peak\n</code></pre>"},{"location":"guides/execution-modes/#streaming-mode","title":"Streaming Mode","text":"<pre><code># Processes 1K rows at a time\nfor chunk in pipeline.execute_stream():\n    pass\n# Time: ~110s, Memory: ~50MB constant\n</code></pre>"},{"location":"guides/execution-modes/#streaming-async","title":"Streaming + Async","text":"<pre><code># Processes 1K rows at a time with 20 concurrent requests per chunk\nasync for chunk in pipeline.execute_stream_async():\n    pass\n# Time: ~18s, Memory: ~50MB constant\n</code></pre>"},{"location":"guides/execution-modes/#choosing-the-right-mode","title":"Choosing the Right Mode","text":"<p>Use this decision tree:</p> <pre><code>Is dataset &gt; 100K rows?\n\u251c\u2500 YES \u2192 Use Streaming\n\u2502         \u2514\u2500 Need speed? \u2192 Add Async (streaming + async)\n\u2502\n\u2514\u2500 NO \u2192 Dataset &lt; 100K rows\n         \u251c\u2500 Need maximum speed?\n         \u2502  \u2514\u2500 YES \u2192 Use Async\n         \u2502\n         \u2514\u2500 NO \u2192 Use Standard (simplest)\n</code></pre>"},{"location":"guides/execution-modes/#memory-considerations","title":"Memory Considerations","text":""},{"location":"guides/execution-modes/#standardasync-memory-usage","title":"Standard/Async Memory Usage","text":"<pre><code>Memory = Base + (Dataset Size \u00d7 Row Size)\n</code></pre> <p>Example: 50K rows \u00d7 10KB/row = ~500MB</p>"},{"location":"guides/execution-modes/#streaming-memory-usage","title":"Streaming Memory Usage","text":"<pre><code>Memory = Base + (Chunk Size \u00d7 Row Size)\n</code></pre> <p>Example: 1K chunk \u00d7 10KB/row = ~10MB (constant)</p>"},{"location":"guides/execution-modes/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/execution-modes/#for-all-modes","title":"For All Modes","text":"<ol> <li>Use appropriate batch size: Larger batches = fewer API calls</li> <li>Enable checkpointing: Resume on failures</li> <li>Set rate limits: Respect provider limits</li> <li>Monitor costs: Use budget controls</li> </ol>"},{"location":"guides/execution-modes/#for-async-mode","title":"For Async Mode","text":"<ol> <li>Tune concurrency: Start low, increase gradually</li> <li>Respect rate limits: Too much concurrency can trigger rate limiting</li> <li>Monitor memory: Each concurrent request consumes memory</li> </ol>"},{"location":"guides/execution-modes/#for-streaming-mode","title":"For Streaming Mode","text":"<ol> <li>Choose appropriate chunk size: Balance memory vs. overhead</li> <li>Write incrementally: Don't accumulate all results in memory</li> <li>Enable checkpointing per chunk: More frequent checkpoints</li> </ol>"},{"location":"guides/execution-modes/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/07_async_execution.py</code> - Async processing</li> <li><code>examples/08_streaming_large_files.py</code> - Streaming</li> <li>Cost Control Guide - Budget management</li> <li>API Reference - Complete API docs</li> </ul>"},{"location":"guides/multi-column/","title":"Multi-Column Processing","text":"<p>Generate multiple output columns from a single LLM call using JSON parsing.</p>"},{"location":"guides/multi-column/#basic-usage","title":"Basic Usage","text":"<p>Use JSON parsing to extract multiple fields:</p> <pre><code>from ondine import PipelineBuilder\nfrom ondine.stages.parser_factory import JSONParser\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"description\"],\n        output_columns=[\"brand\", \"category\", \"price\"]\n    )\n    .with_prompt(\"\"\"\n        Extract product information as JSON:\n        {{\n          \"brand\": \"...\",\n          \"category\": \"...\",\n          \"price\": \"...\"\n        }}\n\n        Description: {description}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(JSONParser())\n    .build()\n)\n\nresult = pipeline.execute()\n# Result has 3 new columns: brand, category, price\n</code></pre>"},{"location":"guides/multi-column/#with-pydantic-validation","title":"With Pydantic Validation","text":"<p>For type-safe validation:</p> <pre><code>from pydantic import BaseModel\nfrom ondine.stages.response_parser_stage import PydanticParser\n\nclass ProductInfo(BaseModel):\n    brand: str\n    category: str\n    price: float\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"products.csv\", ...)\n    .with_prompt(\"...\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(PydanticParser(ProductInfo))\n    .build()\n)\n</code></pre>"},{"location":"guides/multi-column/#multiple-input-columns","title":"Multiple Input Columns","text":"<p>Use multiple input columns in your prompt:</p> <pre><code>pipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"title\", \"description\", \"category\"],\n        output_columns=[\"brand\", \"model\", \"price\"]\n    )\n    .with_prompt(\"\"\"\n        Extract product information:\n        {{\n          \"brand\": \"...\",\n          \"model\": \"...\",\n          \"price\": 0.0\n        }}\n\n        Title: {title}\n        Description: {description}\n        Category: {category}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .with_parser(JSONParser())\n    .build()\n)\n</code></pre>"},{"location":"guides/multi-column/#related","title":"Related","text":"<ul> <li>Structured Output - Pydantic models</li> <li>Pipeline Composition - Complex workflows</li> </ul>"},{"location":"guides/pipeline-composition/","title":"Pipeline Composition","text":"<p>Compose multiple pipelines to process independent columns with dependencies between them.</p>"},{"location":"guides/pipeline-composition/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder, PipelineComposer\n\n# Pipeline 1: Calculate similarity\nsimilarity_pipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"Calculate similarity (0-1): {text1} vs {text2}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Pipeline 2: Explain (depends on similarity result)\nexplanation_pipeline = (\n    PipelineBuilder.create()\n    .with_prompt(\"\"\"\n        The similarity score is {similarity}.\n        Explain why these texts are similar or different:\n        Text 1: {text1}\n        Text 2: {text2}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\n# Compose pipelines\ncomposer = (\n    PipelineComposer(input_data=\"data.csv\")\n    .add_column(\"similarity\", similarity_pipeline)\n    .add_column(\"explanation\", explanation_pipeline, depends_on=[\"similarity\"])\n)\n\nresult = composer.execute()\n</code></pre>"},{"location":"guides/pipeline-composition/#dependencies","title":"Dependencies","text":"<p>Specify dependencies between columns:</p> <pre><code>composer = (\n    PipelineComposer(input_data=df)\n    .add_column(\"category\", category_pipeline)  # No dependencies\n    .add_column(\"sentiment\", sentiment_pipeline)  # No dependencies\n    .add_column(\"recommendation\", recommendation_pipeline, \n                depends_on=[\"category\", \"sentiment\"])  # Depends on both\n)\n</code></pre>"},{"location":"guides/pipeline-composition/#execution-order","title":"Execution Order","text":"<p>Pipelines execute in dependency order: 1. Independent pipelines run first (parallel if async) 2. Dependent pipelines wait for their dependencies 3. Results from previous pipelines are available as input columns</p>"},{"location":"guides/pipeline-composition/#related","title":"Related","text":"<ul> <li>Multi-Column Processing</li> <li>Core Concepts</li> </ul>"},{"location":"guides/structured-output/","title":"Structured Output with Pydantic","text":"<p>Ondine provides type-safe structured output parsing using Pydantic models. This ensures LLM responses conform to your expected schema with automatic validation.</p>"},{"location":"guides/structured-output/#why-use-structured-output","title":"Why Use Structured Output?","text":"<p>Without structured output: <pre><code># Response: \"The brand is Apple and model is iPhone 15 Pro\"\n# Manual parsing required, error-prone, no type safety\n</code></pre></p> <p>With structured output: <pre><code># Response automatically validated and parsed to:\nProductInfo(brand=\"Apple\", model=\"iPhone 15 Pro\", price=999.99, condition=\"new\")\n</code></pre></p> <p>Benefits:</p> <ul> <li>Type safety with Pydantic validation</li> <li>Automatic JSON parsing and error handling</li> <li>Schema enforcement (required fields, types, constraints)</li> <li>IDE autocomplete for response fields</li> <li>Validation errors caught early</li> </ul>"},{"location":"guides/structured-output/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/structured-output/#1-define-your-pydantic-model","title":"1. Define Your Pydantic Model","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass ProductInfo(BaseModel):\n    brand: str = Field(..., description=\"Manufacturer name\")\n    model: str = Field(..., description=\"Product model\")\n    price: float = Field(..., gt=0, description=\"Price in USD\")\n    condition: str = Field(..., pattern=\"^(new|used|refurbished)$\")\n</code></pre>"},{"location":"guides/structured-output/#2-use-with-pipeline","title":"2. Use with Pipeline","text":"<pre><code>from ondine import PipelineBuilder\nfrom ondine.stages.response_parser_stage import PydanticParser\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\n        \"products.csv\",\n        input_columns=[\"product_description\"],\n        output_columns=[\"brand\", \"model\", \"price\", \"condition\"]\n    )\n    .with_prompt(\"\"\"\n        Extract product information and return JSON:\n        {\n          \"brand\": \"manufacturer name\",\n          \"model\": \"product model\",\n          \"price\": 999.99,\n          \"condition\": \"new|used|refurbished\"\n        }\n\n        Description: {product_description}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n    .with_parser(PydanticParser(ProductInfo, strict=True))\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/structured-output/#3-access-validated-results","title":"3. Access Validated Results","text":"<pre><code># Results are automatically validated\nprint(result.data)\n\n#    brand           model      price condition\n# 0  Apple    iPhone 15 Pro    999.99       new\n# 1  Samsung  Galaxy S24      899.99      used\n</code></pre>"},{"location":"guides/structured-output/#pydantic-model-examples","title":"Pydantic Model Examples","text":""},{"location":"guides/structured-output/#simple-model","title":"Simple Model","text":"<pre><code>from pydantic import BaseModel\n\nclass Sentiment(BaseModel):\n    label: str  # \"positive\", \"negative\", \"neutral\"\n    confidence: float  # 0.0 to 1.0\n</code></pre>"},{"location":"guides/structured-output/#model-with-validation","title":"Model with Validation","text":"<pre><code>from pydantic import BaseModel, Field, validator\n\nclass Review(BaseModel):\n    rating: int = Field(..., ge=1, le=5, description=\"Rating from 1-5\")\n    sentiment: str = Field(..., pattern=\"^(positive|negative|neutral)$\")\n    summary: str = Field(..., min_length=10, max_length=200)\n\n    @validator('rating')\n    def rating_must_match_sentiment(cls, v, values):\n        sentiment = values.get('sentiment')\n        if sentiment == 'positive' and v &lt; 4:\n            raise ValueError('Positive sentiment requires rating &gt;= 4')\n        if sentiment == 'negative' and v &gt; 2:\n            raise ValueError('Negative sentiment requires rating &lt;= 2')\n        return v\n</code></pre>"},{"location":"guides/structured-output/#nested-model","title":"Nested Model","text":"<pre><code>from pydantic import BaseModel\nfrom typing import List\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n    postal_code: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    email: str\n    addresses: List[Address]\n</code></pre>"},{"location":"guides/structured-output/#model-with-optional-fields","title":"Model with Optional Fields","text":"<pre><code>from pydantic import BaseModel\nfrom typing import Optional\n\nclass Product(BaseModel):\n    name: str\n    brand: str\n    price: float\n    description: Optional[str] = None  # Optional field\n    sku: Optional[str] = None\n</code></pre>"},{"location":"guides/structured-output/#model-with-enums","title":"Model with Enums","text":"<pre><code>from pydantic import BaseModel\nfrom enum import Enum\n\nclass Category(str, Enum):\n    ELECTRONICS = \"electronics\"\n    CLOTHING = \"clothing\"\n    FOOD = \"food\"\n    BOOKS = \"books\"\n\nclass Item(BaseModel):\n    name: str\n    category: Category\n    price: float\n</code></pre>"},{"location":"guides/structured-output/#complete-example","title":"Complete Example","text":"<pre><code>from pydantic import BaseModel, Field\nfrom ondine import PipelineBuilder\nfrom ondine.stages.response_parser_stage import PydanticParser\nimport pandas as pd\n\n# Define schema\nclass EmailClassification(BaseModel):\n    category: str = Field(..., pattern=\"^(spam|important|promotional|personal)$\")\n    confidence: float = Field(..., ge=0.0, le=1.0)\n    priority: int = Field(..., ge=1, le=5)\n    action: str = Field(..., pattern=\"^(archive|flag|delete|respond)$\")\n\n# Sample data\ndata = pd.DataFrame({\n    \"email\": [\n        \"URGENT: You won $1,000,000! Click here now!\",\n        \"Meeting tomorrow at 2pm with the CEO\",\n        \"50% off sale this weekend only!\"\n    ]\n})\n\n# Build pipeline\npipeline = (\n    PipelineBuilder.create()\n    .from_dataframe(\n        data,\n        input_columns=[\"email\"],\n        output_columns=[\"category\", \"confidence\", \"priority\", \"action\"]\n    )\n    .with_prompt(\"\"\"\n        Classify this email and return JSON:\n        {{\n          \"category\": \"spam|important|promotional|personal\",\n          \"confidence\": 0.0-1.0,\n          \"priority\": 1-5,\n          \"action\": \"archive|flag|delete|respond\"\n        }}\n\n        Email: {email}\n    \"\"\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n    .with_parser(PydanticParser(EmailClassification, strict=True))\n    .build()\n)\n\n# Execute with type-safe validation\nresult = pipeline.execute()\nprint(result.data)\n</code></pre> <p>Output: <pre><code>   email                                   category  confidence  priority   action\n0  URGENT: You won $1,000,000! Click...     spam        0.98         5    delete\n1  Meeting tomorrow at 2pm with CEO      important      0.95         1      flag\n2  50% off sale this weekend only!    promotional      0.92         3   archive\n</code></pre></p>"},{"location":"guides/structured-output/#strict-vs-non-strict-mode","title":"Strict vs Non-Strict Mode","text":""},{"location":"guides/structured-output/#strict-mode-recommended","title":"Strict Mode (Recommended)","text":"<pre><code>.with_parser(PydanticParser(ProductInfo, strict=True))\n</code></pre> <ul> <li>Validation errors stop processing</li> <li>Failed rows are retried (if retry policy configured)</li> <li>Guarantees all results match schema</li> </ul>"},{"location":"guides/structured-output/#non-strict-mode","title":"Non-Strict Mode","text":"<pre><code>.with_parser(PydanticParser(ProductInfo, strict=False))\n</code></pre> <ul> <li>Validation errors logged but processing continues</li> <li>Invalid rows get <code>None</code> values</li> <li>Useful for exploratory analysis</li> </ul>"},{"location":"guides/structured-output/#handling-validation-errors","title":"Handling Validation Errors","text":""},{"location":"guides/structured-output/#with-retries","title":"With Retries","text":"<pre><code>pipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_parser(PydanticParser(ProductInfo, strict=True))\n    .with_retry_policy(max_retries=3)  # Retry validation failures\n    .build()\n)\n\nresult = pipeline.execute()\n\n# Check for failed validations\nif result.metrics.failed_rows &gt; 0:\n    print(f\"Failed to validate {result.metrics.failed_rows} rows\")\n    failed = result.data[result.data['brand'].isna()]\n    print(failed)\n</code></pre>"},{"location":"guides/structured-output/#custom-error-handling","title":"Custom Error Handling","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    result = pipeline.execute()\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n    # Handle validation failures\n</code></pre>"},{"location":"guides/structured-output/#prompt-engineering-for-structured-output","title":"Prompt Engineering for Structured Output","text":""},{"location":"guides/structured-output/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Show example JSON in prompt: <pre><code>prompt = \"\"\"\nExtract product info as JSON:\n{{\n  \"brand\": \"Apple\",\n  \"model\": \"iPhone 15\",\n  \"price\": 999.99\n}}\n\nDescription: {description}\n\"\"\"\n</code></pre></p> </li> <li> <p>Specify field constraints: <pre><code>prompt = \"\"\"\nAnalyze sentiment and return JSON:\n{{\n  \"label\": \"positive|negative|neutral\",\n  \"confidence\": 0.0-1.0,\n  \"keywords\": [\"word1\", \"word2\"]\n}}\n\nText: {text}\n\"\"\"\n</code></pre></p> </li> <li> <p>Use low temperature: <pre><code>.with_llm(provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.0)\n</code></pre></p> </li> <li> <p>Include field descriptions: <pre><code>class Product(BaseModel):\n    brand: str = Field(..., description=\"Manufacturer name (e.g., Apple, Samsung)\")\n    price: float = Field(..., description=\"Price in USD, numeric only\")\n</code></pre></p> </li> </ol>"},{"location":"guides/structured-output/#json-vs-pydantic-parser","title":"JSON vs Pydantic Parser","text":""},{"location":"guides/structured-output/#json-parser-simple","title":"JSON Parser (Simple)","text":"<pre><code>from ondine.stages.parser_factory import JSONParser\n\n# Just parses JSON, no validation\n.with_parser(JSONParser())\n</code></pre> <p>Use when: - Schema is simple and flexible - Don't need type validation - Rapid prototyping</p>"},{"location":"guides/structured-output/#pydantic-parser-type-safe","title":"Pydantic Parser (Type-Safe)","text":"<pre><code>from ondine.stages.response_parser_stage import PydanticParser\n\n# Parses AND validates against schema\n.with_parser(PydanticParser(MyModel, strict=True))\n</code></pre> <p>Use when: - Need type safety and validation - Schema has constraints (ranges, patterns) - Production applications - API responses</p>"},{"location":"guides/structured-output/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guides/structured-output/#multiple-models","title":"Multiple Models","text":"<p>For different output types:</p> <pre><code>from typing import Union\n\nclass ShortSummary(BaseModel):\n    summary: str = Field(..., max_length=100)\n\nclass LongSummary(BaseModel):\n    summary: str = Field(..., max_length=500)\n    key_points: List[str]\n\n# Use Union types\nclass SummaryResponse(BaseModel):\n    content: Union[ShortSummary, LongSummary]\n    type: str\n</code></pre>"},{"location":"guides/structured-output/#post-validation-processing","title":"Post-Validation Processing","text":"<pre><code>from pydantic import BaseModel, validator\n\nclass Price(BaseModel):\n    amount: float\n    currency: str = \"USD\"\n\n    @validator('amount')\n    def round_price(cls, v):\n        return round(v, 2)\n\n    @property\n    def formatted(self) -&gt; str:\n        return f\"${self.amount:.2f}\"\n</code></pre>"},{"location":"guides/structured-output/#dynamic-schema","title":"Dynamic Schema","text":"<p>For runtime schema definition:</p> <pre><code>from pydantic import create_model\n\n# Create model dynamically\nfields = {\n    \"name\": (str, ...),\n    \"age\": (int, ...),\n    \"email\": (str, ...)\n}\n\nDynamicModel = create_model(\"DynamicModel\", **fields)\n\npipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_parser(PydanticParser(DynamicModel))\n    .build()\n)\n</code></pre>"},{"location":"guides/structured-output/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/structured-output/#validation-overhead","title":"Validation Overhead","text":"<p>Pydantic validation adds ~1-5ms per row. Negligible for most use cases.</p> <pre><code># For 10K rows:\n# - Without validation: ~120s\n# - With Pydantic: ~120.05s (0.04% overhead)\n</code></pre>"},{"location":"guides/structured-output/#complex-models","title":"Complex Models","text":"<p>Deeply nested models increase validation time:</p> <pre><code># Simple model: ~1ms\nclass Simple(BaseModel):\n    name: str\n    value: float\n\n# Complex nested: ~5ms\nclass Complex(BaseModel):\n    data: List[Dict[str, List[SubModel]]]\n</code></pre> <p>Tip: Keep models as flat as possible for best performance.</p>"},{"location":"guides/structured-output/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/structured-output/#common-validation-errors","title":"Common Validation Errors","text":"<p>Missing required field: <pre><code>ValidationError: field required (type=value_error.missing)\n</code></pre> Solution: Ensure LLM outputs all required fields in prompt.</p> <p>Type mismatch: <pre><code>ValidationError: value is not a valid float (type=type_error.float)\n</code></pre> Solution: Add type hints in prompt, use temperature=0.0.</p> <p>Pattern mismatch: <pre><code>ValidationError: string does not match regex (type=value_error.str.regex)\n</code></pre> Solution: Show valid values in prompt example.</p>"},{"location":"guides/structured-output/#debugging-tips","title":"Debugging Tips","text":"<ol> <li> <p>Test with small sample first: <pre><code>df_sample = df.head(10)\npipeline = builder.from_dataframe(df_sample, ...).build()\n</code></pre></p> </li> <li> <p>Use non-strict mode for debugging: <pre><code>.with_parser(PydanticParser(Model, strict=False))\n</code></pre></p> </li> <li> <p>Check raw responses: <pre><code># Enable debug logging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n</code></pre></p> </li> </ol>"},{"location":"guides/structured-output/#related","title":"Related","text":"<ul> <li>API Reference: PydanticParser</li> <li>Example: 03_structured_output.py</li> <li>Cost Control - Optimize costs</li> <li>Multi-Column Processing - Multiple outputs</li> </ul>"},{"location":"guides/providers/anthropic/","title":"Anthropic Claude Provider","text":"<p>Configure and use Anthropic Claude models with Ondine.</p>"},{"location":"guides/providers/anthropic/#setup","title":"Setup","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"guides/providers/anthropic/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"analysis\"])\n    .with_prompt(\"Analyze: {text}\")\n    .with_llm(\n        provider=\"anthropic\",\n        model=\"claude-3-5-sonnet-20241022\",\n        temperature=0.0,\n        max_tokens=1024\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/anthropic/#available-models","title":"Available Models","text":"<ul> <li><code>claude-3-5-sonnet-20241022</code> - Most capable (recommended)</li> <li><code>claude-3-5-haiku-20241022</code> - Fast and cost-effective</li> <li><code>claude-3-opus-20240229</code> - Most capable, legacy</li> </ul>"},{"location":"guides/providers/anthropic/#configuration-options","title":"Configuration Options","text":"<pre><code>.with_llm(\n    provider=\"anthropic\",\n    model=\"claude-3-5-sonnet-20241022\",\n    temperature=0.7,\n    max_tokens=4096,\n    top_p=1.0\n)\n</code></pre>"},{"location":"guides/providers/anthropic/#rate-limits","title":"Rate Limits","text":"<p>Recommended concurrency: 10-20</p>"},{"location":"guides/providers/anthropic/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>Cost Control</li> </ul>"},{"location":"guides/providers/azure/","title":"Azure OpenAI Provider","text":"<p>Configure and use Azure OpenAI Service with Ondine.</p>"},{"location":"guides/providers/azure/#setup","title":"Setup","text":"<pre><code>export AZURE_OPENAI_API_KEY=\"...\"\nexport AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\nexport AZURE_OPENAI_API_VERSION=\"2024-02-15-preview\"\n</code></pre>"},{"location":"guides/providers/azure/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(\n        provider=\"azure_openai\",\n        model=\"gpt-4\",\n        azure_endpoint=\"https://your-resource.openai.azure.com/\",\n        azure_deployment=\"your-deployment-name\",\n        api_version=\"2024-02-15-preview\"\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/azure/#configuration","title":"Configuration","text":"<p>The deployment name in Azure OpenAI maps to the model:</p> <pre><code>.with_llm(\n    provider=\"azure_openai\",\n    model=\"gpt-4\",  # Your base model\n    azure_deployment=\"my-gpt4-deployment\",  # Your Azure deployment name\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    api_version=\"2024-02-15-preview\"\n)\n</code></pre>"},{"location":"guides/providers/azure/#rate-limits","title":"Rate Limits","text":"<p>Rate limits in Azure OpenAI are configured per deployment. Adjust concurrency based on your TPM (tokens per minute) limits.</p>"},{"location":"guides/providers/azure/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>Cost Control</li> </ul>"},{"location":"guides/providers/custom/","title":"Custom OpenAI-Compatible APIs","text":"<p>Integrate any OpenAI-compatible API with Ondine, including Together.AI, vLLM, Ollama, and custom endpoints.</p>"},{"location":"guides/providers/custom/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(\n        provider=\"openai\",  # Use openai provider\n        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n        api_base=\"https://api.together.xyz/v1\",  # Custom endpoint\n        api_key=os.getenv(\"TOGETHER_API_KEY\")\n    )\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/custom/#provider-examples","title":"Provider Examples","text":""},{"location":"guides/providers/custom/#togetherai","title":"Together.AI","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    api_base=\"https://api.together.xyz/v1\",\n    api_key=os.getenv(\"TOGETHER_API_KEY\")\n)\n</code></pre>"},{"location":"guides/providers/custom/#vllm-self-hosted","title":"vLLM (Self-Hosted)","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    api_base=\"http://localhost:8000/v1\",\n    api_key=\"dummy\"  # vLLM doesn't require auth\n)\n</code></pre>"},{"location":"guides/providers/custom/#ollama-local","title":"Ollama (Local)","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"llama2\",\n    api_base=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"  # Any non-empty string\n)\n</code></pre>"},{"location":"guides/providers/custom/#custom-endpoint","title":"Custom Endpoint","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"your-model-name\",\n    api_base=\"https://your-api.example.com/v1\",\n    api_key=os.getenv(\"YOUR_API_KEY\")\n)\n</code></pre>"},{"location":"guides/providers/custom/#using-llmspec-advanced","title":"Using LLMSpec (Advanced)","text":"<p>For more control, use <code>LLMSpec</code>:</p> <pre><code>from ondine.core.specifications import LLMSpec\n\ncustom_spec = LLMSpec(\n    provider=\"openai\",\n    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    api_base=\"https://api.together.xyz/v1\",\n    api_key=os.getenv(\"TOGETHER_API_KEY\"),\n    temperature=0.7,\n    max_tokens=1000,\n    # Pricing (optional, for cost tracking)\n    input_cost_per_million=0.20,\n    output_cost_per_million=0.20\n)\n\npipeline = (\n    PipelineBuilder.create()\n    ...\n    .with_llm_spec(custom_spec)\n    .build()\n)\n</code></pre>"},{"location":"guides/providers/custom/#requirements","title":"Requirements","text":"<p>The custom API must be OpenAI-compatible, supporting: - <code>/v1/chat/completions</code> endpoint - Standard OpenAI request/response format - Bearer token authentication</p>"},{"location":"guides/providers/custom/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/providers/custom/#connection-errors","title":"Connection Errors","text":"<p>Check that the API endpoint is accessible: <pre><code>curl -X POST https://api.example.com/v1/chat/completions \\\n  -H \"Authorization: Bearer YOUR_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\":\"test\",\"messages\":[{\"role\":\"user\",\"content\":\"test\"}]}'\n</code></pre></p>"},{"location":"guides/providers/custom/#authentication-errors","title":"Authentication Errors","text":"<p>Verify your API key is correct and has proper permissions.</p>"},{"location":"guides/providers/custom/#model-not-found","title":"Model Not Found","text":"<p>Ensure the model name matches what the API expects.</p>"},{"location":"guides/providers/custom/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>API Reference: LLMSpec</li> </ul>"},{"location":"guides/providers/groq/","title":"Groq Provider","text":"<p>Configure and use Groq for ultra-fast inference with Ondine.</p>"},{"location":"guides/providers/groq/#setup","title":"Setup","text":"<pre><code>export GROQ_API_KEY=\"gsk_...\"\n</code></pre>"},{"location":"guides/providers/groq/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/groq/#available-models","title":"Available Models","text":"<ul> <li><code>llama-3.3-70b-versatile</code> - Best performance</li> <li><code>llama-3.1-70b-versatile</code> - Fast and capable</li> <li><code>mixtral-8x7b-32768</code> - Long context window</li> </ul>"},{"location":"guides/providers/groq/#configuration-options","title":"Configuration Options","text":"<pre><code>.with_llm(\n    provider=\"groq\",\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.7,\n    max_tokens=1000\n)\n</code></pre>"},{"location":"guides/providers/groq/#performance","title":"Performance","text":"<p>Groq is optimized for speed. Recommended concurrency: 30-50</p>"},{"location":"guides/providers/groq/#related","title":"Related","text":"<ul> <li>OpenAI</li> <li>Execution Modes</li> </ul>"},{"location":"guides/providers/local-mlx/","title":"Local MLX Provider (Apple Silicon)","text":"<p>Run models locally on Apple Silicon (M1/M2/M3/M4) with MLX - 100% free, private, offline-capable.</p>"},{"location":"guides/providers/local-mlx/#requirements","title":"Requirements","text":"<ul> <li>macOS with Apple Silicon (M1/M2/M3/M4)</li> <li>16GB+ RAM recommended</li> <li>Python 3.10+</li> </ul>"},{"location":"guides/providers/local-mlx/#installation","title":"Installation","text":"<pre><code>pip install ondine[mlx]\n</code></pre>"},{"location":"guides/providers/local-mlx/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"summary\"])\n    .with_prompt(\"Summarize: {text}\")\n    .with_llm(\n        provider=\"mlx\",\n        model=\"mlx-community/Qwen2.5-7B-Instruct-4bit\",\n        temperature=0.7,\n        max_tokens=500\n    )\n    .build()\n)\n\nresult = pipeline.execute()\nprint(f\"Cost: ${result.costs.total_cost:.2f}\")  # Always $0.00\n</code></pre>"},{"location":"guides/providers/local-mlx/#available-models","title":"Available Models","text":"<p>Any MLX-compatible model from Hugging Face:</p> <ul> <li><code>mlx-community/Qwen2.5-7B-Instruct-4bit</code> - Recommended, fast</li> <li><code>mlx-community/Llama-3.2-3B-Instruct-4bit</code> - Lightweight</li> <li><code>mlx-community/Mistral-7B-Instruct-v0.3-4bit</code> - Good quality</li> </ul>"},{"location":"guides/providers/local-mlx/#configuration","title":"Configuration","text":"<pre><code>.with_llm(\n    provider=\"mlx\",\n    model=\"mlx-community/Qwen2.5-7B-Instruct-4bit\",\n    temperature=0.7,\n    max_tokens=1000\n)\n</code></pre>"},{"location":"guides/providers/local-mlx/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>No concurrency benefit: MLX runs on single GPU, use concurrency=1</li> <li>First run slower: Model downloads and caches on first use</li> <li>Memory usage: 4-bit models use ~4-8GB RAM</li> </ul>"},{"location":"guides/providers/local-mlx/#benefits","title":"Benefits","text":"<ul> <li>Zero cost: No API fees</li> <li>Privacy: Data never leaves your machine</li> <li>Offline: Works without internet</li> <li>No rate limits: Process as much as you want</li> </ul>"},{"location":"guides/providers/local-mlx/#limitations","title":"Limitations","text":"<ul> <li>Only works on Apple Silicon Macs</li> <li>Slower than cloud APIs (but free!)</li> <li>No concurrency benefit</li> </ul>"},{"location":"guides/providers/local-mlx/#related","title":"Related","text":"<ul> <li>Cost Control</li> <li>Custom Providers</li> </ul>"},{"location":"guides/providers/openai/","title":"OpenAI Provider","text":"<p>Configure and use OpenAI models with Ondine.</p>"},{"location":"guides/providers/openai/#setup","title":"Setup","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"guides/providers/openai/#basic-usage","title":"Basic Usage","text":"<pre><code>from ondine import PipelineBuilder\n\npipeline = (\n    PipelineBuilder.create()\n    .from_csv(\"data.csv\", input_columns=[\"text\"], output_columns=[\"result\"])\n    .with_prompt(\"Process: {text}\")\n    .with_llm(provider=\"openai\", model=\"gpt-4o-mini\")\n    .build()\n)\n\nresult = pipeline.execute()\n</code></pre>"},{"location":"guides/providers/openai/#available-models","title":"Available Models","text":"<ul> <li><code>gpt-4o</code> - Most capable, balanced performance</li> <li><code>gpt-4o-mini</code> - Fast and cost-effective (recommended)</li> <li><code>gpt-4-turbo</code> - Advanced reasoning</li> <li><code>gpt-3.5-turbo</code> - Legacy, cost-effective</li> </ul>"},{"location":"guides/providers/openai/#configuration-options","title":"Configuration Options","text":"<pre><code>.with_llm(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\",\n    temperature=0.7,      # 0.0-2.0\n    max_tokens=1000,      # Max response length\n    top_p=1.0,            # Nucleus sampling\n    frequency_penalty=0.0,  # -2.0 to 2.0\n    presence_penalty=0.0    # -2.0 to 2.0\n)\n</code></pre>"},{"location":"guides/providers/openai/#rate-limits","title":"Rate Limits","text":"<p>See OpenAI Rate Limits for your tier.</p> <p>Recommended concurrency: - Tier 1: 10-20 - Tier 4+: 50-100</p>"},{"location":"guides/providers/openai/#related","title":"Related","text":"<ul> <li>Azure OpenAI</li> <li>Cost Control</li> </ul>"}]}